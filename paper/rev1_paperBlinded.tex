\documentclass[fleqn,reqno,12pt]{article}

%========================================
% Packages
%========================================

\usepackage{etex}

\RequirePackage{amsmath}            % Formeln
\RequirePackage{amsfonts}           % Fonts for Formulas
\RequirePackage{amssymb}
\RequirePackage{amsthm}
\RequirePackage{dsfont}             % double stroke fonts
\RequirePackage[final]{graphicx}
\RequirePackage{booktabs}
\RequirePackage{enumerate}
\RequirePackage{paralist}
\RequirePackage[all]{xy}            
\RequirePackage{url}

\RequirePackage{txfonts} % for strict implication symbols
\RequirePackage{soul}
\RequirePackage{relsize} % provides command \relsize{+/-x} for relative
                     % font size changes
\RequirePackage[german,english]{babel}
\RequirePackage[utf8]{inputenc}
\RequirePackage[T1]{fontenc} 
\RequirePackage{xypic}
\RequirePackage{multicol}
\RequirePackage{subcaption}
\RequirePackage{tikz}
\usetikzlibrary{arrows,shapes,automata,backgrounds,petri,fit,decorations.pathmorphing}
\RequirePackage{units}

\RequirePackage{dialogue}

\RequirePackage{setspace}

\RequirePackage[colorinlistoftodos,color=lightgray,bordercolor=blue,textsize=footnotesize]{todonotes}

\RequirePackage{xspace} % for \xspace in definition of acronyms etc.


\RequirePackage[final,            % override "draft" which means "no nothing"
            colorlinks,       % rather than outlining them in boxes
            linkcolor=black,   % override truly awful colour choices
            citecolor=black,   %   (ditto)
            urlcolor=black,    %   (ditto)
            plainpages=false, % to overcome complaints with multiple
            pdfpagelabels,    % multiple page 1-s due to preface
            hypertexnames=false % solves warning, but interferes with
                                % index and \autoref apparently
            ]{hyperref}


\usepackage[backend=bibtex,natbib=true,style=authoryear-comp,backend=bibtex,doi=false,url=false]{biblatex}

\bibliography{choicePrince,biblio}

\newtheoremstyle{Satz}
   {}                      %Space above
   {1em}                  %Space below
   {\normalfont}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\bfseries\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')

\theoremstyle{Satz}
\newtheorem{theorem}{Theorem} 
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{fact}{Fact}
\newtheorem{claim}{Claim} 
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}{Exercise} 
\newtheorem{problem}{Problem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\newtheoremstyle{Bsp}
   {}                      %Space above
   {1em}                  %Space below
   {\itshape}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')
\theoremstyle{Bsp}
% \newtheorem{example}[theorem]{Example}


% Math --------------------
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\tuple}[1]{\left \langle #1\right\rangle}
\newcommand{\card}[1]{\left \lvert \, #1 \, \right\rvert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\setbar}{\ensuremath{\thinspace \mid \thinspace}}
\newcommand{\probbar}{\ensuremath{\mid}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\df}{\rightarrow}
\newcommand{\es}{\emptyset}
\newcommand{\den}[1]{\left [\! \left [ #1 \right ]\! \right]}
% \newcommand{\den}[1]{\left \llbracket #1  \right \rrbracket} % this would use fourier package which makes 'cases' environment bad
\newcommand{\no}{\noindent}
\newcommand{\hin}{"$\Rightarrow$" }
\newcommand{\rueck}{"$\Leftarrow$" }
\newcommand{\exs}{\vspace{.15cm}}
\newcommand{\pow}[1]{\ensuremath{\mathcal{P}(#1)}}	% Powerset
\newcommand{\restr}{{\restriction}}
\newcommand{\implicates}{\ensuremath{\leadsto}}  % arrow for
                                % implicatures in examples
\newcommand{\update}[2]{\ensuremath{#1[#2]}}
\newcommand{\myts}{\ensuremath{\thinspace}}
\newcommand{\mycolon}{\ensuremath{\thinspace \colon \thinspace}}
\newcommand{\mydot}{\ensuremath{\thinspace . \thinspace}}


\makeatletter
\newcommand{\prob}{\@ifstar
  \simpleprob%
  \condprob%
}
\def\simpleprob(#1){\ensuremath{\Pr(#1)}}
\def\condprob(#1|#2){\ensuremath{\Pr(#1 \,|\, #2)}}
\makeatother

% General Text Markup--------------------
\newcommand{\runex}[1]{\begin{center}#1\end{center}}
\newcommand{\mydef}[1]{\textsc{#1}}  		% definitions
\newcommand{\markdef}[1]{\textsc{#1}}  		% definitions; alternative
\newcommand{\myment}[1]{\emph{#1}}  	% first mentions
\newcommand{\myword}[1]{\textbf{\texttt{#1}}}	% refering to the word
\newcommand{\myemph}[1]{\emph{#1}}			% emphasis
\newenvironment{exnonum}{
  \begin{list}{}{
    \setlength{\leftmargin}{2.5em}
    \setlength{\rightmargin}{2.5em}
    %\setlength{\itemindent}{-1.5em}
  }
}{
  \end{list}
}


% Slanted Fractions
\newcommand{\myslantfrac}[2]{\msf{#1}{#2}}
\newcommand{\msf}[2]{\ensuremath{\nicefrac{#1}{#2}}}
\newcommand{\msftext}[2]{\nicefrac{#1}{#2}}


% Symbols for Conditionals
\newcommand{\cond}{\ensuremath{>}}
\newcommand{\bicond}{\ensuremath{\Leftrightarrow}}
\newcommand{\strcondWill}{\ensuremath{\boxRight}}
\newcommand{\strcondMight}{\ensuremath{\DiamondRight}}

% Signaling Games & IBR
\newcommand{\sen}{\ensuremath{S}\xspace}		% Sender variable
\newcommand{\mysen}[1]{\ensuremath{\sen^{#1}}} % Sender of type XYZ
\newcommand{\rec}{\ensuremath{R}\xspace}		% Receiver variable
\newcommand{\myrec}[1]{\ensuremath{\rec_{#1}}} % Receiver of type XYZ
\newcommand{\States}{\ensuremath{T}\xspace}		% Set of States
\newcommand{\state}{\ensuremath{t}\xspace}		% single states
\newcommand{\mystate}[1]{\ensuremath{\state_{\text{#1}}}\xspace} %meaningful states
\newcommand{\Messgs}{\ensuremath{M}\xspace}		% Set of Messages
\newcommand{\messg}{\ensuremath{m}\xspace}		% single messages
\newcommand{\mymessg}[1]{\ensuremath{\messg_{\text{#1}}}\xspace} %meaningful messages
\newcommand{\cost}{\ensuremath\operatorname{C}} % cost function
\newcommand{\Acts}{\ensuremath{A}\xspace}		% Set of R-actions
\newcommand{\act}{\ensuremath{a}\xspace}		% single action
\newcommand{\myact}[1]{\ensuremath{\act_{\text{#1}}}\xspace} %meaningful
\newcommand{\Worlds}{\ensuremath{W}}		% Worlds
\newcommand{\world}{\ensuremath{w}}		% single world
\newcommand{\myworld}[1]{\ensuremath{\world_{\text{#1}}}} %named world
\newcommand{\util}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\Util}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\utils}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\Utils}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\RealUtil}{\ensuremath{\operatorname{V}}}	% material payoffs
\newcommand{\Sstrat}{\ensuremath{\sigma}} % Behav/Probab Sender strategy
\newcommand{\Sstrats}{\ensuremath{\mathcal{S}}}	% Set of S-strategies
\newcommand{\Spure}{\ensuremath{s}} % Pure sender strategy
\newcommand{\Spures}{\ensuremath{\mathsf{S}}} % Set of pure sen strategies
\newcommand{\Smixed}{\ensuremath{\tilde{s}}} % Mixed sender strategy
\newcommand{\Smixeds}{\ensuremath{\Delta(\Messgs^\States)}}
\newcommand{\SpuresW}{\ensuremath{\mathsf{S}}} 
\newcommand{\SpuresS}{\ensuremath{\mathsf{S}^{{\mathrm{S}}}}}
\newcommand{\Rstrat}{\ensuremath{\rho}} % Behav/Probab Receiver strategy
\newcommand{\Rstrats}{\ensuremath{\mathcal{R}}}	% Set of R-Strategies
\newcommand{\Rpure}{\ensuremath{r}} % Pure receiver strategy
\newcommand{\Rpures}{\ensuremath{\mathsf{R}}} % Set of pure rec strategies
\newcommand{\Rmixed}{\ensuremath{\tilde{r}}} % Mixed receiver strategy
\newcommand{\Rmixeds}{\ensuremath{\Delta(\Acts^\Messgs)}} 
\newcommand{\RpuresW}{\ensuremath{\mathsf{R}}} 
\newcommand{\RpuresS}{\ensuremath{\mathsf{R}^{\mathrm{S}}}} 
\newcommand{\PureBR}{\ensuremath{\operatorname{BR}}} % Set of pure best responses
\newcommand{\ProbBR}{\ensuremath{\operatorname{BR_{Prob}}}} % Set of mixed best responses
\newcommand{\bel}{\ensuremath{\pi}}
\newcommand{\Bels}{\ensuremath{\Pi}}
\newcommand{\Sbel}{\ensuremath{\pi_{\sen}}}
\newcommand{\Sbels}{\ensuremath{\Pi_{\sen}}}
\newcommand{\Rbel}{\ensuremath{\pi_{\rec}}}
\newcommand{\Rbels}{\ensuremath{\Pi_{\rec}}}
\newcommand{\EU}{\ensuremath{\operatorname{EU}}} % Expected Utility
\newcommand{\EV}{\ensuremath{\operatorname{EV}}} % Expected Response Utility
\newcommand{\BR}{\ensuremath{\operatorname{BR}}} % Best Response
\newcommand{\QR}{\ensuremath{\operatorname{QR}}} % Quantal Response
\newcommand{\WBR}{\ensuremath{\text{{\relsize{-1}W}BR}}} % Weak Best Response
\newcommand{\SBR}{\ensuremath{\text{{\relsize{-1}W}BR}}} % Strong Best Response
\newcommand{\interpr}{\ensuremath{\delta}} % Interpretation strategy

% OT Stuff
\newcommand{\Gen}{\ensuremath{\operatorname{Gen}}} % Generator
\newcommand{\Eval}{\ensuremath{\operatorname{Eval}}} % Evaluator
\newcommand{\Con}{\ensuremath{\operatorname{Con}}} % Constraints
\newcommand{\metsuc}{\ensuremath{\succ}} % Symbol for BiOT metric
\newcommand{\metsuceq}{\ensuremath{\succeq}} % Symbol for BiOT metric
\newcommand{\Go}[1]{\operatorname{Pool}_{#1}}
\newcommand{\Oo}[1]{\operatorname{Opt}_{#1}}
\newcommand{\Bo}[1]{\operatorname{Blo}_{#1}}
\newcommand{\Gr}[1]{\operatorname{GAM^{\rho}}_{#1}}
\newcommand{\Or}[1]{\operatorname{OPT^{\rho}}_{#1}}
\newcommand{\Br}[1]{\operatorname{BLO^{\rho}}_{#1}}

% Abbreviations/Acronyms:
\newcommand{\acro}[1]{\textsc{#1}\xspace}
\newcommand{\acros}[1]{\textsc{#1}{\relsize{-1}s}\xspace}
\newcommand{\bc}{\acro{bc}} % Biscuit Conditional(s)
\newcommand{\bcs}{\acros{bc}}
\newcommand{\cbc}{\acro{cbc}} % Counterfactual BCs
\newcommand{\cbcs}{\acros{cbc}}
\newcommand{\ibr}{\acro{ibr}} % IBR model
\newcommand{\iqr}{\acro{iqr}} % IQR model
\newcommand{\rsa}{\acro{rsa}} % RSA model
\newcommand{\ot}{\acro{ot}} % BiOT
\newcommand{\biot}{\acro{b{\relsize{-1}i}ot}} % BiOT
\newcommand{\tom}{\acro{t{\relsize{-1}o}m}} % ToM
\newcommand{\fc}{\acro{fc}} % Free Choice
\newcommand{\cp}{\acro{cp}} % Conditional Perfection
\newcommand{\uc}{\acro{uc}} % Unconditional Readings
\newcommand{\pbe}{\acro{pbe}}   % Perfect Bayesian Equil.
\newcommand{\pbes}{\acros{pbe}}
\newcommand{\forind}{\acro{fi}} % forward induction
\newcommand{\tcp}{\acro{tcp}} % truth ceteris paribus
\newcommand{\cmr}{\acro{cmr}} % credible message rationalizability
\newcommand{\cm}{\acro{cm}} % credible message (profile) (Rabin)
\newcommand{\condition}[2]{\acro{#1}{#2}} % conditions 
\newcommand{\br}{\acro{br}} % best response (property)
\newcommand{\wbr}{\acro{{\relsize{-1}w}br}} % weak best response (property)
\newcommand{\sbr}{\acro{{\relsize{-1}s}br}} % strong best response (property)
\newcommand{\curb}{\acro{curb}} % curb sets
\newcommand{\gtp}{\acro{gtp}} % game theoretic pragmatics
\newcommand{\sda}{\acro{sda}} % simplification of disjunctive antecedents
\newcommand{\decprob}{\ensuremath{\mathcal{D}}}
\newcommand{\ques}{\ensuremath{\mathfrak{Q}}}
\newcommand{\vsi}{\acro{vsi}}
\newcommand{\evsi}{\acro{evsi}}
\newcommand{\uv}{\acro{uv}}
\newcommand{\qud}{\acro{qud}}
\newcommand{\NE}{\acro{ne}}
\newcommand{\NEs}{\acros{ne}}
\newcommand{\SNE}{\acro{sne}}
\newcommand{\SNEs}{\acros{sne}}
\newcommand{\SG}{\acro{sg}}
\newcommand{\SGs}{\acros{sg}}
\newcommand{\KO}{\textsc{ko\relsize{-1}bs}} % Kennedy's observation
\newcommand{\EVP}{\acro{evp}}    % extreme-value principle
\newcommand{\illc}{\acro{illc}}

% Evolution
\newcommand{\EGT}{\acro{egt}}
\newcommand{\ESS}{\acro{ess}}
\newcommand{\ESSs}{\acros{ess}}
\newcommand{\NSS}{\acro{nss}}
\newcommand{\NSSs}{\acros{nss}}
\newcommand{\sigsys}{\textsc{SigSys}\xspace} % signaling system
\newcommand{\sigsyss}{\textsc{SigSys{\relsize{-1}s}}\xspace} % signaling system Plural

\newcommand{\fin}{\rule{0mm}{1mm}\hfill{\rule{1.5cm}{0.2pt}}}

%Properly typeset tilde for URLs
\def\urltilde{\kern -.15em\lower .7ex\hbox{\~{}}\kern .04em}


% Beamer footnote for references:
\newcommand{\beamfn}[1]{
  \vfill
      \begin{footnotesize}
        \leftskip 0.1in
        \parindent -0.1in
       \hspace{-0.3cm}\rule{2cm}{0.01cm}\\ \vspace{-0.15cm}
        #1
      \end{footnotesize}
}

\newcommand{\myvec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\transpose}[1]{\ensuremath{\operatorname{T}(#1)}}
\newcommand{\normalize}[1]{\ensuremath{\operatorname{N}(#1)}}

\newcommand{\dn}[1]{\draftnote{#1}}
\newcommand{\fn}[1]{\footnote{#1}}

\newcommand{\stateunmarked}{\ensuremath{\state}\xspace}
\newcommand{\statemarked}{\ensuremath{\state^*}\xspace}
\newcommand{\messgunmarked}{\ensuremath{\messg}\xspace}
\newcommand{\messgmarked}{\ensuremath{\messg^*}\xspace}
\newcommand{\actunmarked}{\ensuremath{\act}\xspace}
\newcommand{\actmarked}{\ensuremath{\act^*}\xspace}

\newcommand{\sunmarked}{\ensuremath{\state}\xspace}
\newcommand{\smarked}{\ensuremath{\state^*}\xspace}
\newcommand{\munmarked}{\ensuremath{\messg}\xspace}
\newcommand{\mmarked}{\ensuremath{\messg^*}\xspace}
\newcommand{\aunmarked}{\ensuremath{\act}\xspace}
\newcommand{\amarked}{\ensuremath{\act^*}\xspace}

\newcommand{\ssome}{\mystate{\exists\neg\forall}}
\newcommand{\sall}{\mystate{\forall}}
\newcommand{\msome}{\mymessg{some}}
\newcommand{\mall}{\mymessg{all}}
\newcommand{\asome}{\myact{\exists\neg\forall}}
\newcommand{\aall}{\myact{\forall}}

% for repeating examples with gb4e

\newcounter{myexememory}
\newenvironment{exer}[1]
{
\setcounter{myexememory}{\value{exx}}
\setcounter{exx}{\getrefnumber{#1}}
\addtocounter{exx}{-1}  
\begin{exe}
}
{
\end{exe}
\setcounter{exx}{\value{myexememory}}
}

\newenvironment{nakedlist}{
  \begin{list}{\quad}{}
}
{
  \end{list}
}

\DefineNamedColor{named}{mycol}{cmyk}{0.6,0.6,0,0}
\DefineNamedColor{named}{mygray}{cmyk}{0.05,0.05,0.05,0.05}
\DefineNamedColor{named}{mygraylight}{cmyk}{0.017,0.017,0.017,0.017}
\DefineNamedColor{named}{mycol2}{cmyk}{0.8,0,0.8,0.2}

\newcommand{\mymark}[1]{{\color{mycol}{#1}}}

\DeclareMathOperator{\expo}{exp}

\newcommand{\mygray}[1]{{\textcolor{gray}{#1}}}
\newcommand{\mycol}[1]{{\textcolor{mycol}{#1}}}
\newcommand{\mycolh}[1]{{\textcolor{mycol2}{#1}}}


% \usepackage{calc}
\newsavebox\CBox
\newcommand\msout[2][0.5pt]{%
  \ifmmode\sbox\CBox{$#2$}\else\sbox\CBox{#2}\fi%
  \makebox[0pt][l]{\usebox\CBox}%  
  \rule[0.5\ht\CBox-#1/2]{\wd\CBox}{#1}}

\newcommand{\greensquare}{\raisebox{1.5pt}{\textcolor{Green}{\Large{\ensuremath{\blacksquare}}}}}
\newcommand{\bluecircle}{\textcolor{blue}{\Huge{\ensuremath{\bullet}}}}
\newcommand{\greencircle}{\textcolor{Green}{\Huge{\ensuremath{\bullet}}}}

\newcommand{\greensquareS}{\raisebox{1.5pt}{\textcolor{Green}{\normalsize{\ensuremath{\blacksquare}}}}}
\newcommand{\greensquareSS}{\raisebox{1.5pt}{\textcolor{Green}{\footnotesize{\ensuremath{\blacksquare}}}}}
\newcommand{\bluecircleS}{\textcolor{blue}{\Large{\ensuremath{\bullet}}}}
\newcommand{\greencircleS}{\textcolor{Green}{\Large{\ensuremath{\bullet}}}}

\newcommand{\soc}{\ensuremath{\theta}\xspace}

\usepackage{todonotes}
\usepackage{subcaption}

\usetikzlibrary{positioning,arrows,calc,fit}
%========================================
% Standard Layout
%========================================

\usepackage{bigints}
\usepackage{MnSymbol}

\usepackage{titlesec}

\usepackage{geometry}
 % \geometry{
 % a4paper,
 % left=300mm,
 % right=250mm,
 % top=400mm,
 % bottom=400mm
 % }

\pagenumbering{gobble} 
% left justification:
\raggedright

%\usepackage{authblk}

% misc
\usepackage{graphicx}
\usepackage{babel}
\usepackage{amsmath}

\makeatletter
\makeatother

% Customisations specific to Philosophy of Science:

% paper size, margins, etc:
\usepackage{geometry}
\geometry{verbose,letterpaper,tmargin=1.5in,bmargin=1.5in,lmargin=1.25in,rmargin=1in}

\doublespacing

\usepackage[T1]{fontenc}


% Itemize
\renewcommand{\labelitemi}{\large{$\mathbf{\cdot}$}}    % itemize symbols
\renewcommand{\labelitemii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiv}{\large{$\mathbf{\cdot}$}}
% Description
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\textsc{#1}}

\renewcommand{\footnotesize}{\normalsize}

\usepackage{setspace} % load setspace before footmisc

\usepackage{footmisc}
\setlength{\footnotesep}{\baselineskip}

\usepackage{setspace,caption}
\AtBeginCaption{\doublespacing} 

\renewcommand{\footnotelayout}{\doublespacing}


% Figure Captions
\usepackage{caption} % use corresponding myfiguresize!
\setlength{\captionmargin}{20pt}
\renewcommand{\captionfont}{\normalsize}
\setlength{\belowcaptionskip}{7pt} % standard is 0pt

\newcommand{\myalert}[1]{\textcolor{red}{#1}}

\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{calc}


\title{Smart Representations: {R}ationality and Evolution in a Richer  Environment}

\date{}

%\titleformat{\subsection}
%       {\normalfont\fontfamily{phv}\fontsize{12}{17}\itshape}{\thesubsection}{1em}{}
     
     
\usepackage{titlesec}
\titleformat*{\section}{\bf}
\titleformat*{\subsection}{\it}
\titleformat*{\subsubsection}{\it}

% indentation
\setlength\parindent{12pt}

\begin{document}

\maketitle

\begin{abstract}
\normalsize
  Standard applications of evolutionary game theory look at a single, fixed game and focus on
  the evolution of behavior for that game alone. Instead, this paper uses tools from
  evolutionary game theory to study the evolutionary competition between \emph{choice
    mechanisms} in a rich and variable environment. A choice mechanism is a way of representing
  a decision situation, paired with a method for choosing an act based on this 
  subjective representation. We demonstrate the usefulness of this approach by a case study
  that compares a number of classic proposals for decision making. The
  main result is that regret-based preference representations can be evolutionarily selected
  for, despite being non-veridical.
\end{abstract}

\newpage

\section{Introduction}
\label{sec:intr--motiv}

% \begin{quotation} \textit{This is what I aim at, because the point of philosophy is to start with something so simple as not to seem worth stating, and to end with something so paradoxical that no one will believe it.} \citep{Russ18} \end{quotation}

%Evolutionary game theory has become an established philosophical tool for probing into
%conceptual issues whose complexity requires mathematical modelling at a high level of
%abstraction.  Abstraction entails simplifying assumptions. 

If agents deal with a rich and variable environment, they have to face many different choice
situations.
Standard evolutionary game models frequently
simplify reality in two ways that are relevant for present purposes. Firstly, the environment
is represented as a fixed \emph{stage game}; secondly, the focus of
evolutionary selection is behavior for that stage game alone. In contrast, some argue for
studying the evolutionary competition of general \emph{choice mechanisms} in a rich and
variable environment
\citep[e.g.,][]{FawcettHamblin2013:Exposing-the-be,McNamara2013:Towards-a-Riche,HammStev12}. To show that
there is no tension here, this paper introduces a method for reconciling established notions of
evolutionary game theory with evolutionary selection of choice mechanisms in variable
environments \citep[see, inter alia, related work
by][]{Harley1981:Learning-the-Ev,ZollmanSmead2010:Plasticity-and-,SmeadZollman2013:The-Stability-o,OConnor2016:Evolving-to-Gen}.

In full generality, a choice mechanism associates decision situations with action choices. A crucial
part of a choice mechanism is the \emph{subjective representation} of the decision situation,
in particular the manner of forming preferences and beliefs about a possibly uncertain world (see
figure~\ref{fig:ChoiceMechanism} below). This paper asks: which preference and belief
representations are ecologically valuable and lead to high fitness?
The evolution of
preferences has been subject of recent interest in theoretical economics
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. Here, we argue that questions of preference evolution should take
variability in uncertainty representation into account as well. In relation to this, 
the main contributions of this work are essentially three. First, we show that the way agents form beliefs, and the type of uncertainty that they entertain in particular, is a key aspect for the evolutionary success. 
Second, we demonstrate
that if agents have imprecise probabilistic beliefs \citep[e.g.][]{gardsah82,levi74,walley96}, faithful and objective representation in terms of true, real evolutionary fitness can be outperformed by
subjective (e.g., regret-based) preference representations that deviate
from the true fitness that natural selection operates on.
Third, although a few examples along similar lines have recently been introduced (REFERENCES), our approach based on a meta-game model is very uncustomary in the literature, and contributes to widen the methods of evolutionary game theory beyond the traditional single-game framework.

% To show this, we study the evolutionary competition of choice mechanisms, or more
% specifically the competition of subjective representations of choice situations, in a
% variable environment. Concretely, we look at evolutionary ``meta games'' whose ``acts'' are
% choice mechanisms and whose ``payoffs'' represent the average expected fitness that different
% choice mechanisms would accrue when playing arbitrary games (from a given class, with a given
% occurrence probability). In this sense, a meta game captures (the modeller's) assumptions
% about the relevant statistics of the environment in which evolutionary forces operate, while
% we are still able to use standard methods from evolutionary game theory, like stability or
% evolutionary dynamics.

The paper is organized as follows. Section \ref{sec:rati--subj} reviews different views of rational choice and relates the
approach we take here to the existing literature. Sections \ref{sec:subj-ut,bel,dec-rules} and \ref{sec:subj-rep,ev-fit} introduce the
relevant decision-theoretic and game-theoretic notions, while Sections \ref{sec:metagame} and \ref{sec:model} present the model.
%spells out four choice mechanisms on whose
%evolutionary competition we zoom in 
The main results of the paper are in Section~\ref{sec:results}, and Section \ref{sec:extensions} extends the evolutionary analysis by
introducing more choice mechanisms and alternative models of the environment. Finally, Section \ref{sec:conclusion} concludes.

\section{Rationality and Representations}
\label{sec:rati--subj}

The standard textbook definition of \textit{rationality} in economics and decision theory
traces back to the seminal work by \citet{deFinetti37}, \citet{Neumannvon-NeumannMorgenstern1944:Theory-of-Games}
and \citet{Savage1954:The-Foundations}. It says that a choice is rational only if it maximizes
(subjective) expected utility.


\noindent Expected utility is subjective in the sense that it is a function of subjective
beliefs and subjective preferences of the decision maker (DM). To wit, a choice can be
rational, i.e., the best choice from the DM's point of view, even if based on peculiar beliefs
and/or aberrant preferences. % (Indeed, while
% \citeauthor{Neumannvon-NeumannMorgenstern1944:Theory-of-Games} assumed that probabilities were
% objectively given, it was \citet{Savage1954:The-Foundations} who famously extended von Neumann
% and Morgenstern's axiomatization to include a subjective notion of probabilistic beliefs on top
% of the subjective notion of preference that von Neumann and Morgenstern started out with.)

If beliefs and preferences are subjective, there is room for \emph{rationalization} or
\emph{redescriptionism} of observable behavior. For example,
\citet{KahnemannTversky1979:Prospect-Theory} famously demonstrated that what appear to be
violations of rationality norms in human decision making can be explained on the assumption
that subjects' beliefs and preferences deviate systematically from the objectively given
parameters in the presented choice task. Similarly for social decision making, including social
preferences such as fairness, altruism or similar, allows us to describe as rational
empirically observed behavior, such as in experimental Prisoner's Dilemmas or public goods
games, that might otherwise appear irrational \citep[e.g.,][]{fehrschmidt99,charrab02}.

The main objection to redescriptionism is that, without additional constraints, the notion of
rationality is likely to collapse, as it seems possible to deem rational almost everything that
is observed, given the freedom to adjust beliefs and preferences at will. \emph{Normativism}
therefore emphasizes that there are many ways in which ascriptions of beliefs and preferences
should be constrained by normative considerations of rationality as well. As for beliefs, where
possible, subjective beliefs should reflect objective chance. 
%Where this is impossible,
%temporal consistency of choices constrains rational subjective beliefs, else exploitation by
%Dutch books is possible. 
As for preferences, in certain situations it may make sense to
postulate on normative grounds that subjective preferences should be oriented towards tracking
objective fitness. For instance, profit maximization seems a necessary requirement for
evolution in a competitive market because only firms behaving according to profit maximization will survive in the long run \citep[e.g.,][]{alch50,Fried53}.

An alternative view on rationality of choice is \emph{adaptationism}
\citep[e.g.,][]{Anderson1991:Is-human-cognit,ChaterOaksford2000:The-Rational-An,HagenChater2012:Decision-Making}. Adaptationism
aims to explain rational behavior by appeal to evolutionary considerations: DMs have acquired
choice mechanisms that have proved to be adaptive with respect to the variable environment
where they have evolved. A choice mechanism can be a set of distinct heuristics (the DM's
\emph{adaptive toolbox}) that have little in common
\citep[e.g.,][]{TverskyKahnemann1981:The-Framing-of-,GigerenzerGoldstein1996:Reasoning-the-F,ScheibehenneRieskamp2013:Testing-the-Ada}. But
to closely relate to the literature on evolution of preferences and to the philosophical debate
about the nature of rational choice, we here suggest to think of a choice mechanism
as a map from choice situations to action choices which includes an explicit level of
\emph{subjective representation} of the situation. Specifically, a subjective representation is a general way of forming preferences and beliefs about the choice situation, and we
are most interested in the question which subjective representations, and which choice mechanisms in general, are better than others
from an evolutionary point of view.


\iffalse

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 2.25cm]

  \begin{scope}

    \node[] (environment) {};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = environment, node distance = 2cm] (situation)
    {environment produces random \\ \textbf{decision situation} $D$};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = situation] (representation) 
    {\textbf{subjective representation} of $D$ \\ \textcolor{gray}{[preference, beliefs, \dots]}};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = representation] (action)
    {\textbf{action choice} $a$};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = action,node distance = 1.5cm] (payoff) {\textbf{payoff} for $a$ in $D$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.5cm]situation.center)--([xshift=-1.75cm,yshift=0.55cm]representation.center)
    node[midway, right] {subj.~representation scheme $\gamma$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.55cm]representation.center)--([xshift=-1.75cm,yshift=0.5cm]action.center)
    node[midway, right] {action selection function $\kappa$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.55cm]action.center)--([xshift=-1.75cm,yshift=0.5cm]payoff.center)
    node[midway, right] {};

    \node[minimum height = 0.75cm, text width=6cm, text
    centered, right of = representation, xshift = 2cm, rotate = 90] (mechanism) {choice mechanism};

    \begin{pgfonlayer}{background}
             \node [rounded corners, dashed,
       very thick,draw=black!40,fit={($(situation.south)+(0,-10pt)$) 
                                     ($(mechanism.west)+(+5pt,+50pt)$) 
                                     % ($(action.north)+(0,+10pt)$) 
                                     ($(representation.west)+(-7pt,0)$)
                                   }] (box) {};
    \end{pgfonlayer}

  \end{scope}



\end{tikzpicture}
  \caption{Schematic representation of action choice. A choice mechanism maps decision
    situation $D$ to action choice $a$ by means of a representation of $D$ in terms of a subjective
    preference and a subjective belief.}
  \label{fig:ChoiceMechanism}
\end{figure}





We submit that meta-games are a useful tool for structuring and sharpening conceptual inquiry
in a domain that is notorious for its fuzziness and elusiveness. To show how this approach can
be applied to the benefit of conceptual debate, we consider the evolution of preferences
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. 
%Concretely, we here pit two different ways of
%representing preferences against each other: (i) veridical representations that correctly
%reflect objective fitness, and (ii) non-veridical subjective preferences in terms of regret
%\citep[e.g.,][]{Savage1951:The-theory-of-s,}. 
We show that subjective, non-veridical preference
representations based on a notion of regret can be adaptive and even outperform objective representations of the true evolutionary fitness, when agents, at least on occasion, hold
\emph{imprecise probabilistic beliefs} \citep[e.g.,][]{gilsch89,levi74,gardsah82} and make
decisions based on a security choice rule (here: the maxmin rule).\footnote{Decision-theoretic background and justifications for the use of maxmin and imprecise probabilities is given in Appendix~\ref{sec:impr-prob-beli}.}

The case study under consideration strips the generality of a meta-game approach down to a perspicuous
comparison between four choice mechanisms, summarized in table~\ref{tab:CMs}.
% the cross-product of two different preference representations (veridical vs.~regret) and two
%different belief representations (precise vs.~imprecise). 
\begin{table}[t]
  \centering
  \begin{tabular}{cccc}
  \multicolumn{3}{c}{choice mechanism} \\  \cmidrule(r){1-3}
    \multicolumn{2}{c}{subjective representation} & action selection & corresponding classic rule \\  \cmidrule(r){1-2}
    preference type & belief type  \\ \midrule
    objective fitness & precise & maxmin EU &  Laplace rule \\
    objective fitness & imprecise & maxmin EU & maxmin \\ 
    regret & precise & maxmin EU & Laplace rule \\
    regret & imprecise & maxmin EU & regret minimization \\ 
  \end{tabular}
  \caption{Overview relevant choice mechanisms and the corresponding (behaviorally equivalent)
    classic choice rules.}
  \label{tab:CMs}
\end{table}
\iffalse
This exclusive selection of choice mechanisms is motivated by a number of
considerations. Firstly, the set of conceivable choice mechanisms is vast. Most of that
conceivable vastness is utter nonsense, and some selection of plausible candidates is required in
any case. Secondly, a restriction to only a small number of suitable candidates will
help demonstrate more distinctly how a meta game approach may help structure philosophical
inquiry, which is our main goal. Thirdly, our case study adds to the recent literature on the
evolution of preferences by making evident the possibly non-trivial interaction between
preference and belief representations, all else equal. This is our secondary goal. 
Finally,
\fi 
The
four choice mechanisms,  
%that we look at here
although phrased in terms of preference and belief representations, map onto well-known classic
proposals for rational decision making (see table~\ref{tab:CMs}). In other words, there is also
motivation from historical relevance for our selection. To see this, we first recapitulate the
relevant classics ---to have a handy label, we call them \emph{classic rules}--- and then show
how they relate to our notion of choice mechanism.



The classic literature on rational choice contains a number of proposals for which acts a
player should choose if she is rational. Maximization of expected utility, as
given in Definition~\ref{def:rationality}, is just one of them, possible only if DM holds a precise probabilistic belief.
Here, we consider three others:
(i) maxmin, (ii) Laplace rule, and (iii) regret minimization. 
%All of these apply to games and
%solitary decision situations.
We focus on games first, but definitions carry over straightforwardly to solitary decisions.



A \emph{choice mechanism} is a map from choice situations to action choices that contains an explicit level, in terms of preferences and beliefs, of subjective representation of
the choice situation. Concretely, let us think of a choice mechanism as a pair of functions
($\gamma,\kappa$), where $\gamma$ is a \emph{subjective representation scheme} and $\kappa$ is
an \emph{action selection function} (see figure~\ref{fig:ChoiceMechanism}).



\noindent This means that objective and regret types are behaviorally indistinguishable, as long as both hold precise beliefs. 
%This will be central in understanding later results. 
On the other hand, if paired with imprecise beliefs, different behavioral predictions
arise.

In the following, we assume that agents are maximally uncertain, so that precise types have a
(singleton set containing a) uniform probability measure
$\overline{\mu} \in \Delta(\Acts_{-i})$ and imprecise types entertain all probabilistic beliefs
about the co-players' behavior $\overline{\Gamma} = \Delta(\Acts_{-i})$. In that case, the
considered choice mechanisms are behaviorally equivalent to classic choice rules, as summarized
in table~\ref{tab:CMs}. This means that most of the following results which we interpret as results about the evolutionary fitness of different subjective representations are also relevant for a comparison of the ecological value of classic choice
rules.

\fi




\section{Subjective Utilities, Beliefs and Decision Rules} \label{sec:subj-ut,bel,dec-rules}


We view a choice mechanism as the combination of three different things: a \textit{subjective utility} (or preference), a \textit{subjective belief} and a \textit{decision rule}. In general, the agent's action choice will depend both on the agent's utility at different possible outcomes of the choice situation and on the agent's beliefs about the realization of these outcomes. The decision rule then combines the agent's subjective utility and belief, and dictates how the agent should act in case of a choice situation whose consequences are uncertain. Therefore, a decision rule is a function that associates an action choice with the agent's utility and beliefs:
$$ \text{Decision Rule: Utility }\times\text{ Beliefs } \rightarrow \text{ Actions.}  $$
The utility of an agent can be formally expressed by a function $u:W \times A \rightarrow \mathbb{R} $, where $A$ stands for a (finite) set of actions available to the agent and $W$ is a (finite) set of possible states of the world. There are many different ways to describe beliefs, but here we assume that the agent's beliefs are represented in terms of a (possibly singleton) set of probability functions $\Gamma \subseteq \Delta(W) $ over the possible states of the world. In line with this definition, we can consider different decision rules, such as:
\begin{enumerate}

\item \textbf{Maxmin}: $$ a^*(u,\Gamma)= \argmax_{a \in A} \min_{\mu \in \Gamma} \sum_{w\in W} u(w,a)  \mu(w).$$

\item \textbf{Maximax}: $$ a^*(u,\Gamma)= \argmax_{a \in A} \max_{\mu \in \Gamma} \sum_{w\in W} u(w,a)  \mu(w).$$

\item \textbf{Laplace rule}: $$ a^*(u,\Gamma)= \argmax_{a \in A} \sum_{w\in W} \frac{1}{|W|}  u(w,a) .$$

\item \textbf{Expected utility maximization} (for $\Gamma$ singleton): $$ a^*(u,\Gamma)= \argmax_{a \in A} \sum_{w\in W} u(w,a)  \mu(w).$$

\end{enumerate}

It is worth noticing that both maxmin and maximax boil down to expected utility maximization when the set  $\Gamma$ is a singleton, and in turn expected utility maximization reduces to Laplace rule when the belief $\mu$ is a uniform probability over the states. 
 
\section{Subjective Representations and Evolutionary Fitness} \label{sec:subj-rep,ev-fit}

For a choice mechanism to prescribe an action, the decision rule  needs to be given a specific subjective utility and belief as input, as described in the previous section. We then call \textit{subjective representation} of the decision problem the pair $(u,\Gamma)$ of a subjective utility and a subjective belief.
In other words, a subjective representation is a general way of forming preferences and beliefs about the choice situation.

The literature on evolution of preferences (REFERENCES) mainly focuses on the evolutionary selection of subjective utilities uniquely. Instead, it will be clear from our model that different subjective beliefs may have a fundamental role in determining the evolutionary success of subjective utilities, and therefore the interplay between utilities and beliefs needs to be taken into account explicitly.
So, here we
are most interested in the question which subjective representations are better than others
from an evolutionary point of view. In order to investigate this issue we are going to use the machinery of evolutionary game theory.

A game is an interactive decision situation. 
For a given game $G=\langle N, (A_i,\pi^G_i)_{i \in N} \rangle$, let us denote the evolutionary payoff, or \textit{fitness}, of player $i$ by the function $\pi^G_i: \Pi_{i \in N} A_i \rightarrow \mathbb{R}$, where $A_i$ is player $i$'s (finite) set of actions. For simplicity of exposition we assume that
  all games that are played are symmetric two-player games, that is, games such that: $N:=\{1,2\}$, $A_1 = A_2 $, and $\pi^G_1(a_1, a'_2)= \pi^G_2(a'_1, a_2)=:\pi^G(a,a')$.\footnote{Since payoff functions are symmetric, we will simply write $\pi^G(a, a')$ for $\pi^G_1(a_1, a'_2)$ and $A:=A_1=A_2$, as usual. However, notice that all the definitions and results can be extended to more general cases.} Then, the evolutionary fitness of a subjective representation $(u,\Gamma)$ is measured in terms of the fitness of a
choice mechanism that contains it, all else equal. 
Formally, the fitness of a choice mechanism $c$ against choice mechanism $c'$ in a symmetric two-player game $G=\langle \{1,2\}, A, \pi^G \rangle$ is the
expected payoff of $c$ against $c'$ in $G$:

$$
 F_G(c,c')= \pi^G(a_c^*(u^G,\Gamma),a_{c'}^*(u^G,\Gamma)), \footnote{Whenever a choice mechanism would not select a unique
  action, we assume that the player chooses one of the equally optimal actions at random. I.e., $F_G(c,c')= \sum_{a \in a_c^*(u^G,\Gamma)} \sum_{a' \in a_{c'}^*(u^G,\Gamma)} \frac{1}{|a_c^*(u^G,\Gamma)|} \frac{1}{|a_{c'}^*(u^G,\Gamma)|} \pi^G(a,a')$.}
$$
where $a^*_c$ is the decision rule of choice mechanism $c$. Given the game theoretic setting, the subjective utility $u$ is now a function $u^G:A \times A \rightarrow \mathbb{R}$, and the subjective belief $\Gamma$ is a set of probability functions over the co-player's actions, $ \Gamma \subseteq \Delta(A) $.

However, studying the fitness and the evolutionary selection of different choice mechanisms by the use of a fixed, single game may look like a rather restrictive perspective and possibly biased by the choice of the particular game. To avoid these limitations, we extend the standard single-game framework of evolutionary game theory by considering a \textit{class} of possible games that can be played within a population. Although not completely novel (seeREFERENCES), this approach has started being investigated only very recently, and we maintain that it can shed new light on the evolution of behavior, and it can open to much broader perspectives on evolutionary game theory in general.


\section{The Meta-Game} \label{sec:metagame}

The ground for the evolutionary competition of choice mechanisms will consist of a class $\mathcal{G}$ of possible two-player symmetric games, together with a probability measure $P_{\mathcal{G}}(G)$ for the occurrence probability of game $G \in \mathcal{G}$. In more biological terms, the probability measure $P_{\mathcal{G}}$ encodes the statistical
properties of the environment. We then call \textit{meta-game} the tuple $ MG=\langle CM, \mathcal{G}, P_{\mathcal{G}},F \rangle$, where $CM$ is a set of choice mechanisms, $\mathcal{G}$ is a class of possible games, $P_{\mathcal{G}}(G)$ is the probability of game $G$ to occur, and $F:CM \times CM \rightarrow \mathbb{R}$ is the fitness function, defined as:
\begin{align}
  \label{eq:FittnessChoiceMechGamePairwise}
  F(c, c') = \int P_{\mathcal{G}}(G) \  F_G(c,c') \text{ d} G \,.
\end{align}
Hence, $F(c,c')$ determines the evolutionary payoff of choice mechanism $c$ against $c'$ in the meta-game. The set $CM$ is though of as the set of choice mechanisms present within a given population that is playing the games from the class $\mathcal{G}$. Consequently, it is possible to compute the average fitness of $c$ against the population, that is given by:
\begin{align}
  \label{eq:FittnessChoiceMechGame}
  F(c) = \int P_{M}(c') \ F(c,c') \text{ d} c' = \int \int P_{\mathcal{G}}(G) \  P_{M}(c') \  F_G(c,c') \text{ d} c' \text{ d} G  \,,
\end{align}
where $P_{M}(c')$ is the probability of
encountering a co-player with choice mechanism $c'$.

Meta-games are then abstract models for the evolutionary competition between choice
mechanisms in interactive decision making contexts.
Standard notions of evolutionary game theory apply to meta-games as well. A choice
mechanism $c$ is a strict Nash equilibrium if $F(c,c) > F(c',c)$ for all $c'$; it is
evolutionarily stable if for all $c'$ either (i) $F(c,c) > F(c',c)$ or (ii) $F(c,c) = F(c',c)$
and $F(c,c') > F(c',c')$; it is neutrally stable if for all $c'$ either (i) $F(c,c) > F(c',c)$
or (ii) $F(c,c) = F(c',c)$ and $F(c,c') \ge F(c',c')$
\citep{Maynard-Smith1982:Evolution-and-t}. Similarly, evolutionary dynamics can be applied to
meta-games. We will later turn towards replicator dynamics
\citep{TaylorJonker1978:Evolutionary-St} and replicator mutator dynamics
\citep[e.g.][]{Nowak2006:Evolutionary-Dy}.
%\todo{footnote with caveat about agent-level processes}

\section{The Model} \label{sec:model}

At first, the main subjective utilities that are going to compete against each other for the survival of the fittest are essentially two:
\begin{enumerate}
\item the \textit{objective} utility, defined by: for all $G \in \mathcal{G}$,
$$\text{obj}^G(a,a')=\pi^G(a,a');$$
\item the \textit{regret}, defined by: for all $G \in \mathcal{G}$,
$$\text{reg}^G(a,a') =\pi^G(a,a')- \max_{a''\in A} \pi^G(a'',a').$$
\end{enumerate}

For a start, the subjective beliefs that we take into consideration are also two:
\begin{enumerate}
\item $prc$: a precise uniform belief $\overline{\mu}$ such that $\overline{\mu}(a)=\frac{1}{|A|}$ for all $a\in A$;
\item $imp$: a maximally imprecise belief $\overline{\Gamma}=\Delta(A)$.
\end{enumerate}

Although a thorough discussion of this issue goes beyond the scope of this work, let us say that these two kinds of belief underlie two different and alternative views on uncertainty. In front of uncertain events, a strict Bayesian will always form a precise belief, specified by a \textit{single} probability $\mu$. In the absence of any information about future uncertain events, the Bayesian would then invoke the \textit{principle of insufficient reason}, and accordingly choose a uniform probability over the possible outcomes. On the contrary, more recently many authors (REFERENCES) argued against the obligation of representing a belief by means of a single probability measure, opposite to the Bayesian prescriptions. They argue instead in favor of a more general account, according to which uncertainty can be unmeasurable, and represented by a (closed and convex) set of probabilities. This line of thought has been originated in decision theory by the famous Ellsberg's paradoxes (REF), and appears extremely relevant
in game-theoretical contexts too. Indeed, in a recent paper \citet{BattCerrMM15} write:

\begin{quote}
  Such uncertainty is inherent in situations of strategic interaction. This is quite obvious
  when such situations have been faced only a few times. (p.~646)
\end{quote}

\noindent In evolutionary game theory, for instance, players will obviously face
uncertainty about the composition of the population that they are part of, and consequently
about the co-player that they are randomly paired with at each round and about the co-player's
action. In case of complete lack of information, a non-Bayesian player would thus entertain maximal unmeasurable uncertainty, i.e., a maximally imprecise belief. 
As already anticipated,  we will see that the way agents can form beliefs, and the possibility of holding imprecise beliefs in particular, may have a fundamental impact on their evolutionary success.

As for the decision rule, we assume that the players use the maxmin rule. This is in line with most of the representation
results of decision making under uncertainty \citep[e.g.,][]{gilsch89,KlibMarMuk05,GhirMar02},
and seems justified by empirical observations too. Ellsberg's paradoxes are prominent examples (\citet{ells61}), and evidence from experimental literature suggests that agents are mostly uncertainty averse \citep[e.g.,][]{TrautKuil16}.

Finally, note that when the maxmin rule acts on subjective representation (obj, imp), the generated behavior corresponds to the classic maxmin (REF:vNM). When the maxmin rule acts on subjective representation (reg, imp), the agent's behavior is known as \textit{regret minimization}.
The notion of regret in decision theory dates back at least to the work by
\citet{Savage1951:The-theory-of-s}, and has later been developed by \citet{bell82}, \citet{fish82} and \citet{loosug82} independently. Recently,
\citet{HalpernPass2012:Iterated-Regret} showed how the use of regret minimization can give
solutions to game theoretical-puzzles (like the Traveller's dilemma and the Centipede game) in a way that is closer to
everyday intuition and empirical data. In this paper the notion of regret defined earlier is the same as in
\citet{HalpernPass2012:Iterated-Regret}. Two facts stem from these observations.
The first is related to our focus on different types of uncertainty that players may entertain.
\begin{fact} \label{fact:maxEU-minReg} 
%Under an identical precise (singleton set) belief, the
%  acts selected by maximization of expected evolutionary payoff in game $G$ are exactly the acts selected
%  by the maximization of expected utility for the negative regret transformation of $G$
Under any identical precise (Bayesian) belief, maximization of expected evolutionary payoff and minimization of expected regret are behaviorally equivalent. %\citep[e.g.,][]{HalpernPass2012:Iterated-Regret}.
\end{fact}
\noindent The second fact highlights another behavioral equivalence, which we will make use of shortly in the following section.
\begin{fact} \label{fact:equivalence2x2} In the class of $2 \times 2$ symmetric games, the acts
  selected by Laplace rule are exactly the acts selected by regret minimization.
\end{fact} 

\subsection{Example}
Consider the coordination game in evolutionary payoffs $G$ depicted in figure \ref{coordgame1}. Since the game is symmetric, it suffices to specify the payoffs for the row player. Figure \ref{coordgame1} also represents the objective utility $\text{obj}^G$, since $\text{obj}^G= \pi^G$ by definition, whereas figure \ref{coordgame1reg} pictures the representation of $G$ in terms of regret-based utilities. While classic maxmin is indifferent
between $I$ and $II$ (figure \ref{coordgame1}), regret minimization uniquely selects $II$ (figure \ref{coordgame1reg}).
\begin{figure}
\begin{center}
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 1 & 0 \\
      II & 0 & 2\\
      \bottomrule
    \end{tabular}
    \caption{Coordination game $G$.}
    \label{coordgame1}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 0 & -2 \\
      II & -1 & 0\\
      \bottomrule
    \end{tabular}
    \caption{Regret-based representation of $G$.}
    \label{coordgame1reg}
  \end{subfigure}
  \caption{A coordination game (left) and the associated regret representation (right).}
    \label{coordgame1mainFig}
    \end{center}
\end{figure}


\section{Results}\label{sec:results}

\subsection{Simulation Results}
\label{sec:simulation-results}

Since we keep the decision rule fixed, a player's choice mechanism will only depend on the player's subjective representation $ (u,\Gamma) $. For brevity, from now on we will refer to the pair $ (u,\Gamma)$, like $(\text{reg}, \text{imp})$ or $(\text{obj}, \text{prc})$, as the \textit{type} of the player. Sometimes we will also name types by referring to the subjective utility, for instance $(\text{reg}, \text{imp})$ and $(\text{reg}, \text{prc})$ are regret types.

As we noticed, meta-games factor in statistical properties of the environment. For particular empirical
purposes, one would consult a specific class of games $\mathcal{G}$ with appropriate,
empirically informed probabilities $P_{\mathcal{G}}$ in order to match the natural environment of a given
population. 
%But for our theoretical purposes we adopt another approach. 
For our theoretical purposes, let $\mathcal{G}$ be a set of
symmetric two-player games with two acts for a start. A game is then individuated solely by its payoff
function, i.e., by a quadruple of numbers $G=(a,b,c,d)$. As for the occurrence probability $P_{\mathcal{G}}(G)$ of
game $G$, we imagine that the values $a,b,c,d$ are i.i.d.~random variables sampled from the set $ \lbrace 0, \dots, 10 \rbrace$ according to uniform probability $P_{V}$. The probability distribution of games is thus the fourfold product of $P_{V}$, that is, $P_{\mathcal{G}}=P_{V}^{4}$. Using Monte Carlo simulations, we can then
approximate the values of equation~(\ref{eq:FittnessChoiceMechGamePairwise})
to construct meta-game payoffs. Results based on $100,000$ randomly sampled games are given in
table~\ref{tab:ExpectedFitness_4Types}.\footnote{Concretely, $100,000$ games were sampled
  repeatedly by choosing independently four integers between 0 and 10 uniformly at random. For
  each game, the action choices of all four choice mechanisms were determined and payoffs from
  all pairwise encounters recorded. The number
  in each cell of table~\ref{tab:ExpectedFitness_4Types} is the average payoff for the choice
  mechanism listed in the row when matched with the choice mechanism in the column.}

\begin{table}[t]
\centering
\begin{tabular}{ccccc}
  \toprule
 & $\text{reg}, \text{imp}$ 
 & $\text{obj}, \text{imp}$ 
 & $\text{reg}, \text{prc}$ 
 & $\text{obj}, \text{prc}$ \\ 
  \midrule
  $\text{reg}, \text{imp}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
  $\text{obj}, \text{imp}$ & 6.486 & 6.484 & 6.486 & 6.486 \\ 
  $\text{reg}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\  
  $\text{obj}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
   \bottomrule
\end{tabular}                    
\caption{Average evolutionary fitness from Monte Carlo simulations of 100,000 symmetric $2 \times 2$ games.}
\label{tab:ExpectedFitness_4Types}
\end{table}

Simulation results obviously reflect Fact~\ref{fact:equivalence2x2} in that all encounters in
which types $(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ or
$(\text{obj}, \text{prc})$ are substituted for one another yield identical results. More
interestingly, table~\ref{tab:ExpectedFitness_4Types} shows that $(\text{obj}, \text{imp})$,
the maxmin strategy, is strictly dominated by the three other types: in each column (i.e.,
for each kind of co-player), the maxmin strategy is strictly worse than any of the other three
competitors. This has a number of interesting consequences.

If we restrict attention to subjective representations with imprecise beliefs only, then a monomorphic state in which every agent has
regret-based preferences is the only \emph{evolutionarily stable state}. More strongly, since
$(\text{obj}, \text{imp})$ is strictly dominated by $(\text{reg}, \text{imp})$, we expect
selection that is driven by (expected) fitness to invariably weed out maxmin players $(\text{obj}, \text{imp})$ in favor of $(\text{reg}, \text{imp})$, regret minimization. 
In terms of choice rules, this means that regret
minimization is evolutionarily better than maxmin, over the class of games considered here. In terms of subjective preferences, it shows that players using the objective representation that directly looks at fitness (possibly money, or profit) are outperformed by non-veridical (regret) representations, when players' beliefs are imprecise.

Next, if we look at the competition between all four types represented in
table~\ref{tab:ExpectedFitness_4Types}, $(\text{reg}, \text{imp})$ is no longer evolutionarily
stable. Rather, given behavioral equivalence (Fact~\ref{fact:equivalence2x2}), types
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$, and $(\text{obj}, \text{prc})$ are all
\emph{neutrally stable} \citep{Maynard-Smith1982:Evolution-and-t}. But since
$(\text{obj}, \text{imp})$ is strictly dominated and so disfavored by fitness-based selection,
we are still drawn to conclude that maxmin behavior is weeded out in favor of a population with
a random distribution of the remaining three types.

Simulation results of the (discrete time) \emph{replicator dynamics}
\citep{TaylorJonker1978:Evolutionary-St} indeed show that random initial population
configurations are attracted to states with only three player types:
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. The
relative proportions of these depend on the initial shares in the population. This variability disappears if
we add a small mutation rate to the dynamics. Take a fixed, small mutation rate $\epsilon$ for
the probability that a player's subjective utility \emph{or} her subjective belief changes to another
utility or belief. The probability that a player's subjective representation randomly mutates into a
completely different representation with altogether different utility and belief would
then be $\epsilon^2$. With these assumptions about ``component-wise mutations'', numerical
simulations of the (discrete time) \emph{replicator mutator dynamics}
\citep{Nowak2006:Evolutionary-Dy} show that already for very small mutation rates almost all
initial population states converge to a single fixed point in which the majority of players have
regret-based utility. For instance, with $\epsilon = 0.001$, almost all initial populations are
attracted to a final distribution with proportions:

\begin{center}
  \begin{tabular}{cccc}
    $(\text{reg}, \text{imp})$ & $(\text{obj}, \text{imp})$ & $(\text{reg},
      \text{prc})$ & $(\text{obj}, \text{prc})$ \\ \hline
    0.289  & 0.021 &   0.398 &    0.289 
  \end{tabular}
\end{center}

What this suggests is that, if biological evolution selects behavior-generating mechanisms, not
behavior as such, it need not be the case that behaviorally equivalent mechanisms are treated
equally all the while. If mutation probabilities are a function of individual components, it can be the case that
certain components of such behavior-generating mechanisms are more strongly favored by a process of random mutation and
selection. This is exactly the case with regret-based preferences. Since regret-based preferences are much better in connection with imprecise
beliefs than veridical preferences are, the proportion of expected regret minimizers,
$(\text{reg}, \text{prc})$, in the attracting state is substantially higher than that of
expected utility maximizers, $(\text{obj}, \text{prc})$, even though these types are
behaviorally equivalent.



\subsection{Analytical Results}
\label{sec:analytical-results}

Results based on the single meta-game in table~\ref{tab:ExpectedFitness_4Types} are not fully general and possibly spoiled by random
fluctuations in the sampling procedure. Fortunately, for the case of $2 \times 2$ symmetric
games, the main result that maxmin types $(\text{obj}, \text{imp})$ are strictly dominated by regret minimizers can
also be shown analytically for considerably general conditions.

\begin{proposition} \label{proposition1}
  Let $\mathcal{G}$ be the class of $2 \times 2$ symmetric games $G=(a,b,c,d)$ generated by i.i.d.~sampling $a,b,c,d$ from a
  set of values with at least three elements in the support. Then,
  $(\text{reg}, \text{imp})$ strictly dominates $(\text{obj}, \text{imp})$ in the resulting
  meta-game.
\end{proposition}

\begin{proof}
All proofs are in Appendix~\ref{sec:proofs}.
\end{proof}

\begin{corollary} \label{corollary1} If we only consider imprecise belief types, $(\text{obj}, \text{imp})$ and
  $(\text{reg}, \text{imp})$, and letting $\mathcal{G}$ be as in Proposition~\ref{proposition1}, then the unique evolutionarily stable state is a monomorphic population of
  $(\text{reg}, \text{imp})$ players.
\end{corollary}

%Obviously, $(\text{reg}, \text{imp})$,
%$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ players will be neutrally stable, as before. 
%and fitness-based selection will tend to weed out maxmin play and leave us with a population of
%arbitrary frequency of $(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
%$(\text{reg}, \text{prc})$ players. 
The result shows that there is support for the main conceptual
point that we wanted to make: 
%non-veridical representations \emph{can}, under
%specific circumstances, persist under evolutionary selection based on objective
%fitness. Moreover,
objective preference representations are not necessarily favored by
natural selection. In particular, under generous circumstances objective preferences are outperformed by non-veridical regret preferences, if agents may have imprecise beliefs.
This
tells us that the main conclusions drawn in the previous section based on the approximated meta-game of table~\ref{tab:ExpectedFitness_4Types} hold more generally 
for
arbitrary $2 \times 2$ symmetric games with i.i.d. sampled payoffs.

Even more in general, we can show that the same result obtains for any imprecise belief, not only for maximally uncertain players. 
Let the uncertainty held by a player be defined by a closed and convex set of probabilities $ [s,t] \subseteq \Delta(A) $ over the co-player's actions, where $s$ is the lower probability and $t$ is the upper probability of action $II$.
We can then prove the following proposition, which
is the equivalent of Proposition~\ref{proposition1} for any possible (not necessarily
maximal) degree of uncertainty $[s,t]$, with $s \neq t$. There is only one difference: we are
now going to require i.i.d. drawing of a \emph{continuous} random variable. This is due
to the fact that, for arbitrarily small intervals $[s,t]$, objective players $([s,t],\text{obj})$
and regret players $([s,t],\text{reg})$ can behave as holding a unique probability measure
(precise belief) if the underlying payoff space is not dense. The reason for this technical requirement
will become clearer from the proof.

\begin{proposition} \label{proposition2}
In the class of $2\times2$ symmetric games generated by i.i.d. drawing
of a continuous random variable that takes values in a real interval
$(\underline{r},\overline{r})$ with probability density $P_V$, for
any imprecise belief $[s,t]$, the only
evolutionarily stable state of a population with regret players $([s,t],\text{reg})$
and objective players $([s,t],\text{obj})$ is a monomorphic state of $([s,t],\text{reg})$
players.
\end{proposition}



\section{Extensions}
\label{sec:extensions}

How do the basic results from the previous section carry over to richer models?
Section~\ref{sec:more-types} first introduces further conceptually interesting subjective representations
that have been considered in the literature. Section~\ref{sec:n-times-n} then addresses the
case of $n \times n$ games for $n \ge 2$. Finally, Section~\ref{sec:solitary-decisions} ends
with a brief comparison to the case of solitary decision making.

\subsection{More Representations}
\label{sec:more-types}


Beyond objective and regret-based utilities, other subjective preferences have been investigated, especially in behavioral economics and in evolutionary game theory. A
famous example is the \textit{altruistic} preference \citep[e.g.,][]{Beck76,BestGuth98}, summoned to
explain the possibility of altruistic behavior. At the other end of the spectrum, the \emph{competitive}
preference is located. The two subjective utilities are defined as follows.
\begin{enumerate}
\item \textit{altruistic} utility: for all $G \in \mathcal{G}$, $\text{alt}^G(a,a') = \pi^G(a,a') + \pi^G(a',a)$;\footnote{A
    more general formulation would be to define $ \alpha$-altruistic utility, for
    $\alpha \in [0,1]$,
    $ u^G_\alpha(a, a')=\pi^G(a,a') + \alpha \pi^G(a',a)$. Since we are not
    interested in the evolution of degrees of altruism, here we simply fix $ \alpha = 1 $. Analogously for $\alpha$-competitive utilities too.}
\item \textit{competitive} utility: for all $G \in \mathcal{G}$, $\text{com}^G(a,a') = \pi^G(a,a') - \pi^G(a',a)$.
\end{enumerate}


\begin{table}[]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
  \hline
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{com}, \text{imp})$
 & $(\text{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\text{com}, \text{prc})$
 & $(\text{alt}, \text{prc})$ \\ 
  \hline
  $(\text{reg}, \text{imp})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{imp})$ & 6.486 & 6.484 & 6.088 & 6.703 & 6.486 & 6.486 & 6.088 & 6.875 \\
  $(\text{com}, \text{imp})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\text{alt}, \text{imp})$ & 5.949 & 5.722 & 5.326 & 6.396 & 5.949 & 5.949 & 5.326 & 6.568 \\
  $(\text{reg}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{com}, \text{prc})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\text{alt}, \text{prc})$ & 6.331 & 5.893 & 5.497 & 6.566 & 6.331 & 6.331 & 5.497 & 7.152 \\
   \hline                          
\end{tabular}    
}                  
\caption{Average evolutionary fitness from Monte Carlo simulations of 100,000 symmetric $2 \times 2$ games.}
\label{tab:ExpectedFitness_2x2_Full}        
\end{table}   
 
Table~\ref{tab:ExpectedFitness_2x2_Full} shows results of Monte Carlo simulations that
approximate the expected fitness in the relevant meta-game with all the subjective representations considered so
far. These results confirm basic intuitions about altruistic and competitive types: everybody would like to have an altruistic co-player and nobody
would like to play against a competitive player. Perhaps more surprisingly, $(\text{alt}, \text{imp})$
come up strictly dominated by $(\text{com}, \text{imp})$, but competitive types themselves are worse off
against all types except against maxmin players $(\text{obj}, \text{imp})$ than any of
the behaviorally equivalent types $(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
$(\text{reg}, \text{prc})$.
\iffalse This is a noteworthy results in the light of the fact
that evolving altruistic preferences have been shown to support cooperative behavior in a
single stage game \myalert{[CITE]}. In contrast, averaging over payoffs in multiple stage
games, like we do here, makes altruistic preferences prime victims of evolutionary eradication.
\fi
It is easy to see then that the previous results still obtain for the larger meta-game in
table~\ref{tab:ExpectedFitness_2x2_Full}: $(\text{reg}, \text{imp})$,
$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ are still neutrally stable;
simulation runs of the (discrete-time) replicator dynamics on the $8 \times 8$ meta-game from
table~\ref{tab:ExpectedFitness_2x2_Full} end up in population states consisting of only these
three types in arbitrary proportion.

In sum, the presence of other subjective representations, such as those based on altruistic or competitive utilities, does not undermine, but rather strengthens our previous results.


                                   
\subsection{More Actions}
\label{sec:n-times-n}

Results from Section~\ref{sec:basic-model-1} relied heavily on Fact~\ref{fact:equivalence2x2}, that
is no longer true when we look at arbitrary $n \times n$ games. 
Table~\ref{tab:ExpectedFitness_10x10} gives approximations of expected fitness in the class of
$n \times n$ symmetric games. Concretely,
the numbers in table~\ref{tab:ExpectedFitness_10x10} are averages of evolutionary payoffs obtained in
100,000 randomly sampled symmetric games, where each game was sampled by first picking a number of acts
$n \in \set{2, \dots, 10}$ uniformly at random, and then filling the necessary $n \times n$
payoff matrix with i.i.d.~sampled numbers, as before.


\begin{table}[]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
  \toprule
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{com}, \text{imp})$
 & $(\text{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\text{com}, \text{prc})$
 & $(\text{alt}, \text{prc})$ \\ 
  \midrule
  $(\text{reg}, \text{imp})$ & 6.567 & 6.570 & 5.650 & 6.992 & 6.564 & 6.564 & 5.593 & 7.409 \\
  $(\text{obj}, \text{imp})$ & 6.476 & 6.483 & 5.896 & 6.818 & 6.484 & 6.484 & 5.850 & 7.124 \\
  $(\text{com}, \text{imp})$ & 6.468 & 6.647 & 5.512 & 7.169 & 6.578 & 6.578 & 5.577 & 7.354 \\
  $(\text{alt}, \text{imp})$ & 5.968 & 5.923 & 5.363 & 6.685 & 5.975 & 5.975 & 5.086 & 6.973 \\
  $(\text{reg}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{obj}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{com}, \text{prc})$ & 6.529 & 6.680 & 5.445 & 7.276 & 6.542 & 6.542 & 5.521 & 7.440 \\
  $(\text{alt}, \text{prc})$ & 6.450 & 6.337 & 5.772 & 6.978 & 6.457 & 6.457 & 5.479 & 7.500 \\
   \bottomrule                         
\end{tabular}  
}                    
\caption{Average evolutionary fitness for 100,000 randomly generated $n \times n$ symmetric games with $n$ randomly drawn from $\set{2, \dots, 10}$.}
\label{tab:ExpectedFitness_10x10}        
\end{table}

The most important result is that the regret minimizing type $(\text{reg}, \text{imp})$ is
strictly dominated by $(\text{reg}, \text{prc})$ and by $(\text{obj}, \text{prc})$ in the
meta-game from table~\ref{tab:ExpectedFitness_10x10}. This means that while simple regret minimization
can thrive in some evolutionary contexts, there are also contexts where it is
demonstrably worse off. While this may be bad news for regret minimizing types
$(\text{reg}, \text{imp})$, it is not the case that regret types \emph{as such} are weeded out
by selection. Since, by Fact~\ref{fact:maxEU-minReg}, $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are behaviorally equivalent in general, it remains that selection
based on meta-games constructed from $n \times n$ games will still not eradicate regret preferences. %So, although regret types may not be selected for in this case, they are also not selected against.

On the other hand, there are plenty of ways in which the basic insight from
Propositions~\ref{proposition1} and \ref{proposition2} that regret-based utilities are strictly better than
veridical objective utilities in case of imprecise beliefs can make for situations in which
evolution would select for regret types exclusively, even in $n \times n$ games. If, for
example, the belief of a player is a trait that biological evolution has no bite on, but rather
something that the particular choice situation would exogenously give us (possibly because of the different amount of information available in different choice situations), then regret-based
preferences can again drive out veridical preferences altogether. For example,
suppose that only preferences compete and that agents' beliefs are exogenously given,
in such a way that both players hold precise (Bayesian) uniform beliefs with probability $p$ and they both have maximally imprecise
beliefs otherwise. This transforms the meta-game from table~\ref{tab:ExpectedFitness_10x10} into a
simpler $4 \times 4$ meta-game in which the payoff obtained by a subjective utility is the weighted
average over the payoffs of the subjective representations including that utility in
table~\ref{tab:ExpectedFitness_10x10}. Setting $p = 0.98$ for illustration, we get the meta-game in table~\ref{tab:ExogeneousEpistemics}.
\begin{table}[]
\centering
\begin{tabular}{ccccc}
  \toprule
  & $\text{reg}$ 
  & $\text{obj}$
  & $\text{com}$
  & $\text{alt}$ \\ 
  \midrule
  $\text{reg} $ & 6.926 & 6.926 & 5.942 & 7.757 \\ 
  $\text{obj} $ & 6.924 & 6.924 & 5.948 & 7.751 \\ 
  $\text{com }$ & 6.566 & 6.570 & 5.481 & 7.434 \\ 
  $\text{alt} $ & 6.463 & 6.461 & 5.478 & 7.469 \\ 
   \bottomrule
\end{tabular}
\caption{Meta-game for the evolutionary competition between subjective utilities when beliefs are exogenously
  given (see main text).}
\label{tab:ExogeneousEpistemics}
\end{table}
The only evolutionarily stable state of this meta-game is again a monomorphic population of regret types. Accordingly, all
our simulation runs of the (discrete-time) replicator dynamics converge to monomorphic
regret-type populations. The reason why regret-based utilities prosper is because they have a substantial
fitness advantage when paired with imprecise beliefs (Propositions~\ref{proposition1} and \ref{proposition2}). If unmeasurable uncertainty is exogenously given
as something that happens to agents because of the information available in some choice situations, and even if that happens only very infrequently
(i.e., for rather low $p$), regret preferences will outperform objective preferences, as
well as competitive and altruistic preferences.


\subsection{Solitary Decisions}
\label{sec:solitary-decisions}

To see how different choice mechanisms behave in evolutionary competition based on solitary
decision making, we approximated, much in the spirit of meta-games, average accumulated
fitness obtained in randomly generated solitary decision problems. For our purposes, a decision
problem $D=\tuple{W, \Acts, \pi^D}$ consists of a set of states of the world $W$, a set of acts $\Acts$, and
a payoff function $\pi^D: W \times \Acts \rightarrow \mathbb{R}$.  We generate arbitrary
decision problems by selecting, uniformly at random, numbers of states and acts
$n_w, n_\act \in \set{2, \dots, 10}$ and then filling the utility table, so to speak, by
i.i.d.~samples for each $\pi^D(w, \act) \in \set{0, 10}$. Unlike with
two-player games, we need to also sample the actual state of the world, which we selected
uniformly at random from the available states in the current decision problem. Accordingly, the fitness of choice mechanism $c$ in decision problem $D$ is given by:
$$F_D(c)= \pi^D(w,a_c^*(u^D,\Gamma))\mu(w).$$
As subjective representations,
we considered the original cast of four from table~\ref{tab:ExpectedFitness_4Types}, since altruistic and
competitive types are meaningless in solitary decision situations. As before, the
relevant fitness measure, defined in equation~(\ref{eq:FittnessChoiceMechSolitary}), was
approximated by Monte Carlo simulation, the results of which are given in
table~\ref{tab:SolitaryDecisions}.
\begin{align}
  \label{eq:FittnessChoiceMechSolitary}
  F(c) = \int P_{\mathcal{D}}(D) \  F_D(c) \text{ d} D 
\end{align}

\begin{table}
  \centering
  \begin{tabular}{cccc}
    \toprule
   $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 \\ \midrule
    6.318 & 6.237 & 6.661 & 6.661 \\ \bottomrule
  \end{tabular}
  \caption{Expected fitness of choice mechanisms approximated from 100,000 simulated solitary
    decision problems (see main text).}
  \label{tab:SolitaryDecisions}
\end{table}

Facts~\ref{fact:maxEU-minReg} and \ref{fact:equivalence2x2} still apply:
$(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$ are behaviorally equivalent in
general, and $(\text{reg}, \text{imp})$ is behaviorally equivalent to the former two in
decision problems with two states and two acts. This shows in the results from
table~\ref{tab:SolitaryDecisions} in that the averages for $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are identical. But since we included decision problems with more
acts and more states as well, the average for regret minimizers $(\text{reg}, \text{imp})$ is
not identical to the one of $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. It
is, in fact, lower, but again not as low as that of $(\text{obj}, \text{imp})$.

This means that every relevant result we have seen about game situations is also borne out for
solitary decisions. Evolutionary selection based on objective fitness will not select against
regret preferences, as these are indistinguishable from veridical preferences
when paired with precise beliefs. But when paired with imprecise beliefs, regret-based
utilities outperform objective utilities. Consequently, if there is a chance, however
small, that agents fall back onto imprecise beliefs, evolution will actually positively select
for non-veridical regret-based preferences.





\iffalse
\subsection{Arbitrary probabilistic beliefs}
\label{sec:arbitr-prob-beli}

So far, we have assumed that epistemic types $\text{prc}$ hold an unbiased, flat belief. It is
worthwhile considering what happens when this assumption is relaxed and we allow agents with
probabilistic beliefs to make use of the full spectrum of probabilistic beliefs. It should be
clear that quality of beliefs directly impacts prospects of successful decision making: if a
decision maker knows the actual state, or can put a high level of credence on the actual state,
accumulated fitness can be expected to be high. We see this, unsurprisingly, in numerical
simulations. We looked at the average payoff accumulated in 100,000 decision problems sampled
as before, except that for belief types with probabilistic beliefs we sampled a random
probabilistic belief $p$ (from an unbiased Dirichlet distribution) and chose the actual world
state with probability weights $p$. This effectively implements a bias for beliefs of the
decision maker that tend to put more weight on the actual state (although the procedure does
admittedly perverse the natural chicken-and-egg logic in this case that the actual world state
should come first and beliefs be a function of it). As a result, the average payoffs of types
$\tuple{\text{reg}, \text{prc}}$ and $\tuple{\text{obj}, \text{prc}}$ are still the exact same
(because we compare what agents with different preference types would do under the same
beliefs; we are not interested in statistical fluctuations given one type better beliefs by
pure happenstance), but with $7.125$ notably higher than before. In effect, better
probabilistic beliefs lead to better decisions. If agents can learn, reason and extrapolate to
form better probabilistic beliefs, that will help them whenever they take their beliefs into
account. 

But this is orthogonal to the arguments in this paper, where the focus is on possibilities for
the evolution of non-veridical preferences. We know from Fact~\ref{fact:maxEU-minReg} that
regret types and veridical preference types, when paired with probabilistic belief types that
maximize/minimize expectations, are behaviorally equivalent, \emph{no matter what they believe},
as long as they belief the same. So, if learning, insight and statistical knowledge of a
recurrent situation \emph{can} be brought to bear, this will not make evolution select against
regret-based preferences. If, on the other hand, agents have no basis for probabilistic
beliefs, then we have shown that there are general and basic circumstances in which
regret-based preferences can actually be selected, despite their non-veridicality.
\fi


\section{Related Literature}

do you think we really need a section for this?


\section{Conclusion} \label{sec:conclusion}


The assumption that players and decision makers maximize their (subjective) preferences is central through all economic literature, and the maximization of actual (objective) payoffs is often justified by appealing to evolutionary arguments and natural
selection. In contrast to the standard view, we showed the
existence of player types with subjective utilities different
from the actual evolutionary payoffs that can outperform types whose
subjective utilities coincide with the evolutionary payoffs.
While the literature on evolution of preferences has focused
on fixed games, we have adopted a
more general approach here. We suggested that attention
to “meta-games” is crucial, because what may be a good
subjective representation in one type of game (e.g., cooperative preferences in the Prisoner’s Dilemma class) need not
be generally beneficial. Taken together, we presented a variety of plausible circumstances in which evolutionary competition between choice
mechanisms on a larger class of games can favor non-veridical preference representations focusing on regret.

Meta-games are useful tools for probing into conceptual questions about ecological
rationality. The approach is rich and general, with many possible further
applications imaginable beyond the case study presented here. For example, the evolutionary
investigation of different risk and ambiguity attitudes when competing against each other may bring new insights on both behavioral and theoretic
decision making under uncertainty, as well as on many issues in theoretical biology and
behavioral ecology. Moreover, variability in the action selection function has been entirely
ignored in this paper. Finally, while this paper only used dummy models of diverse decision
environments, the meta-game approach also allows for empirically informed models of rich and
variable environments.


\appendix

\iffalse
\section{Background on Imprecise Probabilities}
\label{sec:impr-prob-beli}

Consider the following problem. I have, on the desk in front of me, a bag containing either red
or black marbles. There is no further information about the distribution of the two
colors. What is then the probability that I will draw a red marble? As in \citet{walley96}, we
are not talking about physical probability, but rather about epistemic probability, i.e., the
subjective probability that an agent attributes to an event.  In the literature there are two
traditional ways of answering this. According to standard Bayesianism, one should always be
able to determine a precise betting rate, that immediately translates into a precise
probability distribution. In the case
of the bag on my desk, a strict Bayesian, in the absence of any further information, would
probably rely on the \textit{principle of insufficient reason} and answer that the DM should
have a uniform measure that considers all the outcomes equipossible.


The second answer involves imprecise probabilities and is related to the literature about
unmeasurable uncertainty. Many authors argued in favor of this kind of approach
\citep[e.g.,][]{levi74,gardsah82,walley96}. Since it is consistent with the information
available that all the marbles are red, or that no marble is red, the resulting model should
take it into account and specify a lower probability of red $\underline{P}(R)$ and an upper
probability of red $\overline{P}(R)$ that define an interval of possible probabilistic beliefs
[$\underline{P}(R), \overline{P}(R)$]. Given the information available in this case we would
have $\underline{P}(R)=0$ and $\overline{P}(R)=1$. This approach appears particularly relevant
in a game theoretical context. Indeed, in a recent paper \citet{BattCerrMM15} write:

\begin{quote}
  Such uncertainty is inherent in situations of strategic interaction. This is quite obvious
  when such situations have been faced only a few times. (p.~646)
\end{quote}

\noindent In evolutionary game theory, for instance, players in a population obviously face
uncertainty about the composition of the population that they are part of, and consequently
about the co-player that they are randomly paired with at each round and about the co-player's
action. 

We therefore represent maximal uncertainty in two different ways that correspond to the two
approaches discussed in the main text: a Bayesian uniform measure and an imprecise probability
set. The latter represents uncertainty for players who are not able to narrow down the set of
probability measures to a single one and they consider all of them as possible. The former
precise belief type conceptualizes the uncertainty about the population by using the principle
of insufficient reason and ascribes equal probability to all the possible alternatives. As we
have seen, both options are reasonable and justifiable ways to deal with situations of
uncertainty.

In decision theory, the imprecise probability model is in line with most of the representation
results of decision making under uncertainty \citep[e.g.,][]{gilsch89,KlibMarMuk05,GhirMar02},
and seems justified by empirical observations too. A prominent example is Ellsberg's paradox
(\citet{ells61}). A bag contains 90 marbles, 30 are red and the remaining 60 are either black
or yellow. A marble is to be drawn and DM can win 100€ if she guesses the color of the drawn
marble. DM is offered two different bets. In the first one she can choose to bet either on
red (R) or on yellow (Y), while in the second she can bet either on red or black (RB) or on
yellow or black (YB), as shown in table \ref{Ellsberg}.

\definecolor{yellow}{RGB}{255,188,1}


\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\cmidrule(l){2-4}
\multicolumn{1}{c}{} & {\color{red}R}   & {\color{yellow}Y}   & B   \\ \cmidrule(l){2-4} 
$f_{{\color{red}R}}$              & 100 & 0   & 0   \\
$f_{{\color{yellow}Y}}$              & 0   & 100 & 0   \\
$f_{{\color{red}R}B}$            & 100 & 0   & 100 \\
$f_{{\color{yellow}Y}B}$             & 0   & 100 & 100 \\ \bottomrule
\end{tabular}
\caption{Ellsberg's paradox}
\label{Ellsberg}
\end{table}

The pattern consistently observed in experiments is:
$f_{{\color{red}R}} \succ f_{{\color{yellow}Y}}$ and
$f_{{\color{red}R}B} \prec f_{{\color{yellow}Y}B}$. This is clearly incompatible with any
precise probabilistic belief about the proportion of black and yellow marbles. This kind of
behavior has been famously axiomatized by \citet{gilsch89}. The
choice pattern is normally explained in terms of aversion to unmeasurable uncertainty, and the standard
representation is given by means of a set of probability measures together with maxmin rule. We
can think of DM in Ellsberg's example as maximinimizing expected utility over the
probability set
$
[\underline{P}(R)=\overline{P}(R)=\frac{1}{3}, \underline{P}(Y)= \underline{P}(B)=0, \overline{P}(Y)= \overline{P}(B)=\frac{2}{3}]
$.
This amounts to calculating for any probability measure contained in the probability set the
expected utility of any available action, and then to choosing the action that guarantees the
highest minimal expected utility. 

Evidence from experimental literature suggests that agents are mostly uncertainty averse \citep[e.g.,][]{TrautKuil16}. In line with empirical data, we assume that players in the population
are uncertainty averse and conform to maxmin expected utility. The resulting behavior is then
produced by maximinimizing the subjective utility given by the player's preference type over
the set of probabilities given by the belief type.

\fi

\section{Proofs}
\label{sec:proofs}

The proof of Proposition \ref{proposition1} relies on a partition of $\mathcal{G}$, and on some
lemmas. For brevity, let us denote the regret minimizer $(\text{reg}, \text{imp})$ by $R$ and
the maximinimizer $(\text{obj}, \text{imp})$ by $M$. Following
equation~(\ref{eq:FittnessChoiceMechGamePairwise}), let $F_{\mathcal{G}}(X,Y)$ denote the
expected payoff of choice mechanism $X$ against choice mechanism $Y$ on the possibly restricted
class of fitness games $\mathcal{G}$.

\vspace{.5cm}


\noindent \textbf{Proof of Proposition \ref{proposition1}.} By definition of strict dominance,
we have to show that in the class $\mathcal{G}$ of symmetric $2\times2$ games with payoffs
sampled from a set of i.i.d. values with at least 3 elements in the support, it holds that:
\begin{multicols}{2}
  \begin{itemize}
  \item[(i)] $F_{\mathcal{G}}(R,R)>F_{\mathcal{G}}(M,R);$
  \item[(ii)] $F_{\mathcal{G}}(M,M)<F_{\mathcal{G}}(R,M).$
  \end{itemize}
\end{multicols}

\noindent To show this we use the following partition of $\mathcal{G}$, based on payoffs parametrized as
follows:
\begin{center}
  \begin{tabular}{ccc}
    \toprule
    & $I$ & $II$ \\ \midrule
    $I$ & $a$ & $b$ \\
    $II$ & $c$ & $d$ \\ \bottomrule
  \end{tabular}
\end{center}
\begin{enumerate}
\item Coordination games $\mathcal{C}$: $a>c$ and $d>b$;
\item Anti-coordination games $\mathcal{A}$: $a<c$ and $d<b$;
\item Strong dominance games $\mathcal{S}$: aut $(a>c$ and $b>d)$
aut $(a<c$ and $b<d)$;
\item Weak dominance games $\mathcal{W}$: aut $a=c$ aut $b=d$;
\item Boring games $\mathcal{B}$: $a=c$ and $b=d$.
\end{enumerate}
Before proving the lemmas, it is convenient to fix some notation. Let us call $x,y,z$ the 3
elements in the support, and without loss of generality suppose that $ x > y > z $.  We denote
by $C$ a coordination game in $\mathcal{C}$ with payoffs $a_{C}$, $b_{C}$, $c_{C}$, and
$d_{C}$; similarly for games $A \in \mathcal{A}$, $S \in \mathcal{S}$, $W \in \mathcal{W}$, and
$B \in \mathcal{B}$.  Let us denote by $I_{RC}$ the event that a $R$-player plays action $I$ in
the game $C$; and similarly for action $II$, for player $M$, and for games $A$, $S$, $W$ and
$B$. We first consider the case of i.i.d. sampling with finite support.

\begin{lemma} \label{lemma:S-B games} $R$ and $M$ perform equally well in $\mathcal{S}$ and in
  $\mathcal{B}$.
\end{lemma}

\begin{proof}
  By definition of regret minimization and maxmin it is easy to check that whenever in a game
  there is a strongly dominant action $a^{\$}$, then $a^{\$}$ is both the maxmin action and the
  regret minimizing action. Then, for all the games in $\mathcal{S}$, $R$ chooses action $a$ if
  and only if $M$ chooses action $a$. Consequently, $R$ and $M$ always perform equally (well)
  in $\mathcal{S}$. In the case of $\mathcal{B}$ it is trivial to see that all the players
  perform equally.
\end{proof}

\begin{lemma} \label{lemma:W games} In $\mathcal{W}$, $R$ strictly dominates $M$.
\end{lemma}

\begin{proof}
  Assume without loss of generality that $b=d$, and that $ a>c $. There are two cases that we have to check:
  (i) $c < b=d$ and (ii) $c \geq b=d$. In the first case it is easy to see that $R$ and $M$
  perform equally: act $I$ is the choice of both $R$ and $M$. In the case of (ii) instead we have that $I$ is the regret minimizing action, whereas both actions have the same minimum and $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$, since both $I$ and
  $II$ maximize the minimal payoff. Consider now a population of $R$ and $M$ playing games from
  the class $\mathcal{W}$.  Whenever (i) is the case $R$ and $M$ perform equally well. But suppose $W \in \mathcal{W}$ and (ii) is the case. Then,
  $\pi_W(R,R)=a>\frac{1}{2}a+\frac{1}{2}c=\pi_W(M,R)$, whereas
  $\pi_W(M,M)=\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d<\frac{1}{2}a+\frac{1}{2}b=\pi_W(R,M)$.
  Hence, we have that in general
  $F_{\mathcal{W}}(R,R)>F_{\mathcal{W}}(M,R),\mbox{ and
  }F_{\mathcal{W}}(M,M)<F_{\mathcal{W}}(R,M)$.
\end{proof}

\noindent Since it is not difficult to see that both $(R,R)$ and $(M,M)$ are strict Nash equilibria in
$\mathcal{C}$, and that $(R,R)$ and $(M,M)$ are not Nash equilibria in $\mathcal{A}$, the main
part of the proof will be to show that $R$ strictly dominates $C$ in the class
$\mathcal{C}\cup\mathcal{A}$, that is:
\begin{multicols}{2}
  \begin{itemize}
  \item[(i')] $F_{\mathcal{C}\cup\mathcal{A}}(R,R)>F_{\mathcal{C}\cup\mathcal{A}}(M,R),$
  \item[(ii')] $F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M).$
  \end{itemize}
\end{multicols}

\noindent This part needs some more lemmas to be proven, but firstly we introduce
the following bijective function $\phi$ between coordination and
anti-coordination games.

\begin{definition}[$\phi$] \label{def:bijection phi} The permutation $\phi(a,b,c,d)=(c,d,a,b)$
  defines a bijective function $\phi:\mathcal{C}\rightarrow\mathcal{A}$ that for each
  coordination game $C\in\mathcal{C}$ with payoffs $(a_{C},b_{C},c_{C},d_{C})$ gives the
  anti-coordination game $A\in\mathcal{A}$ with payoffs
  $(a_{A},b_{A},c_{A},d_{A})=(c_{C},d_{C},a_{C},b_{C})$. Essentially, $\phi$ swaps rows
  in the payoff matrix.
\end{definition}

\begin{lemma} \label{lemma:probabilities coord-ant}
Occurrence probability of $C$ equals that of $\phi(C)$: $P(\phi(C))=P(C)$.
\end{lemma}

\begin{proof}
  By definition, each game $C\equiv(a_{C},b_{C},c_{C},d_{C})$ is such that $a_{C}>c_{C}$ and
  $d_{C}>b_{C}$, and each game $A\equiv(a_{A},b_{A},c_{A},d_{A})$ is such that $a_{A}<c_{A}$
  and $d_{A}<b_{A}$. Given that $a,b,c,d$ are i.i.d. random variables and that a sequence of
  i.i.d. random variables is exchangeable, it is clear that the probability of
  $(a_{C},b_{C},c_{C},d_{C})$ equals the probability of $(c_{C},d_{C},a_{C},b_{C})$.  Hence,
  $P(\phi(C))=P(C)$.
\end{proof}

\begin{lemma} \label{lemma:probabilities actions coord-ant} Let $P(E)$ be the probability of
  event $E$, e.g., $P(I_{RC})$ is the probability that a random $R$-player plays act $I$ in coordination game $C$, which is either 0, $.5$ or 1.  It then holds that:
\begin{itemize}
\item $P(I_{RC})=P(II_{R\phi(C)})$, and $P(II_{RC})=P(I_{R\phi(C)})$;
\item $P(I_{MC})=P(II_{M\phi(C)})$, and $P(II_{MC})=P(I_{M\phi(C)})$.
\end{itemize}
\end{lemma}

\begin{proof}
  It is easy to check that if $b_{C}-d_{C}>c_{C}-a_{C}$, a $R$-player plays action $I$ in $C$;
  that if $b_{C}-d_{C}<c_{C}-a_{C}$, $R$ plays $II$; and that if $b_{C}-d_{C}=c_{C}-a_{C}$, a
  $R$-player is indifferent between $I$ and $II$ in $C$, and so randomizes with
  $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly, if $a_{A}-c_{A}>d_{A}-b_{A}$, a $R$-player plays
  action $I$ in $A$; if $a_{A}-c_{A}<d_{A}-b_{A}$, $R$ plays $II$; and if
  $a_{A}-c_{A}=d_{A}-b_{A}$, a $R$-player is indifferent between $I$ and $II$ in $A$, and
  randomizes with $(\frac{1}{2}I;\frac{1}{2}II)$. Consequently, if $b_{C}-d_{C}>c_{C}-a_{C}$,
  then $P(I_{RC})=1$, and by definition of $\phi$ we have $P(II_{R\phi(C)})=1$. Likewise, if
  $b_{C}-d_{C}<c_{C}-a_{C}$, then $P(II_{RC})=1=P(I_{R\phi(C)})$; and if
  $b_{C}-d_{C}=c_{C}-a_{C}$,
  then $P(I_{RC})=P(II_{RC})=\frac{1}{2}=P(II_{R\phi(C)})=P(I_{R\phi(C)})$. \\
  In the same way, in coordination games we have that if $b_{C}>c_{C}$, a $M$-player plays $I$;
  if $c_{C}>b_{C}$, a $M$-player plays $II$; and if $b_{C}=c_{C}$, $M$ is indifferent between
  $I$ and $II$, and plays $(\frac{1}{2}I;\frac{1}{2}II)$. In anti-coordination games instead,
  if $a_{A}>d_{A}$, $M$ plays $I$; if $a_{A}<d_{A}$, $M$ plays $II$; if $a_{A}=d_{A}$, $M$
  plays $(\frac{1}{2}I;\frac{1}{2}II)$.  By definition of $\phi$:
  $P(I_{MC})=1=P(II_{M\phi(C)})$ if $b_{C}>c_{C}$; $P(II_{MC})=1=P(I_{M\phi(C)})$ if
  $c_{C}>b_{C}$; and $P(I_{MC})=P(II_{MC})=\frac{1}{2}=P(II_{M\phi(C)})=P(I_{M\phi(C)})$ if
  $b_{C}=c_{C}$.
\end{proof}

\begin{lemma} \label{lemma:action implications}
It holds that:
  \begin{itemize}
  \item $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$;
  \item $a^{C}=d^{C}\rightarrow I_{MC}=I_{RC}$.
  \item $a_{C}<d_{C}\rightarrow(II_{MC}\subseteq II_{RC})$;
  \end{itemize}
\end{lemma}

\begin{proof}
The event that $R$ plays action $I$, $I_{RC}$, with positive probability is the event that $b_{C}-d_{C} \geq c_{C}-a_{C}$: if $b_{C}-d_{C}>c_{C}-a_{C}$, $R$ plays $I$, and if $b_{C}-d_{C}=c_{C}-a_{C}$, $R$ plays
  $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly, the event that $I_{MC}$ has positive occurrence is the event that $b_{C} \geq c_{C}$: if $b_{C}>c_{C}$, $M$ plays $I$, and if
  $b_{C}=c_{C}$, $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$. 
Then, $I_{RC}$ implies that
  $b_{C}-d_{C}\geq c_{C}-a_{C}$, and $I_{MC}$ implies that $b_{C}\geq c_{C}$. Moreover, on the
  assumption that $a_{C}>d_{C}$, it is easy to check that $b_{C}\geq c_{C}$ implies
  $b_{C}-d_{C}>c_{C}-a_{C}$.  Hence, in any $C$ with $a_{C}>d_{C}$ it holds that
  $I_{MC}\mbox{ implies }I_{RC}$, i.e., $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$.
Instead, it is possible that $a_{C}>d_{C}$, $b_{C}-d_{C}>c_{C}-a_{C}$ and $b_{C}<c_{C}$ hold
  simultaneously, so that $I_{MC}\nsupseteq I_{RC}$.
By a symmetric argument it can be shown that
  $a_{C}<d_{C}\rightarrow(II_{MC}\subseteq II_{RC})$ too. Finally, when $a_{C}=d_{C}$ it holds
  that: $b_{C}-d_{C}>c_{C}-a_{C}$ iff $b_{C}>c_{C}$; $b_{C}-d_{C}<c_{C}-a_{C}$ iff
  $b_{C}<c_{C}$; and $b_{C}-d_{C}=c_{C}-a_{C}$ iff $b_{C}=c_{C}$. Hence,
  $a_{C}=d_{C}\rightarrow I_{MC}=I_{RC}$.
\end{proof}

\vspace{.5cm}

\noindent We are now ready to prove that
$F_{\mathcal{C}\cup\mathcal{A}}(R,R)>F_{\mathcal{C}\cup\mathcal{A}}(M,R)$. With notation like
$P(I_{RC} \cap I_{RC})$ denoting the probability that a random $R$-player plays $I$ and another
$R$-player plays $I$ as well in game $C$, rewrite the inequality as:
\begin{align*}
   \sum_{C \in \mathcal{C}}P(C) & [P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot
  d_{C}+P(I_{RC}\cap II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C}] 
  \\ + \sum_{A \in
    \mathcal{A}}P(A) & [P(I_{RA}\cap I_{RA})\cdot a_{A}+P(II_{RA}\cap II_{RA})\cdot
  d_{A}+P(I_{RA}\cap II_{RA})\cdot b_{A}+P(II_{RA}\cap I_{RA})\cdot c_{A}]\\
  > \sum_{C \in
    \mathcal{C}}P(C) & [P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot
  d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C}] \\
   + \sum_{A \in
    \mathcal{A}}P(A) & [P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot
  d_{A}+P(I_{RA}\cap II_{MA})\cdot c_{A}+P(II_{RA}\cap I_{MA})\cdot b_{A}]
\end{align*}
\noindent By Lemma \ref{lemma:probabilities coord-ant} and Lemma \ref{lemma:probabilities
  actions coord-ant}, we can express everything in terms of $C$ only:
\begin{align*}
  & \textstyle{\sum_{C}} P(C)[P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot d_{C}+P(I_{RC}\cap
  II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C} + \\
  & \ \ \ \ P(II_{RC}\cap II_{RC})\cdot c_{C}+P(I_{RC}\cap I_{RC})\cdot b_{C}+P(II_{RC}\cap
  I_{RC})\cdot d_{C}+P(I_{RC}\cap
  II_{RC})\cdot a_{C}] \\
  > & \textstyle{\sum_{C}}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot
  d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C} + \\
  & \ \ \ \ P(II_{RC}\cap II_{MC})\cdot c_{C}+P(I_{RC}\cap I_{MC})\cdot b_{C}+P(II_{RC}\cap
  I_{MC})\cdot a_{C}+P(I_{RC}\cap II_{MC})\cdot d_{C}]
\end{align*}
This simplifies to:
\begin{align*}
  & \textstyle{\sum_{C}}P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C}
  \cdot (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) + 
\\ & \ \ \ \ c_{C} \cdot
  (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap
  II_{RC})+P(II_{RC}\cap I_{RC}))] 
\\ > & \textstyle{\sum_{C}}P(C)[a_{C} \cdot (P(I_{RC}\cap
  I_{MC})+P(II_{RC}\cap I_{MC})) + b_{C} \cdot (P(II_{RC}\cap I_{MC})+P(I_{RC}\cap I_{MC})) + \\
  & \ \ \ \ c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap
  II_{MC})+P(I_{RC}\cap II_{MC}))]
\end{align*}
\noindent Now let us split into $a>d$ and $a<d$, and consider $a>d$ first.  Notice that, by
Lemma \ref{lemma:action implications}, the case $a=d$ is irrelevant in order to discriminate
between $R$ and $M$. If $a>d$, by Lemma \ref{lemma:action implications} we can eliminate the
cases where $R$ plays $II$ and $M$ plays $I$:
\begin{align*}
  & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot
  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) \\ 
  & \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC})
  +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC}))] \\
  > & 
  \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot P(I_{RC}\cap I_{MC}) + b_{C} \cdot P(I_{RC}\cap I_{MC}) +
  c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) \\ 
  & \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap
  II_{MC})+P(I_{RC}\cap II_{MC}))]
\end{align*}
Rewrite:
\begin{align*}
  & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})-
  P(I_{RC}\cap I_{MC})) \\ 
  & \ \ \ \ + b_{C} \cdot (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})-
  P(I_{RC}\cap I_{MC})) \\ 
  & \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})-
  P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\ 
  & \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap
  II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
\noindent We now distinguish between two cases: (1) $ a-c = d-b $ and (2) $  a-c \neq d-b $. Notice that $P(I_{RC}\cap II_{RC}) \neq 0$ if and only if case (1) obtains, and that $a>d$ and (1) imply $II_{MC}$. Then, from (1) we have\footnote{Note that when we have only 3 elements in the support it is not guaranteed that case (1), together with $a>d$, may arise in a coordination game, whereas it is guaranteed that case (2), together with $a>d$, occurs with some positive probability. If we take for instance $x= 5, y= 2, z= 1$, then case (1) cannot obtain, whereas if we take $x= 3, y= 2, z= 1$, both (1) and (2) may obtain ($a=3, b=1, c=2, d=2$ for case (1), and $a=3, b=1, c=2, d=2$ for case (2)). Moreover, under the assumption that $a>d$, having 3 elements in the support is a necessary and sufficient condition for case (2) to have positive occurrence in a coordination game. As it will be clear in the following, a positive occurrence of case (2) only is enough for the theorem to hold.}:

\begin{align*} 
& \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC}))\\
& \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))\\
& \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
that is
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (\frac{1}{4}+\frac{1}{4}) + b_{C} \cdot  (\frac{1}{4}+\frac{1}{4}) + c_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2}) + d_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2})]> 0
\end{align*}
%\begin{align*}
%\textstyle{\sum_{C_{a>d}}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]> 0
%\end{align*}
\noindent Since we have assumed $ a-c = d-b $, the last inequality is not satisfied. We have instead:

$$ \sum_{C_{a>d}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]= 0 $$

\noindent This means that where $a_{C}>d_{C}$ and
where (1) is the case, $R$ and $M$ are equally fit. This changes when we turn to (2). In that
case, since $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$ by Lemma \ref{lemma:action implications}, we have that
$P(I_{RC} \cap I_{RC})-P(I_{RC}\cap I_{MC})=P(I_{RC}\cap II_{MC})$. Moreover, when $a_{C}>d_{C}$, $b_{C}\geq c_{C}$ implies
  $b_{C}-d_{C}>c_{C}-a_{C}$ (see Lemma \ref{lemma:action implications}). Consequently, when $M$ plays either $I$ or $  (\frac{1}{2}I;\frac{1}{2}II)$, $R$ always plays $I$. Hence, whenever $a_{C}>d_{C}$ and (2) obtain, it also holds that $P(II_{RC}\cap II_{MC})=P(II_{RC} \cap II_{RC})$. In this case we can simplify:
\begin{align*}
& \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{RC}\cap I_{RC})- P(I_{RC}\cap I_{MC})) \\
& \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\
& \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap II_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot P(I_{RC}\cap II_{MC}) + b_{C} \cdot  P(I_{RC}\cap II_{MC}) \\- c_{C} \cdot P(I_{RC}\cap II_{MC}) - d_{C} \cdot P(I_{RC}\cap II_{MC})]> 0
\end{align*}
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0
\end{align*}
\noindent We know that $I_{RC}$ implies that $a_{C}-c_{C}\geq d_{C}-b_{C}$.
% and $II_{MC}$ implies that $b_{C}\leq c_{C}$.
Since we have assumed that $a_{C}-c_{C}\neq d_{C}-b_{C}$, we have that
$a_{C}-c_{C} > d_{C}-b_{C}$. Hence, the inequality
$$\sum_{C_{a>d}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0$$
is satisfied. So, when $a_{C}>d_{C}$, $R$
strictly dominates $M$. Symmetrically, from $a<d$ and by distinguishing between the two cases
(1) and (2) as before, in the end we get:
\begin{itemize}
\item[(1)] $\sum_{C_{a<d}} P(C)[-\frac{1}{2}a_{C}- \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]= 0$; and
\item[(2)] $\sum_{C_{a<d}} P(C)[P(II_{RC}\cap I_{MC})\cdot (-a_{C} - b_{C} + c_{C} + d_{C})]> 0$.
\end{itemize}
\noindent Hence, we can conclude that $R$ strictly dominates $M$ in the class
$\mathcal{C}\cup\mathcal{A}$. Notice that in case of i.i.d. sampling with infinite support,
games in $\mathcal{B} $ and $\mathcal{W} $ never arise, but the proof is the same for the
remaining games in $\mathcal{S}$, $\mathcal{C} $ and $\mathcal{A}$.

\medskip{}

\noindent It remains to be shown that
$F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M)$. As before, spell
this out as:
\begin{align*}
& \sum_{C}P(C)[P(I_{MC}\cap I_{MC})\cdot a_{C}+P(II_{MC}\cap II_{MC})\cdot d_{C}+P(I_{MC}\cap II_{MC})\cdot b_{C}+P(II_{MC}\cap I_{MC})\cdot c_{C}] \\
+ & \sum_{A}P(A)[P(I_{MA}\cap I_{MA})\cdot a_{A}+P(II_{MA}\cap II_{MA})\cdot d_{A}+P(I_{MA}\cap II_{MA})\cdot b_{A}+P(II_{MA}\cap I_{MA})\cdot c_{A}] \\
< & \sum_{C}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot b_{C}+P(II_{RC}\cap I_{MC})\cdot c_{C}]\\ 
+ & \sum_{A}P(A)[P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot d_{A}+P(I_{RA}\cap II_{MA})\cdot b_{A}+P(II_{RA}\cap I_{MA})\cdot c_{A}]
\end{align*}
\noindent When $a>d$, similarly to the above derivation, we get: 
\begin{align*} & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC}))\\ & + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC}))\\ & + c_{C} \cdot (P(II_{MC}\cap I_{MC}) +P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\ & + d_{C} \cdot (P(II_{MC}\cap II_{MC})+P(II_{MC}\cap I_{MC})- P(II_{RC}\cap II_{MC}))]< 0
\end{align*}
\noindent We now distinguish between (1) $b=c$, (2) $b>c$, and (3) $b<c$. Notice that either
(1) or (2), together with $a>d$, implies $I_{RC}$. Then we obtain:\footnote{Note that here, when
  we only have 3 elements in the support, case (2) is impossible, but cases (1) and (3) can
  occur with positive probability, and this enough for our purpose.}
\begin{itemize}

\item[(1)] $\sum_{C_{a>d}} P(C)[-\frac{1}{2}a_{C} - \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC}))]= 0$;

\item[(3)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (- P(I_{RC}\cap II_{MC})) + b_{C} \cdot  (- P(I_{RC}\cap II_{MC})) + c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] \leq 0$.

\end{itemize}
\noindent When $a<d$, the derivation proceeds symmetrically and we get: 
\begin{itemize}

\item[(1)] $\sum_{C_{a<d}} P(C)[\frac{1}{2}a_{C} + \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a<d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + c_{C} \cdot (- P(II_{RC}\cap I_{MC})) + d_{C} \cdot (- P(II_{RC}\cap I_{MC}))] \leq 0$;

\item[(3)] $\sum_{C_{a<d}} P(C)[c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] = 0$.

\end{itemize}
\noindent Finally, we can conclude that
$F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M)$. As before, notice
that, when we have i.i.d. sampling with infinite support, games in $\mathcal{W}$ and
$\mathcal{B}$ never occur, but the proof is the same for all the other cases. Hence, both when
the support of $a,b,c,d$ is finite, and when the support is infinite, $R$ strictly dominates
$M$ under the conditions assumed. \hfill $\dashv$


\vspace{.5cm}


\noindent \textbf{Proof of Proposition \ref{proposition2}.} Take an arbitrary real interval $(\underline{r},\overline{r})$, where
$\underline{r}$ is the infimum and $\overline{r}$ is the supremum,
and let $P$ be the PDF over $(\underline{r},\overline{r})$. Given
Lemma \ref{lemma:probabilities coord-ant}, we focus on the case of coordination games. Now fix a set
of probabilities $[s,t]$, corresponding to the uncertainty of the
players about the co-player's action. As we have seen, objective and
regret types will behave exactly the same when $s=t$ and $[s,t]$
reduces to a single probability. Notice that objective and regret
will also be indistinguishable when there is no game such that $s<\frac{a-c}{a-c+d-b}<t$.
It is then a necessary condition for regret types to be better than
objective types that the two types behave differently at least in
some games. Geometrically, the ratio $\frac{a-c}{a-c+d-b}$ denotes
the intersection point of the lines corresponding to action $I$ and
action $II$ (figure~\ref{fig:graphstrictinf1-2})
\begin{figure}
\centering\begin{tikzpicture}[scale=.9]


%
\draw [-, very thick]  (0,0) to (5,0);
%
\draw [-, very thick] (0,0) to (0,5);

%
\draw [-, very thick] (5,0) to (5,5);



%
\node (zero) [] at (-.75,-.5) {$P(II)=0$};

%
\node (s) [] at (2,0)
  [label=below:{$s$}] {|};
%
\node (t) [] at (4,0)
  [label=below:{$t$}] {|};
%
%\node (st) [] at (3,0)
 % [label=below:{$s + \nicefrac{\epsilon}{2}$}] {};


\draw [dashed] (2,0) -- (2,5);
\draw [dashed] (4,0) -- (4,5);

\draw [dotted] (3,0) -- (3,5);

%
\node (a') [] at (1.8,2.6)
  [label=left:{}] {$a'$};
\node (c') [] at (2.2,2)
  [label=left:{}] {$c'$};
\node (d') [] at (4.2,4)
  [label=left:{}] {$d'$};
\node (b') [] at (4.2,2.6)
  [label=left:{}] {$b'$};

%
\node (0y) [] at (0,0)
  [label=left:{$c$}] {};
\node (1y) [] at (0,1)
  [label=left:{}] {-};
\node (2y) [] at (0,2)
  [label=above right:{$I$}] [label=left:{$a$}] {-};

\node (3y) [] at (0,3)
  [label=left:{}] {-};
\node (4y) [] at (0,4)
  [label=left:{}] {-};
\node (5y) [] at (0,5)
  [label=left:{}] {-};
%
\node (1yy) [] at (5,1)
  [label=left:{}] {-};
\node (2yy) [] at (5,2)
  [label=left:{}] {-};

\node (3yy) [] at (5,3)
  [label=right:{$b$}] {-};
\node (4yy) [] at (5,4)
  [label=left:{}] {-};
\node (5yy) [] at (5,5)
  [label=left:{$II$}] [label=right:{$d$}] {-};
%

\node (p1) [] at (5,0)
  [label=below:{$1$}] {};

%


%
\draw [-, very thick] (0,2) to (5,3);
%
\draw [-, very thick] (0,0) to (5,5);
%


\draw[decorate,decoration={brace,amplitude=10}](5,5)--(5,3)node [black,midway,xshift=25pt] {\footnotesize $d-b$};
%
\draw[decorate,decoration={brace,amplitude=10}](0,0)--(0,2)node [black,midway,xshift=-25pt] {\footnotesize $a-c$};
%

\end{tikzpicture}\protect\caption{Example of strictly informative coordination game.}
\label{fig:graphstrictinf1-2}
\end{figure}
. If there is no game where such an intersection point falls into
the interval $[s',t']$, for some $s'\neq t'$, then whenever players
hold the set of probabilities $[s',t']$ they will perceive any game
that can arise as having a dominant action, i.e., as a strong dominance
game in partition cell $\mathcal{S}$. Note also that the ratio $\frac{a-c}{a-c+d-b}$
takes values in the interval $(0,1)$, since to get $\frac{a-c}{a-c+d-b}=0$
we shall have $a=c$ and for $\frac{a-c}{a-c+d-b}=1$ we need $d=b$,
which have no probability of occurrence for continuous random variables.
Furthermore, the ratio $\frac{a-c}{a-c+d-b}$ is dense in $(0,1)$,
and consequently in any interval $[s,t]\subset(0,1)$. Indeed, for
any rational number $\frac{m}{n}\in[s,t]$, we can always find $a,b,c,d\in(\underline{r},\overline{r})$
such that $\frac{a-c}{a-c+d-b}=\frac{m}{n}$ as follows. It is not
difficult to see that for this to be the case we must have 
\[
a-(a-b)\frac{m}{n}=c+(d-c)\frac{m}{n}.
\]
This equation reduces to
\begin{equation}
\frac{n-m}{n}a+\frac{m}{n}b=\frac{n-m}{n}c+\frac{m}{n}d,\label{eq:hit the point}
\end{equation}
which is an equality between two convex combinations with the same
weights $\frac{n-m}{n}$ and $\frac{m}{n}$. Then, whatever $a,b\in(\underline{r},\overline{r})$
we pick it is always possible to find (infinitely many) $c,d\in(a,b)\subset(\underline{r},\overline{r})$
such that equation (\ref{eq:hit the point}) is satisfied. Indeed,
for any $x\in(0,1)$ we have that $c=(\frac{n-m}{n}a+\frac{m}{n}b)x+a(1-x)$
and $d=(\frac{n-m}{n}a+\frac{m}{n}b)x+b(1-x)$ satisfy equation (\ref{eq:hit the point}).
This shows that for any rational number $\frac{m}{n}\in[s,t]$ we
can always find $c,d\in(a,b)\subset(\underline{r},\overline{r})$
such that $\frac{a-c}{a-c+d-b}=\frac{m}{n}$, and so we can always
find $a,b,c,d\in(\underline{r},\overline{r})$ such that $\frac{a-c}{a-c+d-b}=\frac{m}{n}$.
Since rational numbers are dense in $\mathbb{R}$, they are also dense
in any real interval $[s,t]\subset(0,1)$. Hence, the ratio $\frac{a-c}{a-c+d-b}$
is dense in $[s,t]$.

Once we have proven the density of $\frac{a-c}{a-c+d-b}$ for coordination
games $G=(a,b,c,d)$ in any real interval $[s,t]\subset(0,1)$, the
rest of the proof is connected to the proof of Proposition \ref{proposition1}. Specifically,
for a given interval $[s,t]$ we define
\begin{equation}
\begin{array}{ccccc}
a' & := & (1-s)a+sb & = & a+s(b-a)\\
b' & := & (1-t)a+tb & = & a+t(b-a)\\
c' & := & (1-s)c+sd & = & c+s(d-c)\\
d' & := & (1-t)c+td & = & c+t(d-c)
\end{array}\label{eq:a'b'c'd'}
\end{equation}
The equivalent of Lemmas \ref{lemma:probabilities coord-ant}, \ref{lemma:probabilities actions coord-ant} and \ref{lemma:action implications} as well as the rest of the proof of Proposition \ref{proposition1} hold also for $a',b',c',d'$. \hfill $\dashv$



\newpage

\printbibliography[heading=bibintoc]

\end{document}



