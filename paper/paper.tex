\documentclass[fleqn,reqno,11pt]{article}

%========================================
% Packages
%========================================

\usepackage{etex}
\usepackage[nobiblatex]{helpers/mypackages}
\usepackage[backend=bibtex,natbib=true,style=authoryear-comp,backend=bibtex,doi=false,url=false]{biblatex}
\bibliography{choicePrince,biblio}
\usepackage{helpers/myenvironments}
\usepackage{helpers/mycommands}

\usepackage{todonotes}
\usepackage{subcaption}

\usetikzlibrary{positioning,arrows,calc,fit}
%========================================
% Standard Layout
%========================================

\usepackage{bigints}
\usepackage{MnSymbol}
\usepackage{a4wide}

\usepackage[T1]{fontenc}


% Itemize
\renewcommand{\labelitemi}{\large{$\mathbf{\cdot}$}}    % itemize symbols
\renewcommand{\labelitemii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiv}{\large{$\mathbf{\cdot}$}}
% Description
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\textsc{#1}}

% Figure Captions
\usepackage{caption} % use corresponding myfiguresize!
\setlength{\captionmargin}{20pt}
\renewcommand{\captionfont}{\small}
\setlength{\belowcaptionskip}{7pt} % standard is 0pt

\newcommand{\myalert}[1]{\textcolor{red}{#1}}

\title{Smart Representations: {R}ationality and Evolution in a Richer  Environment}
\author{Paolo Galeazzi \& Michael Franke} \date{current version: November 5, 2015}

\begin{document}

% \textbf{some general notes and remarks and todo's}

% \begin{itemize}
% \item micha: let's have a term for pairs (regret type, epistemic type); I'm inclined to use ``player
%   type'', but that's what you reserved for regret type; maybe use \myalert{meta-type}?
% \item can we reformulate the main result (proposition 1) in terms of ``strict dominance''
%   and/or strict Nash equilibrium? sometimes the proof is not ideally precise because it
%   actually shows something stronger, namely strict dominance, but everything is formulated in
%   terms of stability; using dominance is also better for arguments about evolutionary dynamics:
%   dynamics can be attracted by states that are not evolutionarily stable (even if there is an
%   ESS), but dominated strategies will die out in any case;
% \item should we give definitions for ESS and NSS in the main text?
% \item should we include definitions of the replicator dynamic and/or the replicator mutator
%   dynamic?
% \item the most important issue with this text is still that we need to motivate why we care
%   about non-probabilistic beliefs and/or security strategies; everything else will fall into
%   place when that is done
% \item an important ingredient in our argument pro regret is that radical uncertainty could be
%   exogenously given; as it stands this occurs rather ``suddenly'' in the paper and should
%   ideally be prepared and motivated earlier (e.g., in Sections 2 and 3)
% \item a major contribution of our paper is the ``meta-game'' approach, but in the current
%   version I feel that this is not stressed sufficiently; the introduction and the beginning of
%   section 3 could do a bit more to carve out this idea as something generally cool; especially
%   the formula from 3.4 could occur earlier (albeit in more general notation);
% \end{itemize}

% \newpage


\maketitle

\section{Introduction}
\label{sec:intr--motiv}

% \begin{quotation} \textit{This is what I aim at, because the point of philosophy is to start with something so simple as not to seem worth stating, and to end with something so paradoxical that no one will believe it.} \citep{Russ18} \end{quotation}

Evolutionary game theory has become an established philosophical tool for probing into
conceptual issues whose complexity requires mathematical modelling at a high level of
abstraction.  Abstraction entails simplifying assumptions. Evolutionary game models frequently
simplify reality in two ways that are relevant for present purposes. Firstly, the environment
is represented as a fixed, closed and immutable \emph{stage game}; secondly, the focus of
evolutionary selection is behavior for this stage game alone. In contrast, some argue for
studying instead the evolutionary competition of general \emph{choice mechanisms} in a rich and
variable environment
\citep[e.g.,][]{FawcettHamblin2013:Exposing-the-be,McNamara2013:Towards-a-Riche}. To show that
there is no tension here, this paper introduces a method for reconciling established notions of
evolutionary game theory with evolutionary selection of choice mechanisms in variable
environments \citep[see related work
by][]{ZollmanSmead2010:Plasticity-and-,SmeadZollman2013:The-Stability-o}.\todo{More references
  Harley 1981!!!, O Conor \dots}

If agents deal with a rich and variable environment, they face many different choice
situations. A choice mechanism associates decision situations with action choices. A crucial
part of any choice mechanism is a \emph{subjective representation} of the decision situation,
in particular a manner of forming preferences and beliefs about a possibly uncertain world (see
Figure~\ref{fig:ChoiceMechanism} below). This paper asks: which preference and belief
representations are ecologically valuable (i.e., lead to high fitness)? The evolution of
preferences has been subject of recent interest in theoretical economics
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. The particular contribution of this paper is
to provide a case, spelled out formally, that questions of preference evolution should take
variability in belief representation into account as well. As a point in case, we demonstrate
that if agents have imprecise probabilistic beliefs \citep[e.g.][]{gardsah82,levi74,walley96},
preference representations in terms of regret can be selected for, even though they may deviate
from the true, objective fitness that evolutionary selection operates on.

% To show this, we study the evolutionary competition of choice mechanisms, or more
% specifically the competition of subjective representations of choice situations, in a
% variable environment. Concretely, we look at evolutionary ``meta-games'' whose ``acts'' are
% choice mechanisms and whose ``payoffs'' represent the average expected fitness that different
% choice mechanisms would accrue when playing arbitrary games (from a given class, with a given
% occurrence probability). In this sense, a meta-game captures (the modeller's) assumptions
% about the relevant statistics of the environment in which evolutionary forces operate, while
% we are still able to use standard methods from evolutionary game theory, like stability or
% evolutionary dynamics.

Section \ref{sec:rati--subj} reviews different accounts of rationality in decision theory and
relates the approach we take here to the existing literature on the topic. Section
\ref{sec:basic-notions} introduces relevant decision- and game-theoretic notions. Sections
\ref{sec:model} and \ref{sec:basic-model-1} define the evolutionary framework, and Section
\ref{sec:results:-basic-model} presents the main results of the paper. Section
\ref{sec:extensions} extends the evolutionary analysis by introducing more choice mechanisms in
the model and by investigating the evolutionary performances of different choice mechanisms
when facing arbitrary decision problems. Finally, in Section \ref{sec:conclusion} we reflect on
the results achieved.

\todo[inline]{rewrite overview after restructuring}


\section{Rationality, subjective representation schemes \& meta games}
\label{sec:rati--subj}

The standard textbook definition of \textit{rationality} in economics and decision theory
traces back to the seminal work of \citet{deFinetti37}, \citet{Neumannvon-NeumannMorgenstern1944:Theory-of-Games}
and \citet{Savage1954:The-Foundations}. It says that choice is rational only if it maximizes
(subjective) expected utility.

\begin{definition}[Rational choice from maximization of expected utility]
  \label{def:rationality}
  Let $S$ be a (finite) set of states of the world and $A$ a (finite) set of actions. Given a
  probability distribution $\mu$ over $S$ and a utility function
  $u \mycolon S \times A \rightarrow \mathbb{R} $, an action $a^* \in A$ is a rational choice
 if it maximizes the \emph{(subjective) expected utility} 
  $\EU(a) = \sum_{s \in S} \mu(s) \ u(s, a)$.
\end{definition}

\noindent Expected utility is subjective in the sense that it is a function of subjective
preferences and subjective beliefs of the decision maker (DM). To wit, a choice can be
rational, i.e., the best choice from the DM's point of view, even if based on peculiar beliefs
and/or aberrant preferences. % (Indeed, while
% \citeauthor{Neumannvon-NeumannMorgenstern1944:Theory-of-Games} assumed that probabilities were
% objectively given, it was \citet{Savage1954:The-Foundations} who famously extended von Neumann
% and Morgenstern's axiomatization to include a subjective notion of probabilistic beliefs on top
% of the subjective notion of preference that von Neumann and Morgenstern started out with.)

If beliefs and preferences are subjective, there is room for \emph{rationalization} or
\emph{redescriptionism} of observable behavior. For example,
\citet{KahnemannTversky1979:Prospect-Theory} famously demonstrated that what appear to be
violations of rationality norms in human decision making can be explained on the assumption
that subjects' beliefs and preferences deviate systematically from the objectively given
parameters in the presented choice task. Similarly for social decision making, including social
preferences such as fairness, reciprocity or similar, allows us to describe as rational
empirically observed behavior, such as in experimental prisoners' dilemmas or public goods
games, that might otherwise appear irrational \citep[e.g.,][]{fehrschmidt99,charrab02}.

The main objection to redescriptionism is that, without additional constraints, the notion of
rationality is likely to collapse, as it seems possible to deem rational almost everything that
is observed, given the freedom to adjust beliefs and preferences at will. \emph{Normativism}
therefore emphasizes that there are many ways in which ascriptions of beliefs and preferences
are constrained by normative considerations of rationality as well. As for beliefs, where
possible, subjective beliefs should reflect objective chance. Where this is impossible,
temporal consistency of choices constrains rational subjective beliefs, else exploitation by
Dutch books is possible. As for preferences, in certain situations it may make sense to
postulate on normative grounds that subjective preferences should be oriented towards tracking
objective fitness. For instance, profit maximization seems a reasonable assumption for
characterizing outcomes in a competitive market because only firms behaving in a manner that is
consistent with profit maximization will survive in the long run \myalert{(Alchian,
  Friedman)}.\todo{@Paolo: please add references}

An alternative view on rationality of choice is \emph{adaptationism}
\citep[e.g.,][]{Anderson1991:Is-human-cognit,ChaterOaksford2000:The-Rational-An,HagenChater2012:Decision-Making}. \todo{more
  references} Adaptationism aims to explain choice behavior by appeal to evolutionary
considerations: DMs have acquired choice mechanisms that have proved to be adpative in a rich
and variable environment. A choice mechanism could be a set of distinct heuristics (the DM's
\emph{adaptive toolbox}) that have little in common
\citep[e.g.,][]{TverskyKahnemann1981:The-Framing-of-,GigerenzerGoldstein1996:Reasoning-the-F,ScheibehenneRieskamp2013:Testing-the-Ada}. But
to relate to the philosophical debate about the nature of rational choice, we here suggest to
think of a choice mechanism as a map from choice situations to action choices that contains an
explicit level of subjective representation of a decision situation (see
Figure~\ref{fig:ChoiceMechanism}). We are most interested in the question which
\emph{subjective representation schemes}, i.e., general ways of forming preferences and
beliefs, are better than others from an evolutionary point of view.

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 2.25cm]

  \begin{scope}

    \node[] (environment) {};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = environment, node distance = 2cm] (situation)
    {environment produces random \\ \textbf{decision situation} $d$};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = situation] (representation) 
    {\textbf{subjective representation} $s$ of $d$ \\ \textcolor{gray}{[preference, beliefs, \dots]}};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = representation] (action)
    {\textbf{action choice} $a$ based on $s$};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = action,node distance = 1.5cm] (payoff) {\textbf{payoff} for $a$ in $d$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.5cm]situation.center)--([xshift=-1.75cm,yshift=0.55cm]representation.center)
    node[midway, right] {subj.~representation scheme $\gamma$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.55cm]representation.center)--([xshift=-1.75cm,yshift=0.5cm]action.center)
    node[midway, right] {action selection function $\kappa$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.55cm]action.center)--([xshift=-1.75cm,yshift=0.5cm]payoff.center)
    node[midway, right] {};

    \node[minimum height = 0.75cm, text width=6cm, text
    centered, right of = representation, xshift = 2cm, rotate = 90] (mechanism) {choice mechanism};

    \begin{pgfonlayer}{background}
             \node [rounded corners, dashed,
       very thick,draw=black!40,fit={($(situation.south)+(0,-10pt)$) 
                                     ($(mechanism.west)+(+5pt,+50pt)$) 
                                     % ($(action.north)+(0,+10pt)$) 
                                     ($(representation.west)+(-7pt,0)$)
                                   }] (box) {};
    \end{pgfonlayer}

  \end{scope}



\end{tikzpicture}
  \caption{Schematic representation of action choice. A choice mechanism maps decision
    situation $d$ to action choice $a$ by way of representing $d$ in terms of subjective
    preferences and subjective beliefs.}
  \label{fig:ChoiceMechanism}
\end{figure}


The fitness of a subjective representation scheme is measured in terms of the fitness of a
choice mechanism that contains it, all else equal. The fitness of a choice mechanism $c$ is the
expected payoff of $c$ over arbitrary decision situations $d$:
\begin{align}
  \label{eq:FittnessChoiceMechSolitary}
  F(c) = \int P(d) \  \utils_d(c) \text{ d} d \,,
\end{align}
where $P(d)$ is the probability (density) of $d$ occurring and $\Utils_d(c)$ is the payoff of
the act selected by $c$ in $d$. Probability measure $P(d)$ is our model of the statistical
properties of a rich and variable environment.

The same logic also applies to situations of social decision making. A game $g$ is also a
decision situation, albeit a more complex one. A choice mechanism $c$ maps $g$ onto an action
choice in $g$. The fitness of $c$ is then:\footnote{We assume for simplicity of exposition that
  all games with positive occurrence probability are two player games.}
\begin{align}
  \label{eq:FittnessChoiceMechGame}
  F(c) = \int \int P(g) \  P(c') \  \utils_g(c,c') \text{ d} g \,,
\end{align}
where $P(g)$ is the probability (density) of game $g$, $P(c')$ is the probability of
encountering a co-player who uses choice mechanism $c'$ and $\utils_g(c,c')$ is the payoff for
a $c$-player in $g$ against the $c'$-player, given by the payoff in $g$ of the actions that $c$
and $c'$ select in $g$. For the most part, we will focus on the more complex case of social
decision making here. Results about solitary decision making follow from parallel arguments
(see Section~\ref{sec:extensions}).

\emph{Meta games} are abstract models for the evolutionary competition between choice
mechanisms in social decision making contexts. The expected payoff for a $c$-player in a random
encounter with a $c'$-player is the average of payoffs obtained in a random game $g$ weighted
by $g$'s occurrence probability:
\begin{align}
  \label{eq:FittnessChoiceMechGamePairwise}
  F(c, c') = \int P(g) \  \utils_g(c,c') \text{ d} g \,.
\end{align}

\begin{definition}[Meta game]
  \label{def:MetaGame}
  Let $C$ be a set of choice mechanisms and let $P(g)$ give the occurrence probability of game
  $g$ in the environment. The meta game that captures selection of choice mechanisms in this
  environment is the one-population evolutionary game with strategies $C$ and payoff function
  $\pi \mycolon C \times C \rightarrow \mathbb{R}$ such that
  $\pi(c,c') = F(c,c') = \int P(g) \  \utils_g(c,c') \text{ d} g$.
\end{definition}

Standard notions of evolutionary game theory can be applied to meta games as well. A choice
principle $c$ is a strict Nash equilibrium if $F(c,c) > F(c',c)$ for all $c'$; it is
evolutionarily stable if for all $c'$ either (i) $F(c,c) > F(c',c)$ or (ii) $F(c,c) = F(c',c)$
and $F(c,c') > F(c',c')$. It is neutrally stable if for all $c'$ either (i) $F(c,c) > F(c',c)$
or (ii) $F(c,c) = F(c',c)$ and $F(c,c') \ge F(c',c')$
\citep{Maynard-Smith1982:Evolution-and-t}. Similarly, evolutionary dynamics can be applied to
meta games. We will later turn towards the replicator dynamic
\citep{TaylorJonker1978:Evolutionary-St} and the replicator mutator dynamic
\citep[e.g.][]{Nowak2006:Evolutionary-Dy}.\todo{footnote with caveat about agent-level
  processes}




\section{Choice mechanisms: preferences, beliefs and action choice}
\label{sec:basic-notions}

We submit that meta games are a useful tool for structuring and sharpening conceptual inquiry
in a domain that is notorious for its fuzziness and elusiveness. To show how this approach can
be applied to the benefit of conceptual debate, we consider the evolution of preference
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. Concretely, we here pit two different ways of
representing preferences against each other: (i) veridical representations that correctly
reflect objective fitness, and (ii) non-veridical subjective preferences in terms of regret
\citep[e.g.,][]{Savage1951:The-theory-of-s,}. We show that subjective, non-veridical preference
representations based on a notion of regret can be adaptive and even outperform objective,
veridical representations of the actual, true fitness, when agents, at least on occasion, hold
\emph{imprecise probabilistic beliefs} \citep[e.g.,][]{gilsch89,levi74,gardsah82} and make
decisions based on a security choice rule (here: the maxmin rule applied to expected
utility). This case study strips the generality of a meta game approach down to a perspicuous
comparison between only four choice mechanisms, summarized in Table~\ref{tab:CMs}: the
cross-product of two different preference representations (veridical vs.~regret) and two
different belief representations (precise vs.~imprecise). 

\begin{table}[t]
  \centering
  \begin{tabular}{cccc}
    \multicolumn{2}{c}{subjective representation} & action selection & corresponding choice
    rule \\  \cmidrule(r){1-2}
    preference type & belief type  \\ \midrule
    objective fitness & precise & maxmin EU &  Laplace \\
    objective fitness & precise & maxmin EU & maxmin \\ 
    regret & imprecise & maxmin EU & Laplace \\
    regret & imprecise & maxmin EU & regret minimization \\ 
  \end{tabular}
  \caption{Overview relevant choice mechanisms and the corresponding (behaviorally equivalent)
    classic choice rules. All notions are explained in the main text.}
  \label{tab:CMs}
\end{table}

This exclusive selection of choice mechanisms is motivated by a number of
considerations. Firstly, the set of conceivable choice mechanisms is vast. Most of that
conceivable vastness is utter nonsense. Some selection of plausible candidates is required in
any case. Secondly, a severe restriction to only a small number of suitable candidates will
help demonstrate more distinctly how a meta game approach may help structure philosophical
inquiry. This is our main goal. Thirdly, our case study adds to the recent literature on the
evolution of preferences by clearly demonstrating the possibly non-trivial interaction between
preference and belief representations, all else equal. This is our secondary goal. Finally, the
four choice mechanism that we look at here, although phrased in terms of preference and belief
representations, map onto well-known classic proposals for rational choice (see
Table~\ref{tab:CMs}). In other words, there is also an argument by historical relevance for our
selection. To see this, we first recapitulate the relevant classics ---to have a handy label,
we call them \emph{classic rules}--- and then show how they relate to our notion of choice
mechanism.

\subsection{Classic rules for rational choice} 
\label{sec:choice-rules}

The classic literature on rational choice contains a number of proposals for which acts a
player should choose if she is rational. Maximization of expected utilities, as
given in Definition~\ref{def:rationality}, is only one of them. Here, we consider three others:
(i) maxmin, (ii) Laplace's rule and (iii) regret minimization. All of these apply to games and
solitary decision situations. We focus on games, but definitions carry over straightforwardly.

A strategic game is a tuple $ G=\tuple{ N, (A_i , \pi_i)_{i \in N} }$ with $N$ a finite set of
players, $A_i$ a finite set of acts and $\pi_i$ a payoff function for each player $i \in N$.
Part of this paper will focus on symmetric games with two players. A two-player game is
symmetric if players have the same action set, $A_1 = A_2$, and payoffs are symmetric:
$\pi_1(a_k, a_j) = \pi_2(a_j, a_k)$ for all $a_j,a_k \in A_1$. An example is given in
Figure~\ref{coordgame1}. The numbers give the payoff for the player choosing a row act against
an opponent choosing a column act. Since the game is symmetric, it suffices to give just these
numbers.

Rational choice from maximization of expected utilities, as given in
Definition~\ref{def:rationality}, applies to games as follows. To say which acts are rational,
it requires specifying a belief of the DM. Let $A_{-i} = \bigtimes A_j$ for all $j \neq i$ be
the set of all \emph{profiles} of acts for all players except $i$. If player $i$ has a belief
$\mu_i \in \Delta(A_{-i})$ about the behavior of his opponents, he chooses rationally only if
his choice $a^*_i$ maximizes expected utility
$\EU(a_i) := \sum_{\act_{-i} \in \Acts_{-i}} \mu_i(\act_{-i}) \myts \pi_i(a_i, a_{-i})$. For
the example in Figure~\ref{coordgame1}, a rational player would choose act $I$ exactly when he
believes that his opponent will choose act $I$ with a probability of at least
$\nicefrac{2}{3}$.

Perhaps the most well-known classic rule that does not require explicitly stating the DM's
beliefs is the \emph{maxmin rule}. It tells our agent to choose act $\act^*$ only if it
maximizes payoffs for all worst case scenarios:
$\act^* \in \argmax_{\myact{i} \in \Acts_i} \min_{\myact{-i} \in \Acts_{-1}} \pi_i(\myact{i},
\myact{-1})$.
For the example in Figure~\ref{coordgame1}, the maximim rule looks at the minima in each row,
asking: what is the worst possible outcome for each row act? It is $0$ in either
case. Consequently, the maxmin rule would not favor either act: players choosing by the maxmin
rule would be indifferent between acts $I$ and $II$.

\emph{Laplace's rule} selects $\act^*$ if
$\act^* \in \argmax_{\myact{-i} \in \Acts_{-i}} \sum_{\myact{-i}} \pi(\myact{i}, \myact{-i})$.
Laplace's rule is equivalent to expected utility maximization under a \emph{flat belief} that
assigns equal probability to all of the opponents' choices. In the coordination game of
Figure~\ref{coordgame1}, Laplace's rule selects act $II$.

The final classic rule that is relevant for our purposes here is \emph{regret
  minimization}. The notion of regret in decision theory dates back at least to the work by
\citet{Savage1951:The-theory-of-s}, and has later been developed by \citet{bell82}, \citet{fish82} and \citet{loosug82} independently. Lately,
\citet{HalpernPass2012:Iterated-Regret} showed how the use of regret minimization can give
solutions to game theoretical puzzles (like the Traveller's dilemma) in a way that is closer to
everyday intuition and empirical data. In this paper the notion of regret is defined as in
\citet{HalpernPass2012:Iterated-Regret}:

\begin{definition}[Regret, Regret Minimization] \label{defn:regret} Given a strategic game
  $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, the \emph{pairwise regret} of player $i$'s
  act $a_i$ against profile $a_{-1}$ is
  $\text{reg}(a_i,a_{-i}):= \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $, where
  $a_i^\$ \in \argmax_{a_i \in A_i} \pi_i(a_i, a_{-i})$ denotes $i$'s best reply to the
  opponents' actions $a_{-i}$. The \emph{regret} of $a_i$ is then
  $\text{reg}(a_i):= \text{max}_{a_{-i}\in A_{-i}} \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $. An
  action $a^{*}_i $ is regret minimizing if
  $a^{*}_i \in \text{min}_{a_i} \text{max}_{a_{-i}} \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $.
\end{definition}


To illustrate, Figure~\ref{coordgame1reg} shows the pairwise regrets and the regrets of each
row-player action for the coordination game in Figure \ref{coordgame1}. We have
$\text{reg}(I)=2$, because when the other player chooses act $I$ the pairwise regret
$\text{reg}(I,I)$ is $0$ since $I$ is a best reply to act $I$, and when the other player
chooses act $II$ the pairwise regret $\text{reg}(I, II)$ is $2$ since
$\pi_i(II,II)-\pi_i(I,II)=2$. With the same reasoning, $\text{reg}(II)=1$. A regret minimizing
player would therefore choose act $II$.


\begin{figure}

  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 1 & 0 \\
      II & 0 & 2\\
      \bottomrule
    \end{tabular}
    \caption{Coordination game $G^C$}
    \label{coordgame1}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      & $\text{reg}( \cdot, I)$ & $\text{reg}(\cdot, II)$ & $\text{reg}(\cdot)$ \\
      \midrule
      I  & 0 & 2 & 2 \\ 
      II & 1 & 0 & 1\\
      \bottomrule
    \end{tabular}
    \caption{Pairwise regrets and regrets}
    \label{coordgame1reg}
  \end{subfigure}
  \caption{A coordination game (left) and regrets associated with the row player's acts (right).}
    \label{coordgame1mainFig}
\end{figure}



% \begin{example}[The Traveller's Dilemma \citep{HalpernPass2012:Iterated-Regret}]
% \label{example:TravelersDilemma}
% Let us now consider the Traveller's Dilemma case. The story behind the game goes as
% follows. There are two travellers, Ann and Bob, who lost their luggages. Ann and Bob had
% exactly the same luggage and they had insured the luggages with the same insurance company. The
% insurance policy is that the two travellers have to separately claim a certain amount between
% 2\$ and 100\$ for the reimbursement, and if they both claim the same amount then they will be
% reimbursed by that amount. Otherwise, if one of them claims more than the other, then the one
% who claimed the higher amount will get the lower amount minus 2\$, while the one who claimed
% the lower amount will get what (s)he claimed plus 2\$. The game is symmetric, and the payoff
% function for both players is:
% \begin{align*}
%   \pi_i(a_i,a_{-i}) = \begin{cases} a_i \text{ \hspace{1cm} if } a_i = a_{-i} \\ a_i + 2 \text{
%       \hspace{.4cm} if } a_i < a_{-i} \\ a_{-i} -2 \text{ \hspace{.25cm} if } a_{-i} <
%     a_i \end{cases}
% \end{align*}
% By a simple computation we have:  $\forall a_i \in \lbrace 96\$, ..., 100\$ \rbrace \text{, }
% \text{reg}(a_i)=3 $ and $\forall a_i \in \lbrace 2\$, ..., 95\$ \rbrace \text{, }
% \text{reg}(a_i)>3$. If we now eliminate all the actions that do not minimize regret in the
% first place and iterate the regret minimization only on the action set $\lbrace 96\$, ...,
% 100\$ \rbrace$, we get that 97\$ is the action that minimize regret. Indeed, $\text{reg}(97)=2$
% and $\forall a_i \in \lbrace 96\$, 98\$, 99\$, 100\$ \rbrace \text{, } \text{reg}(a_i)=3
% $. Hence, one step of regret minimizing reasoning eliminates all the actions smaller than 96\$,
% and if we iterate the process once more the unique outcome is 97\$. Finally, it is worth
% noticing that the only rationalizable equilibrium of the game corresponds to the outcome
% $(2,2)$, that is consequently the only Nash equilibrium, the only perfect equilibrium, and the
% only sequential equilibrium of the game.

% \begin{itemize}
% \item \textcolor{red}{add here:}
%   \begin{itemize}  
%   \item outcome of rationalizability (max-EU)
%   \item outcome of maxmin rule
%   \item outcome of Laplace rule
%   \end{itemize}

% \end{itemize}

%  $ \medsquare $
  
% \end{example}

Regret minimization is intimately related to the maxmin rule. Let the \emph{negative regret
  transformation} of a strategic game $G$ be the game $G'$ derived from $G$ by replacing the payoff
function $\pi$ of $G$ with $\pi'$ such that $\pi'_i(a_i, a_{-i}) = - reg(a_i,a_{-1})$, i.e.,
the payoff of all players is the pairwise negative regret (under the payoffs from $G$). For
example, the negative regret transformation of the game in Figure~\ref{coordgame1} would have the
following payoff matrix, i.e., the negatives of the pairwise regrets given in
Figure~\ref{coordgame1reg}:

\begin{center}
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 0 & -2 \\
      II & -1 & 0\\
      \bottomrule
    \end{tabular}
\end{center}

\noindent Acts selected by the maxmin rule for the negative regret transformation of $G$ are
always exactly the acts selected by regret minimization for the original game $G$.

This means that we can think of the maxmin rule and regret minimization as essentially the same
operation on different subjective representations of preference: while maxmin considers actual
objective payoffs, regret minimization considers non-veridical, regret-based subjective
preferences. As the example of Figure~\ref{coordgame1} shows, different subjective preferences
can give rise to different choice prescriptions all else equal. While maxmin is indifferent
between $I$ and $II$, regret minimization uniquely selects $II$.

In sum, classic proposals for rational choice are normative prescription of action choices
for a rational agent. Such prescriptions may rest on implicit assumptions about what the DM
believes (e.g., when interpreting Laplace's rule as rational choice under flat beliefs). They may
also make implicit assumptions about what the DM prefers (e.g., when reconstructing regret
minimization in terms of maxmin-choices on the regret transformation). This is rather messy. To
disentangle and avoid confusion, we turn to choice mechanisms.

\subsection{Choice mechanisms}
\label{sec:choice-mechanisms}

A \emph{choice mechanism} is a map from choice situations to action choices that, unlike the
classic choice rules visited above, contains an explicit level of subjective representation of
a decision situation. Let us think of a choice mechanism as a pair of functions
($\gamma,\kappa$), where $\gamma$ is a \emph{subjective representation scheme} and $\kappa$ is
an \emph{action selection function} (see Figure~\ref{fig:ChoiceMechanism}).

The subjective representation scheme $\gamma_i$ of agent $i$ is a function $\gamma_i$ that
takes a strategic game $G = \tuple{ N, (A_i , \pi_i)_{i \in N} }$ and maps it onto
$\gamma_i(G) = (\theta, e_i)$ where
$\theta \mycolon \bigtimes_{i \in N} A_i \rightarrow \mathbb{R}$ is payoff function suitable
for $G$, and $e_i$ is some representation of $i$'s uncertainty about the co-player's
behavior. We consider two \emph{preference types}: the \emph{objective type} maps each game
onto its own, true payoff function; The \emph{regret type} maps each game onto its regret
transformation, as introduced above. We also consider two \emph{belief types}: the
\emph{precise type} represents uncertainty in a single probability distribution
$\mu_i \in \Delta(\Acts_{-i})$ about the co-players' behavior; the \emph{imprecise type}
represents uncertainty as a set of probability distributions $X \subseteq \Delta(\Acts_{-i})$.
It is convenient to conceptualize precise beliefs as a special case of imprecise beliefs by
looking at $\mu_i$ as a stand-in for the singleton set $\set{\mu_i}$. More details about
imprecise probabilities and our initial assumption of ignorance are given in
Section~\ref{sec:impr-prob-beli} below.

The only action choice function $\kappa$ that we consider here is \emph{maximinimization of
  expected (subjective) utilities} (see Section~\ref{sec:impr-prob-beli} for
motivation).

\begin{definition}[Maximinimization of expected (subjective) utility]
  \label{def:maxminSEU}
  Fix a strategic game $G$. Let agent $i$'s subjective representation scheme
  $\gamma_i(G) = (\theta, e_i)$ be such that $\theta$ is $i$'s subjective payoff function for
  $G$ and $e_i$ is a (possibly singleton) set of probability distributions in
  $\Delta(\Acts_{-i})$. Agent $i$'s expected (subjetive) utility for act $\myact{i}$ and
  $\mu \in e$ is
  $\EU(\myact{i}, \mu) = \sum_{\myact{-i} \in \Acts_{-i}} \mu(\myact{-i}) \theta(\myact{i},
  \myact{-i})$.
  Act $\myact{i}^*$ is selected by maximinimization of expected (subjective) utility iff:
  $\myact{i}^* \in \arg \max_{\myact{i} \in \Acts_{i}} \min_{\mu \in e} \EU(\myact{i}, \mu)$.
\end{definition}

\begin{fact} \label{fact:singleton probability set}
Maximinimization under a precise (singleton set) belief is just standard
maximization of expected utilities.
\end{fact}

\begin{fact} \label{fact:maxEU-minReg} Under an identical precise (singleton set) belief, the
  acts selected by maximization of expected utility in game $G$ are exactly the acts selected
  by the maximization of expected utility for the negative regret transformation of $G$
  \citep[e.g.,][]{HalpernPass2012:Iterated-Regret}.
\end{fact}

This means that as far as behavior is concerned, regret types cannot be distinguished from
objective types, as long as both hold precise beliefs. This will be central in understanding
later results: under precise beliefs evolution will not favor either preference type over the
other. On the other hand, if paired with imprecise beliefs, different behavioral predictions
arise. 

In the following, we assume that agents are maximally uncertain, so that precise types have a
(singleton set containing a) flat probability distribution
$\overline{\mu}_i \in \Delta(\Acts_{-i})$ and imprecise types consider all probabilistic
beliefs about the co-players' behavior $\overline{X} = \Delta(\Acts_{-i})$. In that case, the
considered choice mechanisms are behaviorally equivalent to classic choice rules, as summarized
in Table~\ref{tab:CMs}. This means that most of the following results which we would like to
interpret as results about the evolutionary fitness of subjective representation schemes have
value also for anyone interested only in a comparison of the ecological value of classic choice
rules.

Maximal uncertainty also means that, given Fact~\ref{fact:equivalence2x2}, all choice
mechanisms are behaviorally equivalent on $2 \times 2$ symmetric games, except for objective
preference types with imprecise beliefs.

\begin{fact} \label{fact:equivalence2x2} In the class of $2 \times 2$ symmetric games, acts
  selected by Laplace's rule are exactly the acts selected by regret minimization. 
\end{fact} 


\section{The basic model}
\label{sec:basic-model-1}

\subsection{Simulation results}
\label{sec:simulation-results}

To demonstrate the usefulness of a meta-game approach, let us concentrate first on a basic
model that contains the four choice mechanisms obtained from crossing the preference and belief
types introduced in Section~\ref{sec:basic-notions}. From now on we speak of types when we mean
pairs like $(\text{reg}, \text{imp})$, a choice mechanism with regret-based preferences and
imprecise beliefs, or $(\text{obj}, \text{prc})$, one with objective preferences and precise
beliefs. 

Meta games factor in statistical properties of the environment. For a particular empirical
purposes, one would consult a specific class of games $\mathcal{G}$ with appropriate, possibly
empirically informed probabilities $P(G)$ in order to match the natural environment of a given
population. But for our theoretical purposes we adopt another approach, since we would not know
how to justify any more specific assumption. To begin with, let $\mathcal{G}$ be a set of
symmetric 2-player games with two acts. A game is then individuated solely by its payoff
function, which is essentially a matrix with four numbers. As for occurrence probabilities of
games, we imagine that entries in a game's payoff matrix are i.i.d.~random variables sampled
uniformly from set $ \lbrace 0, \dots, 10 \rbrace$.  Using Monte Carlo simulation, we can then
approximate the relevant fitness values of Equation~(\ref{eq:FittnessChoiceMechGamePairwise})
to construct meta game payoffs. Results based on $100,000$ randomly sampled games are given in
Table~\ref{tab:ExpectedFitness_4Types}.\footnote{Concretely, $100,000$ games were sampled
  repeatedly by choosing independently four integers between 0 and 10 uniformly at random. For
  each game, the action choices of all four choice mechanisms were determined and payoffs from
  all pairwise encounters recorded. (Whenever a choice mechanism would not select a unique
  action in a given game, we recorded unbiased averages over all selected actions.)  The number
  in each cell of Table~\ref{tab:ExpectedFitness_4Types} is the average payoff for the choice
  mechanism listed in the row when matched with the choice mechanism in the column.}



\begin{table}[t]
\centering
\begin{tabular}{ccccc}
  \toprule
 & $\text{reg}, \text{imp}$ 
 & $\text{obj}, \text{imp}$ 
 & $\text{reg}, \text{prc}$ 
 & $\text{obj}, \text{prc}$ \\ 
  \midrule
  $\text{reg}, \text{imp}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
  $\text{obj}, \text{imp}$ & 6.486 & 6.484 & 6.486 & 6.486 \\ 
  $\text{reg}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\  
  $\text{obj}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
   \bottomrule
\end{tabular}                    
\caption{Average evolutionary fitness from Monte Carlo simulations of 100,000 symmetric $2 \times 2$ games}
\label{tab:ExpectedFitness_4Types}
\end{table}

Simulation results obviously reflect Fact~\ref{fact:equivalence2x2} in that all encounters in
which types $(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ or
$(\text{obj}, \text{prc})$ are substituted for one another yield identical results. More
interestingly, Table~\ref{tab:ExpectedFitness_4Types} shows that the maxmin strategy
$(\text{obj}, \text{imp})$ is strictly dominated by the three other strategies: in each column
(i.e., for each kind of opponent), the maxmin strategy is strictly worse than any of the other
three competitors. This has a number of interesting consequences.

Let's begin with a restricted scenario and look at more and more complex cases afterwards. If
we restrict attention to only imprecise belief types, then a state in which every agent has
regret-based preferences is the only \emph{evolutionarily stable state}. More strongly, since
$(\text{obj}, \text{imp})$ is strictly dominated by $(\text{reg}, \text{imp})$, we expect
selection that is driven by expected fitness to invariably weed out $(\text{obj}, \text{imp})$,
the maxmin choice rule, in favor of $(\text{reg}, \text{imp})$, regret minimization. In other
words, when taking average payoffs over the class of random games considered here, regret
minimization is a better classic choice rule than the maxmin rule from an evolutionary point of
view, despite the fact that the former, but not the latter, entertains non-veridical subjective
preferences.

Next, if we look at the competition between all four types represented in
Table~\ref{tab:ExpectedFitness_4Types}, $(\text{reg}, \text{imp})$ is no longer evolutionarily
stable. Rather, given behavioral equivalence (Fact~\ref{fact:equivalence2x2}), types
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$, and $(\text{obj}, \text{prc})$ are all
\emph{neutrally stable} \citep{Maynard-Smith1982:Evolution-and-t}. But since
$(\text{obj}, \text{imp})$ is strictly dominated and so disfavored by fitness-based selection,
we are still drawn to conclude that maxmin behavior is weeded out in favor of a population with
a random distribution of the remaining three types.

Simulation results of the (discrete time) \emph{replicator dynamics}
\citep{TaylorJonker1978:Evolutionary-St} indeed show that random initial population
configurations are attracted to states with only three player types:
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. The
relative proportions of these depend on the initial population. This variability disappears if
we add a small mutation rate to the dynamics. Take a fixed, small mutation rate $\epsilon$ for
the probability that a player's preference type \emph{or} her belief type changes to another
preference type or belief type. The probability that a player randomly mutates into a
completely different player with altogether different preference type and belief type would
then be $\epsilon^2$. With these assumptions about ``component-wise mutations'', numerical
simulations of the (discrete time) \emph{replicator mutator dynamics}
\citep{Nowak2006:Evolutionary-Dy} show that already for very small mutation rates almost all
initial populations converge to a single fixed point in which the majority of players are
regret types. For instance, with $\epsilon = 0.001$, almost all initial populations are
attracted to a final distribution with proportions:\todo{recalculate these numbers for the
  actual 4x4 game!}

\begin{center}
  \begin{tabular}{cccc}
    $(\text{reg}, \text{imp})$ & $(\text{obj}, \text{imp})$ & $(\text{reg},
      \text{prc})$ & $(\text{obj}, \text{prc})$ \\ \hline
    0.289  & 0.021 &   0.398 &    0.289 
  \end{tabular}
\end{center}

What this suggests is that, if biological evolution selects behavior-generating mechanisms, not
behavior as such, it need not be the case that behaviorally equivalent mechanisms are treated
equally all the while. If mutation probabilities are a function of individual components (i.e.,
preference and belief types) of such behavior-generating mechanisms, it can be the case that
certain components are more strongly favored by a process of random mutation and
selection. This is exactly the case with regret-based subjective preferences in the present
example. Since regret-based subjective preferences are much better in connection with imprecise
beliefs than veridical preferences are, the proportion of expected regret minimizers,
$(\text{reg}, \text{prc})$, in the attracting state is substantially higher than that of
expected utility maximizers, $(\text{obj}, \text{prc})$, even though these types are
behaviorally equivalent.

\subsection{Analytical results}
\label{sec:analytical-results}

Results based on the single meta-game in Table~\ref{tab:ExpectedFitness_4Types}, which was
obtained by Monte Carlo simulation, are not fully general and possibly spoiled by random
fluctuations in the sampling procedure. Fortunately, for the case of $2 \times 2$ symmetric
games, the main result that maxmin types $(\text{obj}, \text{imp})$ are strictly dominated can
also be shown analytically for generous general conditions on how likely randomly sampled games
are. 

\begin{proposition} \label{proposition1}
  Let $\mathcal{G}$ be the class of $2 \times 2$ symmetric games with payoffs sampled from a
  set of i.i.d.~values with at least three elements in the support. Then,
  $(\text{reg}, \text{imp})$ strictly dominates $(\text{reg}, \text{prc})$ in the resulting
  meta-game.
\end{proposition}

\begin{corollary} \label{corollary1} Considering only $(\text{obj}, \text{imp})$ and
  $(\text{reg}, \text{imp})$, and letting $\mathcal{G}$ be the class of $2 \times 2$ symmetric
  games with payoffs sampled from a set of i.i.d.~values with at least three elements in the
  support, the unique evolutionarily stable state is a monomorphic population of
  $(\text{reg}, \text{imp})$-players.
\end{corollary}

Proofs for Proposition \ref{proposition1} and Corollary~\ref{corollary1} are given in
Appendix~\ref{sec:proofs}. They tell us that the main conclusions drawn in the previous section
based on the approximated meta game in Table~\ref{tab:ExpectedFitness_4Types} hold more
generally for any class of games in which the probability of encountering a game is the product
of independently sampling four identically distributed payoff values. As before,
$(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ players
will be neutrally stable, and fitness-based selection will tend to weed out maxmin play and
leave us with a population of arbitrary frequency of $(\text{reg}, \text{imp})$,
$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ players. This shows that there is
support for the main conceptual point that we wanted to make: non-veridical subjective
preference relations \emph{can}, under specific circumstances, persist under evolutionary
selection based on objective fitness. Moreover, regret-based preferences can not only persist,
but even be favored by natural selection if agents may have imprecise beliefs. This holds
generally for playing arbitrary $2 \times 2$ symmetric games whose payoffs are sampled
identically and independently at random.

\section{Extensions}
\label{sec:extensions}

How do the basic results from the previous section carry over to more encompassing, richer
models? Section~\ref{sec:more-types} first introduces further conceptually interesting
preference types that have been considered in evolutionary game theory. Section~\ref{sec:n-times-n}
then addresses the case of $n \times n$ games for $n \ge 2$. Finally, to put our results into
a broader perspective, Section~\ref{sec:solitary-decisions} ends with a brief comparison of
game situations with cases of solitary decision making.

\subsection{More types}
\label{sec:more-types}


In evolutionary game theory other types of player have been investigated. A famous example is the \textit{altruistic type} [REF], introduced in the literature in order to explain the possible survival of altruistic behavior. In our framework, we can represent the altruistic type as follows. 


\iffalse

\begin{definition}[Altruistic preference] \label{defn:altpref}

For a game $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, we define the altruistic preference $ \theta^{alt}: A \rightarrow \mathbb{R} $ such that $ \theta^{alt}(a_i,a_{-i})=\pi(a_i,a_{-i}) + \sum_{j \neq i} \pi(a_{j},a_{-j})$.\footnote{A more general formulation [see: REFERENCES] would be to define an $ \alpha$-altruistic type, for $\alpha \in [0,1]$, with subjective utility $ \theta^{\alpha lt}(a_i,a_{-i})=\pi(a_i,a_{-i}) + \alpha \sum_{j \neq i} \pi(a_{j},a_{-j})$. Since we are not interested in the evolution of degrees of altruism here, we simply fix $ \alpha = 1 $.}

\end{definition}

\fi

\begin{definition}[Altruistic type] \label{defn:alttype}

Given a two player game  $ G $, the \textit{altruistic type} $\tau^{alt}$ is characterized by the subjective utility function $\tau^{alt}(G)(a_i, a_j):=\pi_i(a_i,a_j) + \pi_j(a_i,a_j)$.\footnote{A more general formulation would be to define an $ \alpha$-altruistic type, for $\alpha \in [0,1]$, with subjective utility $ \tau^{\alpha lt}(G)(a_i, a_j):=\pi_i(a_i,a_j) + \alpha \pi_j(a_i,a_j)$. Since we are not interested in the evolution of degrees of altruism, here we simply fix $ \alpha = 1 $.}

\end{definition}

\noindent As opposite to the altruistic type, it is possible to define a \textit{competitive type}. 

\iffalse

\begin{definition}[Competitive preference] \label{defn:compref}

For a game $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, we define the competitive preference $ \theta^{com}: A \rightarrow \mathbb{R} $ such that $ \theta^{com}(a_i,a_{-i})=\pi(a_i,a_{-i}) - \sum_{j \neq i} \pi(a_{j},a_{-j})$.

\end{definition}

\fi

\begin{definition}[Competitive type] \label{defn:comtype}

Given a two player game  $ G $, the \textit{competitive type} $\tau^{com}$ is characterized by the subjective utility function $\tau^{com}(G)(a_i, a_j):=\pi_i(a_i,a_j) - \pi_j(a_i,a_j)$.

\end{definition}

Similarly to the evolutionary competition that we simulated before, we can study the
evolutionary fitness of this extended set of players when they are all represented in the
population. Table~\ref{tab:ExpectedFitness_2x2_Full} shows simulation results that approximate the
expected fitness in the relevant meta-game.

\begin{table}[]
\centering
\footnotesize
\begin{tabular}{ccccccccc}
  \hline
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\tau^{com}, \text{imp})$
 & $(\tau^{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\tau^{com}, \text{prc})$
 & $(\tau^{alt}, \text{prc})$ \\ 
  \hline
  $(\text{reg}, \text{imp})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{imp})$ & 6.486 & 6.484 & 6.088 & 6.703 & 6.486 & 6.486 & 6.088 & 6.875 \\
  $(\tau^{com}, \text{imp})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\tau^{alt}, \text{imp})$ & 5.949 & 5.722 & 5.326 & 6.396 & 5.949 & 5.949 & 5.326 & 6.568 \\
  $(\text{reg}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\tau^{com}, \text{prc})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\tau^{alt}, \text{prc})$ & 6.331 & 5.893 & 5.497 & 6.566 & 6.331 & 6.331 & 5.497 & 7.152 \\
   \hline                          
\end{tabular}                      
\caption{Average payoff in simulations of 100,000
  randomly generated $2 \times 2$ symmetric games}
\label{tab:ExpectedFitness_2x2_Full}        
\end{table}   
 
\todo[inline]{is it true that competitive preferences types are behaviorally equivalent under
  either epistemic type? can we show that?}
 
The approximations in Table~\ref{tab:ExpectedFitness_2x2_Full} confirm basic intuitions about
altruistic and competitive types: averaging over randomly sampled games, everybody would like
to have an altruistic opponent and nobody would like to play against a competitive
opponent. Perhaps more surprisingly, altruistic types come up strictly dominated by competitive
types, but competitive types themselves are worse off against all opponent types except against
maxmin players $(\text{obj}, \text{imp})$ than any of the behaviorally equivalent types
$(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
$(\text{reg}, \text{prc})$. 
\iffalse
This is a noteworthy results in the light of the fact that
evolving altruistic preferences have been shown to support cooperative behavior in a single
stage game \myalert{[CITE]}. In contrast,
averaging over payoffs in multiple stage games, like we do here, makes altruistic preferences
prime victims of evolutionary eradication.
\fi
It is easy to see then that the previous result still obtains for the larger meta-game in
Table~\ref{tab:ExpectedFitness_2x2_Full}: $(\text{reg}, \text{imp})$,
$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ are still neutrally stable;
all simulations of the (discrete-time) replicator dynamic on the $8 \times 8$ meta-game from
Table~\ref{tab:ExpectedFitness_2x2_Full} end up in populations consisting of only these players
in arbitrary proportion.

\iffalse

The following proposition (partially) confirms this result.\todo{rephrase to acknowledge all
  epistemic types?}

\begin{proposition} \label{proposition2}

Fix $\Lambda = \lbrace (\text{obj}, \text{imp}), (\text{reg}, \text{imp}), (\tau^{alt}, \text{imp}), (\tau^{com}, \text{imp}) \rbrace$ and $\mathcal{G}$ the class of symmetric $2 \times 2$ games with payoffs sampled from a finite, or compact and convex, set of i.i.d. values. Then, $(\text{reg}, \text{imp})$ is the only evolutionarily stable player in the population.

\end{proposition}

\begin{proof}
See Appendix.\todo{is this there already?}
\end{proof}

\fi


In sum, the presence of other plausible preference types, such as competitive and altruistic
types does not undermine the previous results about the evolutionary drive towards regret-based
subjective preferences.


                                   
\subsection{More actions: $n \times n$ symmetric games}
\label{sec:n-times-n}

Results from Section~\ref{sec:results:-basic-model} relied heavily on
Fact~\ref{fact:equivalence2x2} that, for the special case of $2 \times 2$ symmetric games,
regret minimization $(\text{reg}, \text{imp})$ is behaviorally equivalent with expected
regret minimization $(\text{reg}, \text{prc})$ and expected utility maximization
$(\text{obj}, \text{prc})$. This is no longer true when we look at arbitrary $n \times n$
games with $n \ge 2$. We should therefore see what happens in a broader class of games.

Table~\ref{tab:ExpectedFitness_10x10} gives approximations for expected fitness in a
meta-game over a class of $n \times n$ symmetric games where $n$ is randomly drawn from
$\set{2, \dots, 10}$. Concretely, the numbers in Table~\ref{tab:ExpectedFitness_10x10} are
averages of payoffs obtained in 100,000 randomly sampled games, where each game was sampled by
first picking a number of acts $n \in \set{2, \dots, 10}$ uniformly at random, and then filling
the resulting $n \times n$ payoff matrix of the stage game with i.i.d.~sampled payoffs as
before.


\begin{table}[]
\centering
\footnotesize
\begin{tabular}{ccccccccc}
  \toprule
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\tau^{com}, \text{imp})$
 & $(\tau^{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\tau^{com}, \text{prc})$
 & $(\tau^{alt}, \text{prc})$ \\ 
  \midrule
  $(\text{reg}, \text{imp})$ & 6.567 & 6.570 & 5.650 & 6.992 & 6.564 & 6.564 & 5.593 & 7.409 \\
  $(\text{obj}, \text{imp})$ & 6.476 & 6.483 & 5.896 & 6.818 & 6.484 & 6.484 & 5.850 & 7.124 \\
  $(\tau^{com}, \text{imp})$ & 6.468 & 6.647 & 5.512 & 7.169 & 6.578 & 6.578 & 5.577 & 7.354 \\
  $(\tau^{alt}, \text{imp})$ & 5.968 & 5.923 & 5.363 & 6.685 & 5.975 & 5.975 & 5.086 & 6.973 \\
  $(\text{reg}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{obj}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\tau^{com}, \text{prc})$ & 6.529 & 6.680 & 5.445 & 7.276 & 6.542 & 6.542 & 5.521 & 7.440 \\
  $(\tau^{alt}, \text{prc})$ & 6.450 & 6.337 & 5.772 & 6.978 & 6.457 & 6.457 & 5.479 & 7.500 \\
   \bottomrule                         
\end{tabular}                      
\caption{Average payoff in simulations of 100,000
  randomly generated $n \times n$ symmetric games with $n$ randomly drawn from $\set{2, \dots, 10}$.}
\label{tab:ExpectedFitness_10x10}        
\end{table}

As is to be expected, average payoffs obtained in games with more than 2 acts are higher, as
can be seen by comparing Table~\ref{tab:ExpectedFitness_10x10} with
Table~\ref{tab:ExpectedFitness_2x2_Full}. But increases in approximate expected fitness are not
uniform. Most importantly, the regret minimizing type $(\text{reg}, \text{imp})$ is
strictly dominated by $(\text{reg}, \text{prc})$ and by $(\text{obj}, \text{prc})$ in
the meta-game from Table~\ref{tab:ExpectedFitness_10x10}. This means that while regret
minimization \emph{can} thrive in some evolutionary contexts, there are also contexts where it
is demonstrably worse off.

Still, although this may be bad news for regret minimizing types
$(\text{reg}, \text{imp})$, it is not the case that regret types \emph{as such} are weeded
out by selection. Since, by Fact~\ref{fact:maxEU-minReg}, $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are generally behaviorally equivalent, it remains that
selection based on meta-games constructed from $n \times n$ games will still not eradicate all
regret types. So, although regret types may not be selected for in this case, they are also not
selected against.

On the other hand, there are plenty of ways in which the basic insight from
Proposition~\ref{proposition1} that regret-based preferences can be strictly better than
veridical objective preferences in case of radical uncertainty could make for situations in
which evolution would select for regret types exclusively, even in $n \times n$ games. If, for
example, epistemic types of players are not what biological evolution selects, but rather
something that the particular choice situation would exogenously give us, then regret-based
preference \emph{can} again drive out veridical preferences altogether. For concreteness,
suppose that only preference types compete and that agents' belief types are exogenously
given, in such a way that agents hold flat probabilistic beliefs $\text{prc}$ with probability
$1-p$ and are of type $\text{imp}$ with probability $p$. This transforms the meta-game from
Table~\ref{tab:ExpectedFitness_10x10} into the simpler $4 \times 4$ game in which the payoff
obtained for a preference type is the weighted average over the payoffs of that preference
types in Table~\ref{tab:ExpectedFitness_10x10}. Setting $p = 0.02$ for illustration, we get the
meta-game in Table~\ref{tab:ExogeneousEpistemics}. 

\begin{table}[]
\centering
\begin{tabular}{ccccc}
  \toprule
  & $\text{reg}$ 
  & $\text{obj}$ 
  & $\tau^{com}$
  & $\tau^{alt}$ \\ 
  \midrule
  $\text{reg} $ & 6.926 & 6.926 & 5.942 & 7.757 \\ 
  $\text{obj} $ & 6.924 & 6.924 & 5.948 & 7.751 \\ 
  $\tau^{com }$ & 6.566 & 6.570 & 5.481 & 7.434 \\ 
  $\tau^{alt} $ & 6.463 & 6.461 & 5.478 & 7.469 \\ 
   \bottomrule
\end{tabular}
\caption{Meta-game for the evolutionary competition between preference types when epistemic types are exogenously
  given}
\label{tab:ExogeneousEpistemics}
\end{table}

The only evolutionarily stable state of this meta-game is the regret type. Accordingly, all of
our simulation runs of the (discrete-time) replicator dynamic converged to monomorphic
regret-type populations. The reason why regret types prosper is because they have a substantial
payoff advantage if paired with radical uncertainty. If radical uncertainty is exogenously
given as something unavoidable that happens to agents (e.g., in situation where they really have no
way of narrowing down the probability set), and even if that happens only very
infrequently (i.e., for rather low $p$), regret types \emph{will} outperform veridical
preference types, as well as competitive and altruistic types.


\subsection{Solitary decisions}
\label{sec:solitary-decisions}

\todo[inline]{text in this section and the next is rough and unpolished, only intended to have
  the basic results recorded, so that we can reshape the article around the basic results}

To see how different choice mechanisms behave in evolutionary competition based on solitary decision
problems, we approximated, much in the spirit of meta-games, average accumulated payoffs
obtained in randomly generated solitary decision problems. For our purposes, a decision problem
$\tuple{S, \Acts, u}$ consists of a set of world states $S$, a set of acts
$\Acts$, and a utility function $u \mycolon S \times \Acts \rightarrow
\mathbb{R}$.
We generated arbitrary decision problems by selecting numbers of states and acts
$n_s, n_\act \in \set{2, \dots, 10}$ and then filling the utility table, so to speak, by
independently picking a number $u(s, \act) \in \set{0, 10}$ uniformly at random, just
as we did before with symmetric games. Unlike with two-player games, we need to also sample the
actual state of the world, which we selected uniformly at random from the available states in
the current decision problem. Player types, consisting of epistemic types $\text{prc}$ and
$\text{imp}$ and preference types $\text{reg}$ and $\text{obj}$, give rise to behavior in
the same way as before.\footnote{Obviously, now with $ \text{imp} \equiv \text{imp}(S) :=\lbrace \mu \in \mathbb{R}^{S}: \sum_{s \in S} \mu(s) = 1 \rbrace$.} (It is obviously not possible to carry altruistic or competitive
preference types over to solitary decision making.) As before, we recorded the behavioral
decisions for each of the four relevant player types in each sampled decision problem, and
determined the actual payoff obtained, given the actual state. Average payoff over 100,000 decision
problems are given in Table~\ref{tab:SolitaryDecisions}.

\begin{table}
  \centering
  \begin{tabular}{cccc}
    \toprule
   $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 \\ \midrule
    6.318 & 6.237 & 6.661 & 6.661 \\ \bottomrule
  \end{tabular}
  \caption{Average payoffs over 100,000 simulated solitary decision problems}
  \label{tab:SolitaryDecisions}
\end{table}

Facts~\ref{fact:maxEU-minReg} and \ref{fact:equivalence2x2} still apply:
$(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$ are behaviorally equivalent
in general, and $(\text{reg}, \text{imp})$ is behaviorally equivalent to the former two in
decision problems with 2 states and 2 acts. \todo{show this?} We see this in part in the
results in Table~\ref{tab:SolitaryDecisions} in that the averages for $(\text{reg},
  \text{prc})$ and $(\text{obj}, \text{prc})$ are identical. But since we included decision
problems with more acts and more states as well, the averages for regret minimizers
$(\text{reg}, \text{imp})$ are not identical to the one of $(\text{reg},
  \text{prc})$ and $(\text{obj}, \text{prc})$. They are, in fact, lower, but not as low as
those of the minimax choosers. 

What that means is that basically every relevant result we have seen about game situations is also borne out
when reasoning about solitary decisions. Evolutionary selection based on objective fitness will
not select against regret-based subjective preferences, as these are indistinguishable from
veridical preferences when paired with probabilistic beliefs. But when paired with
non-probabilistic belief types, regret-based preferences actually outperform veridical
preferences. Consequently, if there is a chance, however small, that agents must fall back onto
non-probabilistic beliefs, evolution will actually positively select for non-veridical
regret-based preferences.


\iffalse
\subsection{Arbitrary probabilistic beliefs}
\label{sec:arbitr-prob-beli}

So far, we have assumed that epistemic types $\text{prc}$ hold an unbiased, flat belief. It is
worthwhile considering what happens when this assumption is relaxed and we allow agents with
probabilistic beliefs to make use of the full spectrum of probabilistic beliefs. It should be
clear that quality of beliefs directly impacts prospects of successful decision making: if a
decision maker knows the actual state, or can put a high level of credence on the actual state,
accumulated fitness can be expected to be high. We see this, unsurprisingly, in numerical
simulations. We looked at the average payoff accumulated in 100,000 decision problems sampled
as before, except that for belief types with probabilistic beliefs we sampled a random
probabilistic belief $p$ (from an unbiased Dirichlet distribution) and chose the actual world
state with probability weights $p$. This effectively implements a bias for beliefs of the
decision maker that tend to put more weight on the actual state (although the procedure does
admittedly perverse the natural chicken-and-egg logic in this case that the actual world state
should come first and beliefs be a function of it). As a result, the average payoffs of types
$\tuple{\text{reg}, \text{prc}}$ and $\tuple{\text{obj}, \text{prc}}$ are still the exact same
(because we compare what agents with different preference types would do under the same
beliefs; we are not interested in statistical fluctuations given one type better beliefs by
pure happenstance), but with $7.125$ notably higher than before. In effect, better
probabilistic beliefs lead to better decisions. If agents can learn, reason and extrapolate to
form better probabilistic beliefs, that will help them whenever they take their beliefs into
account. 

But this is orthogonal to the arguments in this paper, where the focus is on possibilities for
the evolution of non-veridical preferences. We know from Fact~\ref{fact:maxEU-minReg} that
regret types and veridical preference types, when paired with probabilistic belief types that
maximize/minimize expectations, are behaviorally equivalent, \emph{no matter what they believe},
as long as they belief the same. So, if learning, insight and statistical knowledge of a
recurrent situation \emph{can} be brought to bear, this will not make evolution select against
regret-based preferences. If, on the other hand, agents have no basis for probabilistic
beliefs, then we have shown that there are general and basic circumstances in which
regret-based preferences can actually be selected, despite their non-veridicality.
\fi


\section{Conclusion} \label{sec:conclusion}


The assumption that players and decision makers maximize their (subjective) preferences is central through all economic literature, and the maximization of actual (objective) payoffs is often justified by appealing to evolutionary arguments and natural
selection. In contrast to the standard view, we showed the
existence of player types with subjective utilities different
from the actual payoffs that can outperform types whose
subjective utilities coincide with the actual payoffs.
While the literature on evolution of preference has focused
on fixed games, or fixed types of games, we have adopted a
more general approach here. We suggested that attention
to meta-games is crucial, because what may be a good
subjective representation in one type of game (e.g., cooperative preferences in the Prisoners Dilemma class) need not
be generally beneficial. Taken together, we presented a variety of plausible circumstances in which evolutionary competition between choice
mechanisms on a larger class of games can favor subjective
preference representations focusing on regret.

Finally, we hope to help the research direction towards a broader perspective on evolutionary game theory, as outlined by recent works in theoretical biology \citep[e.g.,][]{FawcettHamblin2013:Exposing-the-be}. In partcular, the meta-game approach allows the researcher to both explicitly model the relevant features of the environment and shift the target of evolution from plain behavior to more general behavioral patterns, or choice mechanisms.
Several uses and improvements of the present model are still possible in many respects. We can sketch some of them here. For example, the evolutionary investigation of different risk and ambiguity attitudes when competing against each other in an evolutionarily richer environment may bring new insights on both behavioral and theoretic decision making under uncertainty, as well as on many issues in theoretical biology and behavioral ecology.


\appendix

\section{Background on imprecise probabilistic beliefs}
\label{sec:impr-prob-beli}

Consider the following problem. I have, on the desk in front of me, a bag containing either red
or black marbles. There is no further information about the distribution of the two
colors. What is then the probability that I will draw a red marble? As in \citet{walley96}, we
are not talking about physical probability, but rather about epistemic probability, i.e., the
subjective probability that an agent attributes to an event.  In the literature there are two
traditional ways of answering this. According to standard Bayesianism, one should always be
able to determine a precise betting rate (that immediately translates into a precise
probability distribution), which is fair, in the sense that the DM is indifferent between
betting on or against red at the fair rate, and would bet on red at any rate that is more
favourable than the fair rate and against red at any rate that is less favourable. In the case
of the bag on my desk, a strict Bayesian, in the absence of any further information, would
probably rely on the \textit{principle of insufficient reason} and answer that the DM should
have a uniform distribution that considers all the outcomes equipossible.


The second answer involves imprecise probabilities and is related to the literature about
unmeasurable uncertainty. Many authors argued in favor of this kind of approach
\citep[e.g.,][]{levi74,gardsah82,walley96}. Since it is consistent with the information
available that all the marbles are red, or that no marble is red, the resulting model should
take it into account and specify a lower probability of red $\underline{P}(R)$ and an upper
probability of red $\overline{P}(R)$ that define an interval of possible probabilistic beliefs
[$\underline{P}(R), \overline{P}(R)$]. Given the information available in this case we would
have $\underline{P}(R)=0$ and $\overline{P}(R)=1$. This approach appears particularly relevant
in a game theoretical context. Indeed, in a recent paper \citet{BattCerrMM15} write:

\begin{quote}
  Such uncertainty is inherent in situations of strategic interaction. This is quite obvious
  when such situations have been faced only a few times.\todo{insert page number}
\end{quote}

\noindent In evolutionary game theory, for instance, players in a population obviously face
uncertainty about the composition of the population that they are part of, and consequently
about the co-player that they are randomly paired with at each round and about the co-players'
action. 

We therefore represent extreme uncertainty in two different ways that correspond to the two
approaches discussed above: a Bayesian uniform distribution and an imprecise probability
set. The latter represents uncertainty for players who are not able to narrow down the set of
probability distributions to a single one and they consider all of them as possible. The former
precise belief type conceptualizes the uncertainty about the population by using the principle
of insufficient reason and ascribes equal probability to all the possible alternatives. As we
have seen, both options are reasonable and justifiable ways to deal with situations of
uncertainty.

In decision theory, the imprecise probability model is in line with most of the representation
results of decision making under uncertainty \citep[e.g.,][]{gilsch89,KlibMarMuk05,GhirMar02},
and seems justified by empirical observations too. A prominent example is Ellsberg's paradox
(\citet{ells61}). A bag contains 90 marbles, 30 are red and the remaining 60 are either black
or yellow. A marble is to be drawn and DM can win 100 if she guesses the color of the drawn
marble. The DM is offered two different bets. In the first one she can choose to bet either on
red (R) or on yellow (Y), while in the second she can bet either on red or black (RB) or on
yellow or black (YB), as shown in Table \ref{Ellsberg}.

\definecolor{yellow}{RGB}{255,188,1}


\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\cmidrule(l){2-4}
\multicolumn{1}{c}{} & {\color{red}R}   & {\color{yellow}Y}   & B   \\ \cmidrule(l){2-4} 
$f_{{\color{red}R}}$              & 100 & 0   & 0   \\
$f_{{\color{yellow}Y}}$              & 0   & 100 & 0   \\
$f_{{\color{red}R}B}$            & 100 & 0   & 100 \\
$f_{{\color{yellow}Y}B}$             & 0   & 100 & 100 \\ \bottomrule
\end{tabular}
\caption{Ellsberg's paradox}
\label{Ellsberg}
\end{table}

The pattern consistently observed in experimets is:
$f_{{\color{red}R}} \succ f_{{\color{yellow}Y}}$ and
$f_{{\color{red}R}B} \prec f_{{\color{yellow}Y}B}$. This is clearly incompatible with any
precise probabilistic belief about the proportion of black and yellow marbles. This kind of
behavior has been famously axiomatized by Gilboa and Schmeidler in a paper from 1989. The
choice pattern is normally explained in terms of aversion to uncertainty, and the standard
representation is given by means of a set of probability measures together with maxmin rule. We
can think of the DM in Ellsberg's example as maximinimizing expected utility over the
probability set
$
[\underline{P}(R)=\overline{P}(R)=\frac{1}{3},\underline{P}(Y)=0,\underline{P}(B)=0,\overline{P}(Y)=\frac{2}{3},\overline{P}(B)=\frac{2}{3}]
$.
This amounts to calculating for any probability measure contained in the probability set the
expected utility of any available action, and then to choosing the action that guarantees the
highest minimal expected utility. 

Evidence from experimental literature suggests that agents are mainly uncertainty averse
[REF]. In line with empirical data, we assume that players in the population are uncertainty
averse and conform to maxmin expected utility. The resulting behavior is then produced by
maximinimizing the subjective utility given by the player's preference type over the set of
probabilities given by the belief type.


\section{Proofs}
\label{sec:proofs}


The proof of Proposition \ref{proposition1} relies on a partition of $\mathcal{G}$,
and on some lemmas. For brevity, let us denote the regret minimizer
by $R$ and the maximinimizer by $M$, i.e, $R \equiv \kappa(\text{reg}, \text{imp})$ and $M \equiv  \kappa(\text{obj}, \text{imp})$. Recall from Equation \ref{eq:pairwiseEF} that $EF_{\mathcal{G}}(X,Y)$ denotes the expected evolutionary fitness of choice mechanism $X$ against choice mechanism $Y$
with respect to the class of fitness games $\mathcal{G}$.

\vspace{.5cm}


\noindent \textbf{Proof of Proposition \ref{proposition1}.} By definition of strict Nash equilibrium, we have to
show that in the class $\mathcal{G}$ of symmetric $2\times2$ games with payoffs sampled from a set of i.i.d. values with at least 3 elements in the support, it holds:
\begin{itemize}
\item[(i)] $EF_{\mathcal{G}}(R,R)>EF_{\mathcal{G}}(M,R);$
\item[(ii)] $EF_{\mathcal{G}}(M,M)<EF_{\mathcal{G}}(R,M).$
\end{itemize}


\noindent To show
the result we will use the following partition of $\mathcal{G}$.
\begin{enumerate}
\item Coordination games $\mathcal{C}$: $a>c$ and $d>b$;
\item Anti-coordination games $\mathcal{A}$: $a<c$ and $d<b$;
\item Strong dominance games $\mathcal{S}$: aut $(a>c$ and $b>d)$
aut $(a<c$ and $b<d)$;
\item Weak dominance games $\mathcal{W}$: aut $a=c$ aut $b=d$;
\item Boring games $\mathcal{B}$: $a=c$ and $b=d$.
\end{enumerate}
Before proving the lemmas, it is convenient to fix some notation. Let us call $x,y,z$ the 3 elements in the support, and without loss of generality suppose that $ x > y > z $. 
We denote by $C$ a coordination game in $\mathcal{C}$ with
payoffs $a_{C}$, $b_{C}$, $c_{C}$, and $d_{C}$;
similarly for games $A \in \mathcal{A}$, $S \in \mathcal{S}$, $W \in \mathcal{W}$, and $B \in \mathcal{B}$. 
Let us denote by $I_{RC}$ the event that a $R$-player plays action $I$ in the game $C$; and similarly for action $II$, for player $M$, and for games $A$, $S$, $W$ and $B$. We first consider the case of i.i.d. sampling with finite support.


\medskip{}


\begin{lemma} \label{lemma:S-B games}
$R$ and $M$ perform equally well in $\mathcal{S}$
and in $\mathcal{B}$. 
\end{lemma}

\begin{proof}
By definition of regret minimization and maxmin it
is easy to check that whenever in a game there is a strongly dominant
action $a^{\$}$, then $a^{\$}$ is both the maxmin action and the
regret minimizing action. Then, for all the games in $\mathcal{S}$,
$R$ chooses action $a$ if and only if $M$ chooses action $a$. Consequently,
$R$ and $M$ always perform equally (well) in $\mathcal{S}$. In
the case of $\mathcal{B}$ it is trivial to see that all the players
perform equally.
\end{proof}

\medskip{}

\begin{lemma} \label{lemma:W games}
In $\mathcal{W}$, $(R,R)$ is the only (strict) Nash equilibrium.
\end{lemma}

\begin{proof}
Assume without loss of generality that $b=d$. There are two situations that we have to check: (i) $b=d \geq a,c$ and $ a \neq c $; (ii) $b=d \leq a,c$ and $ a \neq c $.
In the first case it
is easy to see that $R$ and $M$ perform equally well. Indeed, given
(i), both $R$ and $M$ choose action $I$ if $a>c$, and action $II$
if $c>a$. If case (ii) holds instead, we have that $R$ plays $I$
if $a>c$ and $II$ if $c>a$ as before, but $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$,
since both $I$ and $II$ maximize the minimal payoff. Consider now
a population of $R$ and $M$ playing games from the class $\mathcal{W}$.
Whenever (i) is the case both $R$ and $M$ perform equally. Suppose that
(ii) is the case and that $a>c$, without loss of generality. Then,  $EF(R,R)=a>\frac{1}{2}a+\frac{1}{2}c=EF(M,R)$, whereas $EF(M,M)=\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d<\frac{1}{2}a+\frac{1}{2}b=EF(R,M)$.
Hence, we have that in general $EF_{\mathcal{W}}(R,R)>EF_{\mathcal{W}}(M,R),\mbox{ and }EF_{\mathcal{W}}(M,M)<EF_{\mathcal{W}}(R,M)$.
\end{proof}

\medskip{}


Since it is not difficult to see that both $(R,R)$ and $(M,M)$ are strict Nash equilibria in $\mathcal{C}$, and that $(R,R)$ and $(M,M)$ are not Nash equilibria in $\mathcal{A}$, the main part of
the proof will be to show that $(R,R)$ is the only (strict) Nash equilibrium in the class $\mathcal{C}\cup\mathcal{A}$, that is:
\begin{itemize}
\item[(i')] $EF_{\mathcal{C}\cup\mathcal{A}}(R,R)>EF_{\mathcal{C}\cup\mathcal{A}}(M,R),$
\item[(ii')] $EF_{\mathcal{C}\cup\mathcal{A}}(M,M)<EF_{\mathcal{C}\cup\mathcal{A}}(R,M).$
\end{itemize}


\noindent This part needs some more lemmas to be proven, but firstly we introduce
the following bijective function $\phi$ between coordination and
anti-coordination games.

\medskip{}

\begin{definition}[$\phi$] \label{def:bijection phi}
The permutation $\phi(a,b,c,d)=(c,d,a,b)$ defines a bijective function $\phi:\mathcal{C}\rightarrow\mathcal{A}$ that for each
coordination game $C\in\mathcal{C}$ with payoffs $(a_{C},b_{C},c_{C},d_{C})$
gives the anti-coordination game $A\in\mathcal{A}$ with payoffs
$(a_{A},b_{A},c_{A},d_{A})=(c_{C},d_{C},a_{C},b_{C})$.
\end{definition}

\medskip{}

\begin{lemma} \label{lemma:probabilities coord-ant}
$P(\phi(C))=P(C)$.
\end{lemma}

\begin{proof}
By definition, each game $C\equiv(a_{C},b_{C},c_{C},d_{C})$
is such that $a_{C}>c_{C}$ and $d_{C}>b_{C}$, and
each game $A\equiv(a_{A},b_{A},c_{A},d_{A})$
is such that $a_{A}<c_{A}$ and $d_{A}>b_{A}$. Given that
$a,b,c,d$ are i.i.d. random variables and that a sequence of i.i.d. random variables is exchangeable,
it is clear that the probability of $(a_{C},b_{C},c_{C},d_{C})$ equals the probability of $(c_{C},d_{C},a_{C},b_{C})$.
Hence, $P(\phi(C))=P(C)$.
\end{proof}

\medskip{}

\begin{lemma} \label{lemma:probabilities actions coord-ant}
Let $P(E)$ be the probability of event $E$, then
it holds that:
\begin{itemize}
\item $P(I_{RC})=P(II_{R\phi(C)})$, and $P(II_{RC})=P(I_{R\phi(C)})$;
\item $P(I_{MC})=P(II_{M\phi(C)})$, and $P(II_{MC})=P(I_{M\phi(C)})$.
\end{itemize}
\end{lemma}

\begin{proof}
It is easy to check that if $b_{C}-d_{C}>c_{C}-a_{C}$,
a $R$-player plays action $I$ in $C$; that if $b_{C}-d_{C}<c_{C}-a_{C}$,
$R$ plays $II$; and that if $b_{C}-d_{C}=c_{C}-a_{C}$,
a $R$-player is indifferent between $I$ and $II$ in $C$, and
we assume that randomizes with $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly,
if $a_{A}-c_{A}>d_{A}-b_{A}$, a $R$-player plays action
$I$ in $A$; if $a_{A}-c_{A}<d_{A}-b_{A}$, $R$
plays $II$; and if $a_{A}-c_{A}=d_{A}-b_{A}$, a
$R$-player is indifferent between $I$ and $II$ in $A$, and randomizes
with $(\frac{1}{2}I;\frac{1}{2}II)$. Consequently, if $b_{C}-d_{C}>c_{C}-a_{C}$,
then $P(I_{RC})=1$, and by definition of $\phi$ we have
$P(II_{R\phi(C)})=1$. Likewise, if $b_{C}-d_{C}<c_{C}-a_{C}$,
then $P(II_{RC})=1=P(I_{R\phi(C)})$; and if $b_{C}-d_{C}=c_{C}-a_{C}$,
then $P(I_{RC})=P(II_{RC})=\frac{1}{2}=P(II_{R\phi(C)})=P(I_{R\phi(C)})$. \\
In the same way, in coordination games we have that if $b_{C}>c_{C}$,
a $M$-player plays $I$; if $c_{C}>b_{C}$, a $M$-player plays
$II$; and if $b_{C}=c_{C}$, $M$ is indifferent between $I$
and $II$, and plays $(\frac{1}{2}I;\frac{1}{2}II)$. In anti-coordination
games instead, if $a_{A}>d_{A}$, $M$ plays $I$; if $a_{A}<d_{A}$,
$M$ plays $II$; if $a_{A}=d_{A}$, $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$.
By definition of $\phi$: $P(I_{MC})=1=P(II_{M\phi(C)})$
if $b_{C}>c_{C}$; $P(II_{MC})=1=P(I_{M\phi(C)})$
if $c_{C}>b_{C}$; and $P(I_{MC})=P(II_{MC})=\frac{1}{2}=P(II_{M\phi(C)})=P(I_{M\phi(C)})$
if $b_{C}=c_{C}$.
\end{proof}

\medskip{}

\begin{lemma} \label{lemma:action implications}
It holds that:
\begin{itemize}
\item $a_{C}>d_{C}\rightarrow(I_{MC}\subset I_{RC})$;
\item $a_{C}<d_{C}\rightarrow(II_{MC}\subset II_{RC})$;
\item $a^{C}=d^{C}\rightarrow I_{MC}=I_{RC}$.
\end{itemize}
\end{lemma}

\begin{proof}
If $b_{C}-d_{C}>c_{C}-a_{C}$, $R$ plays
$I$, and if $b_{C}-d_{C}=c_{C}-a_{C}$, $R$ plays
$(\frac{1}{2}I;\frac{1}{2}II)$, whereas if $b_{C}>c_{C}$,
$M$ plays $I$, and if $b_{C}=c_{C}$, $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$.
Then, $I_{RC}$ implies that $b_{C}-d_{C}\geq c_{C}-a_{C}$,
and $I_{MC}$ implies that $b_{C}\geq c_{C}$. Moreover,
on the assumption that $a_{C}>d_{C}$, it is easy to check
that $b_{C}\geq c_{C}$ implies $b_{C}-d_{C}>c_{C}-a_{C}$.
Hence, in any $C$ with $a_{C}>d_{C}$ it holds that $I_{MC}\mbox{ implies }I_{RC}$,
i.e., $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$.
Instead, it is possible that $a_{C}>d_{C}$, $b_{C}-d_{C}>c_{C}-a_{C}$
and $b_{C}<c_{C}$ hold simultaneously, so that $I_{MC}\nsupseteq I_{RC}$.
Consequently: $a_{C}>d_{C}\rightarrow(I_{MC}\subset I_{RC})$.
By symmetric argument it can be shown that $a_{C}<d_{C}\rightarrow(II_{MC}\subset II_{RC})$
too. \\
Finally, when $a_{C}=d_{C}$ it holds that: $b_{C}-d_{C}>c_{C}-a_{C}$
iff $b_{C}>c_{C}$; $b_{C}-d_{C}<c_{C}-a_{C}$
iff $b_{C}<c_{C}$; and $b_{C}-d_{C}=c_{C}-a_{C}$
iff $b_{C}=c_{C}$. Hence, $a_{C}=d_{C}\rightarrow I_{MC}=I_{RC}$.
\end{proof}

\medskip{}


\noindent We are now ready to prove that:
\begin{itemize}
\item[(i')] $EF_{\mathcal{C}\cup\mathcal{A}}(R,R)>EF_{\mathcal{C}\cup\mathcal{A}}(M,R);$
\item[(ii')] $EF_{\mathcal{C}\cup\mathcal{A}}(M,M)<EF_{\mathcal{C}\cup\mathcal{A}}(R,M).$
\end{itemize}
We can rewrite $EF_{\mathcal{C}\cup\mathcal{A}}(R,R)>EF_{\mathcal{C}\cup\mathcal{A}}(M,R)$
more explicitly as:

\medskip{}

\noindent $\sum_{C \in \mathcal{C}}P(C)[P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot d_{C}+P(I_{RC}\cap II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C}]+\sum_{A \in \mathcal{A}}P(A)[P(I_{RA}\cap I_{RA})\cdot a_{A}+P(II_{RA}\cap II_{RA})\cdot d_{A}+P(I_{RA}\cap II_{RA})\cdot b_{A}+P(II_{RA}\cap I_{RA})\cdot c_{A}] > \sum_{C \in \mathcal{C}}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C}]+\sum_{A \in \mathcal{A}}P(A)[P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot d_{A}+P(I_{RA}\cap II_{MA})\cdot c_{A}+P(II_{RA}\cap I_{MA})\cdot b_{A}]$
\medskip{}

\noindent Then, the next derivation follows. By Lemma \ref{lemma:probabilities coord-ant} and Lemma \ref{lemma:probabilities actions coord-ant}, we can express
everything in terms of $C$ only:
\begin{itemize}

%\item $\sum_{C}P(C)[P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot d_{C}+P(I_{RC}\cap II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C}]+\sum_{A}P(A)[P(I_{RA}\cap I_{RA})\cdot a_{A}+P(II_{RA}\cap II_{RA})\cdot d_{A}+P(I_{RA}\cap II_{RA})\cdot b_{A}+P(II_{RA}\cap I_{RA})\cdot c_{A}] > \sum_{C}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C}]+\sum_{A}P(A)[P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot d_{A}+P(I_{RA}\cap II_{MA})\cdot c_{A}+P(II_{RA}\cap I_{MA})\cdot b_{A}]$

\item $\sum_{C}P(C)[P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot d_{C}+P(I_{RC}\cap II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C} +P(II_{RC}\cap II_{RC})\cdot c_{C}+P(I_{RC}\cap I_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot d_{C}+P(I_{RC}\cap II_{RC})\cdot a_{C}] > \sum_{C}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C}+P(II_{RC}\cap II_{MC})\cdot c_{C}+P(I_{RC}\cap I_{MC})\cdot b_{C}+P(II_{RC}\cap I_{MC})\cdot a_{C}+P(I_{RC}\cap II_{MC})\cdot d_{C}]$

\item $\sum_{C}P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC}))]> \sum_{C}P(C)[a_{C} \cdot (P(I_{RC}\cap I_{MC})+P(II_{RC}\cap I_{MC})) + b_{C} \cdot (P(II_{RC}\cap I_{MC})+P(I_{RC}\cap I_{MC})) + c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap II_{MC})+P(I_{RC}\cap II_{MC}))]$

\end{itemize}

\noindent Now let us split into $a>d$ and $a<d$, and consider $a>d$ first.
Notice that, by Lemma \ref{lemma:action implications}, the case $a=d$ is irrelevant in order to
discriminate between $R$ and $M$. If $a>d$, by Lemma \ref{lemma:action implications} we can eliminate
the cases where $R$ plays $II$ and $M$ plays $I$:

\begin{itemize}

\item $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC}))]> 
\sum_{C_{a>d}} P(C)[a_{C} \cdot P(I_{RC}\cap I_{MC}) + b_{C} \cdot P(I_{RC}\cap I_{MC}) + c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap II_{MC})+P(I_{RC}\cap II_{MC}))]$

\item $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})- P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})- P(I_{RC}\cap I_{MC})) + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0$

\end{itemize}

\noindent We now distinguish between two cases: (1) $ a-c = d-b $ and (2) $  a-c \neq d-b $. Notice that $P(I_{RC}\cap II_{RC}) \neq 0$ if and only if case (1) obtains, and that $a>d$ and (1) imply $II_{MC}$. Then, from (1) we have\footnote{Note that when we have only 3 elements in the support it is not guaranteed that case (1), together with $a>d$, may arise in a coordination game, whereas it is guaranteed that case (2), together with $a>d$, occurs with some positive probability. If we take for instance $x= 5, y= 2, z= 1$, then case (1) cannot obtain, whereas if we take $x= 3, y= 2, z= 1$, both (1) and (2) may obtain ($a=3, b=1, c=2, d=2$ for case (1), and $a=3, b=1, c=2, d=2$ for case (2). Moreover, under the assumption that $a>d$, having 3 elements in the support is a necessary and sufficient condition for case (2) to have positive occurrence in a coordination game. As it will be clear in the following, a positive occurrence of case (2) only is enough for the theorem to hold.}:

\begin{itemize}

\item $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0$

\item $\sum_{C_{a>d}} P(C)[a_{C} \cdot (\frac{1}{4}+\frac{1}{4}) + b_{C} \cdot  (\frac{1}{4}+\frac{1}{4}) + c_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2}) + d_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2})]> 0$

\item $\sum_{C_{a>d}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]> 0$

\end{itemize}

\noindent Since we have assumed $ a-c = d-b $, the last inequality is not satisfied. We have instead: 

$$ \sum_{C_{a>d}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]= 0 $$

\noindent This means that, when (1) is the case, $(R,R)$ is not a strict Nash equilibrium. To prove that $(R,R)$ is strict we then have to show that the inequality is strict when case (2) obtains. \\
When (2) is the case, it is easy to check, by Lemma \ref{lemma:action implications}, that $P(I_{RC} \cap I_{RC})-P(I_{RC}\cap I_{MC})=P(I_{RC}\cap II_{MC})$
and $P(II_{RC}\cap II_{MC})=P(II_{RC} \cap II_{RC})$.
Consequently, we have:

\begin{itemize}

\item $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{RC}\cap I_{RC})- P(I_{RC}\cap I_{MC})) + c_{C} \cdot (P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0$

\item $\sum_{C_{a>d}} P(C)[a_{C} \cdot P(I_{RC}\cap II_{MC}) + b_{C} \cdot  P(I_{RC}\cap II_{MC}) - c_{C} \cdot P(I_{RC}\cap II_{MC}) - d_{C} \cdot P(I_{RC}\cap II_{MC})]> 0$

\item $\sum_{C_{a>d}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0$

\end{itemize}
\medskip{}

\noindent We know that $I_{RC}$ implies that $a_{C}-c_{C}\geq d_{C}-b_{C}$.
%and $II_{MC}$ implies that $b_{C}\leq c_{C}$. 
Since we have assumed that $a_{C}-c_{C}\neq d_{C}-b_{C}$, we have that $a_{C}-c_{C} > d_{C}-b_{C}$. Hence, the inequality 
$$\sum_{C_{a>d}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0$$
is satisfied. So, when (2) obtains, $(R,R)$ is a strict Nash equilibrium.
%From $b_{C}-d_{C}\geq c_{C}-a_{C}$, it is easy to see that $\sum_{G_{a>d}^{C}}P(G_{a>d}^{C})\cdot P(I_{RC}\cap II_{MC})\cdot[a_{C}-c_{C}]\geq\sum_{G_{a>d}^{C}}P(G_{a>d}^{C})\cdot P(I_{RC}\cap II_{MC})\cdot[d_{C}-b_{C}]$ holds. When the event $I_{RC}\cap II_{MC}$ obtains on the assumption that $a_{C}>d_{C}$, there are only three possible payoff orderings: $a>c\geq d>b$, $a>d>c>b$, and $a>d>c=b$. If games are randomly sampled from a finite set with i.i.d. values for $a,b,c$ and $d$, then $a>d>c=b$ occurs with positive probability, and the last formula of the derivation holds with strict inequality too. Hence, RM is evolutionarily stable when the value set for $a,b,c,d$ is finite. \\ 
%If i.i.d. values are sampled from a compact and convex set instead, it is again easy to see that $\forall a,d,c\ \exists\epsilon$ such that $\forall b\in(d,d+\epsilon)$ the above formula holds with strict inequality. We can simply take $\epsilon\equiv\frac{a-c}{2}$, and the interval $(d,d+\epsilon)$ always has positive probability. Hence, RM is evolutionarily stable when the value set for $a,b,c,d$ is a compact and convex set too. \\ 
Symmetrically, from $a<d$ and by distinguishing between the two cases (1) and (2) as before, in the end  we get:

\begin{itemize}

\item[(1)] $\sum_{C_{a<d}} P(C)[-\frac{1}{2}a_{C}- \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]= 0$; and

\item[(2)] $\sum_{C_{a<d}} P(C)[P(II_{RC}\cap I_{MC})\cdot (-a_{C} - b_{C} + c_{C} + d_{C})]> 0$.

\end{itemize}


\noindent Hence, we can conclude that $(R,R)$ is a strict Nash equilibrium in the class $\mathcal{C}\cup\mathcal{A}$. \\
In case of i.i.d. sampling with infinite support instead, games in $\mathcal{B} $ and $\mathcal{W} $ never arise, and the proof is the same for the remaining games in $\mathcal{S}$,  $\mathcal{C} $ and $\mathcal{A} $.

\medskip{}

The second half of the result is that $(M,M)$ is not a (strict) Nash equilibrium in $\mathcal{C}\cup\mathcal{A}$: $EF_{\mathcal{C}\cup\mathcal{A}}(M,M)<EF_{\mathcal{C}\cup\mathcal{A}}(R,M)$.
As before, we write it more explicitly as:

\medskip{}


\noindent $\sum_{C}P(C)[P(I_{MC}\cap I_{MC})\cdot a_{C}+P(II_{MC}\cap II_{MC})\cdot d_{C}+P(I_{MC}\cap II_{MC})\cdot b_{C}+P(II_{MC}\cap I_{MC})\cdot c_{C}]+\sum_{A}P(A)[P(I_{MA}\cap I_{MA})\cdot a_{A}+P(II_{MA}\cap II_{MA})\cdot d_{A}+P(I_{MA}\cap II_{MA})\cdot b_{A}+P(II_{MA}\cap I_{MA})\cdot c_{A}] < \sum_{C}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot b_{C}+P(II_{RC}\cap I_{MC})\cdot c_{C}]+\sum_{A}P(A)[P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot d_{A}+P(I_{RA}\cap II_{MA})\cdot b_{A}+P(II_{RA}\cap I_{MA})\cdot c_{A}]$.
\medskip{}

\noindent When $a>d$, similarly to the above derivation, we get: 

\medskip{}
\noindent $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC})) + c_{C} \cdot (P(II_{MC}\cap I_{MC}) +P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})+P(II_{MC}\cap I_{MC})- P(II_{RC}\cap II_{MC}))]< 0$
\medskip{}

\noindent We now distinguish between (1) $b=c$, (2) $b>c$, and (3) $b<c$. Notice that either (1) or (2), together with $a>d$, implies $I_{RC}$. Then we obtain\footnote{Note that here, when we only have 3 elements in the support, case (2) is impossible. However, we can insure that cases (1) and (3) will eventually obtain. Again, this will be enough for the theorem to hold.}:
\begin{itemize}

\item[(1)] $\sum_{C_{a>d}} P(C)[-\frac{1}{2}a_{C} - \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC}))]= 0$;

\item[(3)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (- P(I_{RC}\cap II_{MC})) + b_{C} \cdot  (- P(I_{RC}\cap II_{MC})) + c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] \leq 0$.

\end{itemize}

\noindent When $a<d$, the derivation proceeds symmetrically and we get: 

\begin{itemize}

\item[(1)] $\sum_{C_{a<d}} P(C)[\frac{1}{2}a_{C} + \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a<d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + c_{C} \cdot (- P(II_{RC}\cap I_{MC})) + d_{C} \cdot (- P(II_{RC}\cap I_{MC}))] \leq 0$

\item[(3)] $\sum_{C_{a<d}} P(C)[c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] = 0$.

\end{itemize}


\noindent Finally, we can conclude that $(M,M)$ is
not a (strict) Nash equilibrium, and therefore $(R,R)$ is the unique strict Nash equilibrium. \\
As before, when we have i.i.d. sampling with infinite support, games in $\mathcal{W}$ and $\mathcal{B}$ never occur, and the proof is the same for all the other cases. Hence, both when the support of $a,b,c,d$
is finite, and when the support is infinite, $(R,R)$ is the only (strict) Nash equilibrium. This concludes the proof. $\dashv$

\vspace{1cm}




\noindent \textbf{Proof of Corollary \ref{corollary1}.} Let us recall the definition of evolutionarily stable strategy and neutrally stable strategy.

\medskip{}

\begin{definition}[ESS-NSS] \label{def:ESS-NSS}
A strategy $s^{\$}$ is \emph{evolutionarily stable
(ESS)} if for any other strategy $s$:
\begin{enumerate}
\item $\pi(s^{\$},s^{\$})>\pi(s,s^{\$})$, or
\item $\pi(s^{\$},s^{\$})=\pi(s,s^{\$})$ and $\pi(s^{\$},s)>\pi(s,s)$.
\end{enumerate}
\noindent A strategy $s^{\$}$ is \emph{neutrally stable (NSS)} if
for any other strategy $s$:
\begin{enumerate}
\item $\pi(s^{\$},s^{\$})>\pi(s,s^{\$})$, or
\item $\pi(s^{\$},s^{\$})=\pi(s,s^{\$})$ and $\pi(s^{\$},s)\geq\pi(s,s)$.
\end{enumerate}
\end{definition}

\medskip{}

\noindent Then we extend Definition \ref{def:ESS-NSS} to include choice mechanisms. 

\begin{definition}[ESM-NSM] \label{def:ESM-NSM}
Given a class of games $\mathcal{G}$, a choice mechanism $(\gamma, \kappa)$ is \emph{evolutionarily stable
(ESM)} if for any other choice mechanism $(\gamma', \kappa')$:
\begin{enumerate}
\item $EF_{\mathcal{G}}((\gamma, \kappa),(\gamma, \kappa))>EF_{\mathcal{G}}((\gamma', \kappa'),(\gamma, \kappa))$, or
\item $EF_{\mathcal{G}}((\gamma, \kappa),(\gamma, \kappa))=EF_{\mathcal{G}}((\gamma', \kappa'),(\gamma, \kappa))$ and $EF_{\mathcal{G}}((\gamma, \kappa),(\gamma', \kappa'))>EF_{\mathcal{G}}((\gamma', \kappa'),(\gamma', \kappa'))$.
\end{enumerate}
\noindent A choice mechanism $(\gamma, \kappa)$ is \emph{neutrally stable (NSM)} if
for any other choice mechanism $(\gamma', \kappa')$:
\begin{enumerate}
\item $EF_{\mathcal{G}}((\gamma, \kappa),(\gamma, \kappa))>EF_{\mathcal{G}}((\gamma', \kappa'),(\gamma, \kappa))$, or
\item $EF_{\mathcal{G}}((\gamma, \kappa),(\gamma, \kappa))= EF_{\mathcal{G}}((\gamma', \kappa'),(\gamma, \kappa))$ and $EF_{\mathcal{G}}((\gamma, \kappa),(\gamma', \kappa'))\geq EF_{\mathcal{G}}((\gamma', \kappa'),(\gamma', \kappa'))$.
\end{enumerate}
\end{definition}

\noindent From Proposition \ref{proposition1} we know that: 

\begin{itemize}
\item[(i)] $EF_{\mathcal{G}}(R,R)>EF_{\mathcal{G}}(M,R);$
\item[(ii)] $EF_{\mathcal{G}}(M,M)<EF_{\mathcal{G}}(R,M).$
\end{itemize}

\noindent Then, from Definition \ref{def:ESM-NSM} it immediately follows that the unique evolutionarily stable state is a monomorphic population of $R$-players. $\dashv$




\printbibliography[heading=bibintoc]








\end{document}



