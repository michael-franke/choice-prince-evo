\documentclass[fleqn,reqno,11pt]{article}

%========================================
% Packages
%========================================

\usepackage[]{helpers/mypackages}
%\usepackage[natbib=true,style=authoryear-comp,backend=bibtex,doi=false,url=false]{biblatex}
% \bibliography{helpers/MyRefLocal}
\bibliography{choicePrince}
\usepackage{helpers/myenvironments}
\usepackage{helpers/mycommands}

\usepackage{todonotes}
\usepackage{subcaption}


%========================================
% Standard Layout
%========================================

\usepackage{bigints}
\usepackage{MnSymbol}
\usepackage{a4wide}

\usepackage[T1]{fontenc}


% Itemize
\renewcommand{\labelitemi}{\large{$\mathbf{\cdot}$}}    % itemize symbols
\renewcommand{\labelitemii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiv}{\large{$\mathbf{\cdot}$}}
% Description
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\textsc{#1}}

% Figure Captions
\usepackage{caption} % use corresponding myfiguresize!
\setlength{\captionmargin}{20pt}
\renewcommand{\captionfont}{\small}
\setlength{\belowcaptionskip}{7pt} % standard is 0pt

\newcommand{\myalert}[1]{\textcolor{red}{#1}}

\title{Smart Representations: {R}ationality, Regret, Evolution \& the Environment}
\author{Paolo Galeazzi \& Michael Franke} \date{current version: November 5, 2015}

\begin{document}

% \textbf{some general notes and remarks and todo's}

% \begin{itemize}
% \item micha: let's have a term for pairs (regret type, epistemic type); I'm inclined to use ``player
%   type'', but that's what you reserved for regret type; maybe use \myalert{meta-type}?
% \item can we reformulate the main result (proposition 1) in terms of ``strict dominance''
%   and/or strict Nash equilibrium? sometimes the proof is not ideally precise because it
%   actually shows something stronger, namely strict dominance, but everything is formulated in
%   terms of stability; using dominance is also better for arguments about evolutionary dynamics:
%   dynamics can be attracted by states that are not evolutionarily stable (even if there is an
%   ESS), but dominated strategies will die out in any case;
% \item should we give definitions for ESS and NSS in the main text?
% \item should we include definitions of the replicator dynamic and/or the replicator mutator
%   dynamic?
% \item the most important issue with this text is still that we need to motivate why we care
%   about non-probabilistic beliefs and/or security strategies; everything else will fall into
%   place when that is done
% \item an important ingredient in our argument pro regret is that radical uncertainty could be
%   exogenously given; as it stands this occurs rather ``suddenly'' in the paper and should
%   ideally be prepared and motivated earlier (e.g., in Sections 2 and 3)
% \item a major contribution of our paper is the ``meta-game'' approach, but in the current
%   version I feel that this is not stressed sufficiently; the introduction and the beginning of
%   section 3 could do a bit more to carve out this idea as something generally cool; especially
%   the formula from 3.4 could occur earlier (albeit in more general notation);
% \end{itemize}

% \newpage


\maketitle

\section{Introduction / Motivation}
\label{sec:intr--motiv}

Evolutionary game theory has become an established philosophical tool for probing into
conceptual issues whose complexity requires mathematical modeling. However, most applications
look at evolutionary selection of behavior in a single \emph{stage game}. There are two common
simplifications in this approach. Firstly, the stage game is usually considered fixed, closed
and immutable. Secondly, what evolves are behavioral strategies for the stage game. It has
consequently been argued that, instead of focusing on a fixed, immutable stage game and the
behavioral selection for that stage game alone, we should rather study the evolutionary
competition of \emph{choice mechanisms} under selective pressure in a rich and variable
environment \citep[e.g.][]{FawcettHamblin2013:Exposing-the-be,McNamara2013:Towards-a-Riche}.
But only little attention has been paid to modeling the evolutionary competition between choice
mechanisms in a game-theoretic framework \citep[but
see][]{ZollmanSmead2010:Plasticity-and-,SmeadZollman2013:The-Stability-o}.\todo{More references
  Harley 1981!!!, O Conor \dots}.

If agents deal with a rich and varied environment, they face many different choice
situations. A crucial part of a choice mechanism (conceived of as a mapping from a choice
situation to a concrete choice of action) is a \emph{subjective representation} of that choice
situation, i.e., the agents' manner of representing their preferences and the manner of forming
decision relevant beliefs. Since almost any reasonable learning or decision-making process will
make use of some (subjective, agent-internal (possibly unconscious)) representation of the
quality of choice options, the question of which such representations are ecologically valuable
(i.e., lead to high fitness) is central and has been subject of recent interest
(\textcolor{red}{CITE literature on evolution of preferences}). Na\"ive intuition might
convince us that preferences that veridically represent the actual, true fitness measure that
evolutionary selection is based upon must surely be the best. We demonstrate that, if we take
into account how agents represent uncertainty, this na\"ive conclusion is premature. Under
certain reasonable epistemic conditions, non-veridical preference representations in terms of
regret can be evolutionarily beneficial.

To show this, we study the evolutionary competition of choice mechanisms, or more specifically
the competition of subjective representations of choice situations, in a variable
environment. Concretely, we look at evolutionary ``meta-games'' whose ``acts'' are types of
subjective representations of games and whose ``payoffs'' represent the average expected
fitness that choice mechanisms using these representation types would accrue when playing
arbitrary games (from a given class, with a given occurrence probability) against other
subjective representations of games. In this sense, a meta-game captures (the modeller's)
assumptions about the relevant statistics of the environment in which evolutionary forces
operate, while we are still able to use standard methods from evolutionary game theory, like
stability or evolutionary dynamics.



\todo[inline]{add overview}

\section{Rationality, subjectivity, beliefs \& preferences}
\label{sec:rati--subj}

The standard textbook definition of \textit{rationality} in economics and decision theory
traces back to the seminal work of \citet{Neumannvon-NeumannMorgenstern1944:Theory-of-Games}
and \citet{Savage1954:The-Foundations}. It says that choice is rational only if it maximizes
(subjective) expected utility.

\begin{definition}[Rational choice from maximization of expected utility]
  \label{def:rationality}
  Let $S$ be a (finite) set of states of the world and $A$ a (finite) set of actions. Given a
  probability distribution $P$ over $S$ and a utility function
  $u \mycolon S \times A \rightarrow \mathbb{R} $, an action $a^* \in A$ is a rational choice
  only if it maximizes the \emph{(subjective) expected utility}
  $\EU(a) = \sum_{s \in S} P(s) \ u(s, a)$.
\end{definition}

\noindent Expected utility is subjective in the sense that it is a function of subjective
preferences and subjective beliefs of the decision maker (DM). To wit, a choice can be
rational, i.e., the best choice from the DM's point of view, even if based on peculiar beliefs
and/or aberrant preferences. % (Indeed, while
% \citeauthor{Neumannvon-NeumannMorgenstern1944:Theory-of-Games} assumed that probabilities were
% objectively given, it was \citet{Savage1954:The-Foundations} who famously extended von Neumann
% and Morgenstern's axiomatization to include a subjective notion of probabilistic beliefs on top
% of the subjective notion of preference that von Neumann and Morgenstern started out with.)

If beliefs and preferences are subjective, there is room for \emph{rationalization} or
\emph{redescriptionism} of observable behavior. Prospect theory is, perhaps, the most prominent
example. \citet{KahnemannTversky1979:Prospect-Theory} looked at experimental data from human
choices in suitably defined tasks and demonstrated that what appears to be violations of
rationality norms can be explained on the assumption that subjects' beliefs and preferences
deviate systematically from the objectively given parameters in the presented choice task
(e.g., by taking into account reference points and diminishing returns for both gains and
losses).  Similarly, tinkering with the notion of subjective utility, in particular by
including social preferences such as fairness, reciprocity or similar, allows us to describe as
rational empirically observed behavior in situations of social decision making, such as the
prisoners' dilemma or public goods games, that might otherwise appear irrational (e.g. Fehr and
Schmidt, Charness and Rabin, etc.).\todo{@Paolo: please include proper references}

It could be objected that, without additional constraints, the notion of rationality is likely
to collapse, as it seems possible to deem rational everything that is observed, given the
freedom to adjust beliefs and preferences at will. But there are many ways in which ascriptions
of beliefs and preferences are constrained by normative considerations of rationality. To begin
with, if the DM's uncertainty is about objective chance, it can be argued that subjective
beliefs should conform (at least roughly) to objective chance. When the DM's uncertainty cannot
be anchored in an objective notion of chance, then temporal consistency in his decision making
should at least constrain his subjective beliefs, else he fall prey to exploitation by Dutch
books. Similarly, in certain situations it may make sense to postulate on normative grounds
that subjective preferences should be oriented towards tracking objective utility, e.g.,
fitness. For instance, in their classic work \myalert{(Alchian, Friedman)} argue that profit
maximization is a reasonable assumption for characterizing outcomes in a competitive market
because only firms behaving in a manner that is consistent with profit maximization will
survive in the long run.\todo{@Paolo: please add references}

An alternative to descriptive vs.~normative views on rationality is \emph{adaptationism}
\citep[e.g.][]{HagenChater2012:Decision-Making}. \todo{more references} Adaptationism aims to
explain choice behavior by appeal to evolutionary considerations. The key idea is that decision
makers (human and other life forms) have evolved choice mechanisms which were fitness enhancing
in the relevant domains of choice making, at the relevant time of evolution. When it comes to
preferences, this is clearly related to Alchian and Friedman's argument that firms
\emph{should} optimize that which makes them ``live,'' so to speak. For biological agents, the
argument would be that subjective preferences \emph{should} track whatever determines objective
fitness.

We would like to put this latter idea to the test. We present a formal argument intended to
support the conclusion that subjective preferences that differ from objective fitness can be
adaptive. We argue that it is important to take into account the potential mutual dependence of
subjective preferences and subjective beliefs in adaptationist approaches to decision
making. In other words, we focus on the evolutionary competition between \emph{subjective
  representation schemes}: general ways of forming subjective preferences and types of
subjective beliefs. We show that subjective, non-veridical preference representations based on
a notion of regret \citep[e.g.][]{Savage1951:The-theory-of-s,LoomesSugden1982:Regret-Theory:-}
can be adaptive and even outperform objective, veridical representations of the actual, true
fitness, when agents, at least on occasion, hold \emph{imprecise probabilistic beliefs}
(\textcolor{red}{CITATIONS}) and make decisions based on a security choice rule (here: the
maximin rule applied to expected utility). The next section introduces all of these notions
(precise/imprecise beliefs, veridical/regret representations, \dots) in detail.

% The method by which we want to advance our argument is to integrate ideas from the recent
% literature on preference evolution (CITE) with basic tenets of adaptationism: we are interested
% in studying the evolutionary competition of choice mechanisms, conceived here as pairs of
% subjective preference and subjective belief representations. Evolutionary competition between
% choice mechanisms is driven by the expected objective fitness that each choice mechanisms
% accrues when put to use repeatedly in a range of choice situations. We focus on the more
% complex case of multi-agent decision making first, i.e., game situations. Solitary decision
% making is then easy to deal with subsequently. The next section introduces the choice
% mechanisms we focus on. After that, Section~\ref{sec:model} introduces the ``meta-game''
% modeling approach.


\section{Choice mechanisms: preferences, beliefs and action choice}
\label{sec:basic-notions}

A strategic game is a tuple $ G=\tuple{ N, (A_i , \pi_i)_{i \in N} }$ with $N$ a set of
players, $A_i$ a set of acts and $\pi_i$ a payoff function for each player $i \in N$. For the
most part, this paper will focus on symmetric games with two players. A two-player game is
symmetric if players have the same action set, $A_1 = A_2$, and payoffs are symmetric:
$\pi_1(\myact{k}, \myact{l}) = \pi_2(\myact{l}, \myact{k})$ for all $k$ and $j$. An example is
given in Figure~\ref{coordgame1}, which shows a coordination game. The numbers give the payoff
for the player choosing a row act against an opponent choosing a column act. Since the game is
symmetric, it suffices to give just these numbers.

With some due simplification, there are two types of approaches to solving games. One looks at
a collection of players (e.g., a whole population of them) and considers sets of choices of all
players that are, in some sense or other, stable or in equilibrium. Another approach is to
focus on each individual agent's perpective and specify which of her actions the agent
\emph{should} choose if she is rational. We will look at the latter here; the former will be
relevant only later, when turn to evolutionary competition between choice mechanisms in
Section~\ref{sec:model}. We will first look at what we call \emph{choice rules}: classical
proposals of how rational agents should choose acts. We will then introduce our notion of
\emph{choice mechanism} which makes explicit several elements that choice rules might brush
under the table, in particular the crucial notion of a subjective representation scheme whose
evolutionary selection we are most interested in.


\subsection{Choice rules} 
\label{sec:choice-rules}

For the purposes of this paper, we think of a choice rule as a function that takes a
(strategic) game as input and outputs a choice of action for that game. The literatures has
suggested a number of choice rules that aspire to capture a normative element of rational
choice. For reasons that will become clear as we go along, we focus here on three classical
choice rules: (i) maximin, (ii) Laplace and (iii) regret minimization.

Rational choice from maximization of expected utilities, as given in
Definition~\ref{def:rationality}, is \emph{not} a choice rule in the present sense. Although
the notion applies to game situations, of course, it requires an additional component, namely a
belief of the DM, before it can say which acts the DM should choose. Let
$A_{-i} = \bigtimes A_j$ for all $j \neq i$ be the set of all \emph{profiles} of acts for all
players except $i$. If player $i$ has a belief $\mu_i \in \Delta(A_{-i})$ about the behavior of
his opponents, he chooses rationally only if his choice $a^*_i$ maximizes expected utility
$\EU(a) = \sum_{\act_{-i} \in \Acts_{-i}} \mu_i(\act_{-i}) \myts \pi_i(a_i, a_{-i})$. So this
requires a probabilistic belief about the opponents' action choices. In the example from
Figure~\ref{coordgame1}, a rational player would choose act $I$ exactly when he believes that
his opponent will choose act $I$ with a probability at least $\nicefrac{2}{3}$.

Perhaps the most well-known choice rule that does not require explicitly stating the DM's
beliefs is the \emph{maximin rule}. It tells our agent to choose act $\act^*$ only if it
maximizes payoffs for all worst case scenarios. For a strategic game with 2 players this can be
formalized as
$\act^* \in \argmax_{\myact{i} \in \Acts_i} \min_{\myact{j} \in \Acts_j} \pi_i(\myact{i},
\myact{j})$.
For the example in Figure~\ref{coordgame1}, the maximim rule looks at the minima in each row,
asking: what is the worst possible outcome for each row act? It is $0$ in either
case. Consequently, the maximin rule would not favor either act; players choosing by the
maximin rule would be indifferent between acts $I$ and $II$.

Another well-known choice rule is \emph{Laplace's rule} which prescribes to choose $\act^*$ if
$\act^* \in \argmax_{a \in A} \sum_{a'} \pi(a, a')$. Laplace's rule is equivalent to rationally
choosing under a \emph{flat belief} that assigns equal probability to all of the opponent's
choices. In the coordination game of Figure~\ref{coordgame1}, Laplace's rule selects act $II$.

The final choice rule that is relevant for our purposes here is \emph{regret
  minimization}. Regret theory has been introduced in decision theory by
\citet{Savage1951:The-theory-of-s} and Niherens [], and later developed by Bell [1982], Loomes
and Sugden [1982], and Fishburn [1982] independently.\todo{fix references}\ Lately,
\citet{HalpernPass2012:Iterated-Regret} showed how the use of regret minimization can give
solutions to game theoretical puzzles (like the Traveller's dilemma) in a way that is closer to
everyday intuition and empirical data. In this paper the notion of regret is defined as in
\citet{HalpernPass2012:Iterated-Regret}:

\begin{definition}[Regret, regret minimization] \label{defn:regret} Given a strategic game
  $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, the \emph{pairwise regret} of player $i$'s
  act $a_i$ against profile $a_{-1}$ is
  $\text{reg}(a_i,a_{-i}):= \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $, where
  $a_i^\$ \in \argmax_{a_i \in A_i} \pi_i(a_i, a_{-i})$ denotes $i$'s best reply to the
  opponents' actions $a_{-i}$. The \emph{regret} of $a_i$ is then
  $\text{reg}(a_i):= \text{max}_{a_{-i}\in A_{-i}} \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $. An
  action $a^{*}_i $ is regret minimizing if
  $a^{*}_i \in \text{min}_{a_i} \text{max}_{a_{-i}} \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $.
\end{definition}


To illustrate, Figure~\ref{coordgame1reg} shows the pairwise regrets and the regrets of each
row-player action for the coordination game in Figure \ref{coordgame1}. We have
$\text{reg}(I)=2$, because when the other player chooses act $I$ the pairwise regret
$\text{reg}(I,I)$ is $0$ since $I$ is a best reply to act $I$, and when the other player
chooses act $II$ the pairwise regret $\text{reg}(I, II)$ is $2$ since
$\pi_i(II,II)-\pi_i(I,II)=2$. With the same reasoning, $\text{reg}(II)=1$. A regret minimizing
player would therefore choose act $II$.


\begin{figure}

  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 1 & 0 \\
      II & 0 & 2\\
      \bottomrule
    \end{tabular}
    \caption{Coordination game $G^C$}
    \label{coordgame1}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      & $\text{reg}( \cdot, I)$ & $\text{reg}(\cdot, II)$ & $\text{reg}(\cdot)$ \\
      \midrule
      I  & 0 & 2 & 2 \\ 
      II & 1 & 0 & 1\\
      \bottomrule
    \end{tabular}
    \caption{Pairwise regrets and regrets}
    \label{coordgame1reg}
  \end{subfigure}
  \caption{A coordination game (left) and regrets associated with each row-player acts (right).}
    \label{coordgame1mainFig}
\end{figure}



% \begin{example}[The Traveller's Dilemma \citep{HalpernPass2012:Iterated-Regret}]
% \label{example:TravelersDilemma}
% Let us now consider the Traveller's Dilemma case. The story behind the game goes as
% follows. There are two travellers, Ann and Bob, who lost their luggages. Ann and Bob had
% exactly the same luggage and they had insured the luggages with the same insurance company. The
% insurance policy is that the two travellers have to separately claim a certain amount between
% 2\$ and 100\$ for the reimbursement, and if they both claim the same amount then they will be
% reimbursed by that amount. Otherwise, if one of them claims more than the other, then the one
% who claimed the higher amount will get the lower amount minus 2\$, while the one who claimed
% the lower amount will get what (s)he claimed plus 2\$. The game is symmetric, and the payoff
% function for both players is:
% \begin{align*}
%   \pi_i(a_i,a_{-i}) = \begin{cases} a_i \text{ \hspace{1cm} if } a_i = a_{-i} \\ a_i + 2 \text{
%       \hspace{.4cm} if } a_i < a_{-i} \\ a_{-i} -2 \text{ \hspace{.25cm} if } a_{-i} <
%     a_i \end{cases}
% \end{align*}
% By a simple computation we have:  $\forall a_i \in \lbrace 96\$, ..., 100\$ \rbrace \text{, }
% \text{reg}(a_i)=3 $ and $\forall a_i \in \lbrace 2\$, ..., 95\$ \rbrace \text{, }
% \text{reg}(a_i)>3$. If we now eliminate all the actions that do not minimize regret in the
% first place and iterate the regret minimization only on the action set $\lbrace 96\$, ...,
% 100\$ \rbrace$, we get that 97\$ is the action that minimize regret. Indeed, $\text{reg}(97)=2$
% and $\forall a_i \in \lbrace 96\$, 98\$, 99\$, 100\$ \rbrace \text{, } \text{reg}(a_i)=3
% $. Hence, one step of regret minimizing reasoning eliminates all the actions smaller than 96\$,
% and if we iterate the process once more the unique outcome is 97\$. Finally, it is worth
% noticing that the only rationalizable equilibrium of the game corresponds to the outcome
% $(2,2)$, that is consequently the only Nash equilibrium, the only perfect equilibrium, and the
% only sequential equilibrium of the game.

% \begin{itemize}
% \item \textcolor{red}{add here:}
%   \begin{itemize}  
%   \item outcome of rationalizability (max-EU)
%   \item outcome of maximin rule
%   \item outcome of Laplace rule
%   \end{itemize}

% \end{itemize}

%  $ \medsquare $
  
% \end{example}

Regret minimization is intimately related to the maximin rule. Let the \emph{negative regret
  transformation} of a strategic game $G$ be the game $G'$ derived from $G$ by replacing the payoff
function $\pi$ of $G$ with $\pi'$ such that $\pi'_i(a_i, a_{-i}) = - reg(a_i,a_{-1})$, i.e.,
the payoff of all players is the pairwise negative regret (under the payoffs from $G$). For
example, the negative regret transform of the game in Figure~\ref{coordgame1} would have the
following payoff matrix, i.e., the negatives of the pairwise regrets given in
Figure~\ref{coordgame1reg}:

\begin{center}
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 0 & -2 \\
      II & -1 & 0\\
      \bottomrule
    \end{tabular}
\end{center}

\noindent Acts selected by the maximin rule for the negative regret transformation of $G$ are
always exactly the acts selected by regret minimization for the original game $G$.\todo{can we
  cite a paper that proves this? should we give a proof ourselves?}

This means that we can think of the maximin rule and regret minimization as essentially the
same operation on different subjective representations of preference: while maximin considers
actual objective payoffs (as defined by the modeller's game), regret minimization considers
non-veridical, regret-based subjective preferences. As the example of Figure~\ref{coordgame1}
shows, different subjective preferences can give rise to different choice prescriptions all
else equal. While maximin is indifferent between $I$ and $II$, regret minimization uniquely
selects $II$.

In sum, a choice rule is a normative prescription of action choices for a rational agent in any
game situation. Choice rules may rest on implicit assumptions about what the DM believes (e.g.,
when interpreting Laplace rule as rational choice under flat beliefs). They may also make
implicit assumptions about what the DM prefers (e.g., when reconstructing regret minimization
in terms of maximin-choices on the regret transformation). This is rather messy. To disentangle
and avoid confusion, we turn to choice mechanisms.

\subsection{Choice mechanisms}
\label{sec:choice-mechanisms}

A \emph{choice mechanism} is a map from choice situations to action choices that, unlike choice
rules, contains an explicit level of subjective representation of a decision situation. For our
purposes, we should think of a choice mechanism is a pair of functions $\gamma$ and $\kappa$,
where $\gamma$ is a \emph{subjective representation scheme} and $\kappa$ is an \emph{action
  selection function} (see FigureXYZ).\todo{make figure when notation and terminology are
  fixed}\ The subjective representation scheme $\gamma$ maps a game (or more generally any
decision situation) onto a subjective representation of the game (or choice situation) and
other decision-relevant beliefs (e.g., about what the opponent is likely to play). The action
selection function $\kappa$ takes such a subjective representation that $\gamma$ returns, and
maps it onto a concrete action choice. The crucial bit for our purposes here is the
representation function $\gamma$, because we are interested in which ways of conceptualizing a
decision situation are evolutionarily successful.

More generally even, a choice mechanism could be a set of distinct heuristics (the DM's
\emph{adaptive toolbox}) that have little in common
\citep[e.g.][]{TverskyKahnemann1981:The-Framing-of-,GigerenzerGoldstein1996:Reasoning-the-F,ScheibehenneRieskamp2013:Testing-the-Ada}. But
to keep matters simple and implementable for a start, we will here only look at choice
mechanisms that differ only with respect to their subjective representation function $\gamma$,
which in turn is conceived of as a \emph{uniform} way of forming preference and belief
\emph{types}. \textcolor{red}{Formally, $\gamma$ is a function in
  $\mathcal{G} \rightarrow \Theta \times E$, where \dots. Then, given a set of subjective
  preferences $ \Theta $ [@Paolo: minimally define subjective preferences somewhere above] and
  a set of epistemic states (or belief types) $E$, a choice mechanism is a function
  $\kappa: \Theta \times E \rightarrow A_i $.}

\textcolor{red}{More concretely, we focus on two \emph{preference types} and two \emph{belief
    types}. We view a preference type as a function $\tau: \mathcal{G} \rightarrow \Theta$, and
  a belief type as a general disposition to form beliefs about the co-player’s behavior. The
  two possible dispositions that we take account of are probabilistic (precise) belief and
  non-probabilistic (imprecise) belief, as we will explain in the next section.}

\textcolor{red}{We are most interested here in studying whether evolutionary pressure could
  support non-veridical subjective preferences under certain conditions. We therefore consider
  two \emph{preference types}: veridical type $\tau^\pi$, who uses the actual payoff function
  $\pi$ of the actual game $G$, and regret type $\tau^{reg}$, who looks at the regret
  transformation of the actual game $G$.}

\textcolor{red}{The choice mechanism that we use in this paper is \textit{maxmin expected
    utility} [GS89], for reasons that will be clear and justified in the following two
  sections.  }

% It turns out that whether $\tau^{reg}$ can have a fitness advantage over $\tau^\pi$ depends on
% the general \emph{kind} of (probabilistic) beliefs that DM's form about the opponent's
% behavior. In other words, we even abstract away from the content of the DM's beliefs about his
% opponents' behavior that he could have acquired by learning or observation, but first turn on
% the more general question of how uncertainty should be represented in order to interact with
% subjective preferences in such a way that the resulting behavior will be fitness enhancing for
% the DM in the long run. We consider therefore two \emph{belief types} (to be defined below):
% imprecise belief types $\Delta(A_{-i})$, who have a set of probabilistic beliefs about the
% opponents' behavior and precise belief types $\overline{\mu}$ who have a single probabilistic
% belief.

% The following subsection describes what these belief types are and how a single action
% selection function (maximin expected utility) gives rise to an intersting parallel with
% classical choice rules.

\subsection{Imprecise probabilistic beliefs}
\label{sec:impr-prob-beli}

Consider the following problem. I have, on the desk in front of me, a bag containing either red or black marbles. There is no further information about the distribution of the two colors. What is then the probability that I will draw a red marble? As in Walley91-96, we are not talking about physical probability, but rather about epistemic probability, i.e., the subjective probability that an agent attributes to an event.
In the literature there are two traditional ways of answering this kind of questions. According to standard Bayesianism, one should always be able to determine a precise betting rate (that immediately translates into a precise probability distribution), which is fair, in the sense that the DM is indifferent between betting on or against red at the fair rate, and would bet on red at any rate that is more favourable than the fair rate and against red at any rate that is less favourable. In the case of the bag on my desk, a strict Bayesian, in the absence of any further information, would probably rely on the \textit{principle of insufficient reason} and answer that the DM should have a uniform distribution that considers all the outcomes equipossible. \\
The second answer involves imprecise probabilities and is related to the literature about unmeasurable uncertainty. Many authors argued in favor of this kind of approach [e.g., Walley91-96, Levi74, GardenforsANDSahlin82]. Since it is consistent with the information available that all the marbles are red, or that no marble is red, the resulting model should take it into account and specify a lower probability of red $\underline{P}(R)$ and an upper probability of red $\overline{P}(R)$ that define an interval of possible probabilistic beliefs [$\underline{P}(R), \overline{P}(R)$]. Given the information available in this case we would have $\underline{P}(R)=0$ and $\overline{P}(R)=1$. This approach appears particularly relevant in a game theoretical context. Indeed, in a recent paper [Battigalliet al.15] Battigalli et al. write

\begin{quote}
Such uncertainty is inherent in situations of strategic interaction. This is quite obvious when such situations have been faced only a few times.
\end{quote}

In evolutionary game theory, for instance, players in a population obviously face uncertainty about the composition of the population that they are part of, and consequently about the co-player that they are randomly paired with at each round and about the co-player's action. We represent this uncertainty in two different ways that correspond to the two approaches discussed above: a Bayesian uniform distribution and an imprecise probability set. This amounts to having two different belief types, $ \overline{\mu}$ and $ \Delta(A_{-i}) \rbrace $, where $\overline{\mu}$ stands for the uniform distribution and $\Delta(A_{-i})$ is the probability set, defined as follows:

\begin{itemize}

\item $ \Delta(A_{-i}):=\lbrace \mu \in \mathbb{R}^{A_{-i}}: \sum_{a_{-i} \in A_{-i}} \mu(a_{-i}) = 1 \rbrace$;

\item $\overline{\mu} \in \Delta(A_{-i})$ is the probability measure over the co-player's actions such that: $\forall a_{-i} \in A_{-i}$, $\overline{\mu}(a_{-i})= \frac{1}{|A_{-i}|}$.

\end{itemize} 

Hence, we consider two possible epistemic states in the face of uncertainty. The belief type $\Delta(A_{-i})$ represents a situation of non-Bayesian radical uncertainty, where the players are not able to narrow down the set of probability distributions to a single one and they consider all of them as possible. The other belief type $\overline{\mu}$ conceptualizes the uncertainty about the population by using the principle of insufficient reason and ascribes equal probability to all the possible alternatives. As we have seen, both options are reasonable and justifiable ways to deal with situations of uncertainty [REFERENCES]. \\

In decision theory, the imprecise probability model is in line with most of the representation results in decision making under uncertainty [GilboaSchmeidler89,KMM06], and seems justified by empirical observations too.

A prominent example is Ellsberg's paradox. A bag contains 90 marbles, 30 are red and the remaining 60 are either black or yellow. A marble is to be drawn and DM can win 100€ if she guesses the color of the drawn marble. The DM is offered two different bets. In the first one she can choose to bet either on red (R) or on yellow (Y), while in the second she can bet either on red or black (RB) or on yellow or black (YB), as shown in Table \ref{Ellsberg}.

\definecolor{yellow}{RGB}{255,188,1}


\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\cmidrule(l){2-4}
\multicolumn{1}{c}{} & {\color{red}R}   & {\color{yellow}Y}   & B   \\ \cmidrule(l){2-4} 
$f_{{\color{red}R}}$              & 100 & 0   & 0   \\
$f_{{\color{yellow}Y}}$              & 0   & 100 & 0   \\
$f_{{\color{red}R}B}$            & 100 & 0   & 100 \\
$f_{{\color{yellow}Y}B}$             & 0   & 100 & 100 \\ \bottomrule
\end{tabular}
\caption{Ellsberg's paradox}
\label{Ellsberg}
\end{table}

The pattern consistently observed in experimets is: $f_{{\color{red}R}} \succ f_{{\color{yellow}Y}}$ and $f_{{\color{red}R}B} \prec f_{{\color{yellow}Y}B}$. This is clearly incompatible with any precise probabilistic belief about the proportion of black and yellow marbles. This kind of behavior has been
famously axiomatized by Gilboa and Schmeidler in a paper from 1989. The choice pattern is normally explained in terms of aversion to uncertainty, and the standard representation is given by means of probability sets together with maxmin rule. We can think of the DM in Ellsberg's example as maximinimizing expected utility over the probability set $ [\underline{P}(R)=\frac{1}{3},\underline{P}(Y)=0,\underline{P}(B)=0,\overline{P}(R)=\frac{1}{3},\overline{P}(Y)=\frac{2}{3},\overline{P}(B)=\frac{2}{3}]  $. This amounts to calculating for any probability distribution contained in the probability set the expected utility of any available action, and then to choosing the action that guarantees the highest minimal expected utility. The following observation is then trivial.

\begin{fact} \label{fact:singleton probability set}
Whenever the probability set is a singleton, maximinization of
expected utility coincides with maximization of expected utility.
\end{fact}

Evidence from experimental literature suggests that agents are mainly uncertainty
averse [REF]. In line with empirical data,
we assume that players in the population are uncertainty averse and conform to maxmin expected
utility. The resulting behavior is then produced by maximinimizing the subjective utility
given by the player (preference, UNIFORM TERMINOLOGY CHECK) type over the set of probabilities given by the belief type of the agent.



\subsection{Choice mechanisms \& classical choice rules}

Fixing the choice mechanism (action selection component) to maximin expected utility, and allowing two belief and
two preference types, we regain all the choice rules considered in sectionNN from a more general and unified standpoint, as it is summed up in
Table~\ref{tab:CMs}. \\
We are now in the position of formally deriving behavior from a pair $(\theta, e) \in \Theta \times E$.

\begin{definition} \label{def:utility-belief}

Given a game $G$, an action $a_i \in A_i$, and a belief $\mu \in \lbrace \mu' \in \mathbb{R}^{A_{-i}}: \sum_{a_{-i} \in A_{-i}} \mu'(a_{-i}) = 1 \rbrace$, we define $\tau(G)(a_i, \mu):= \sum_{a_{-i} \in A_{-i}} \tau(G)(a_i, a_{-i}) \cdot \mu(a_{-i})$. 

\end{definition}

\begin{definition} \label{def:maxminSEU}

Given a game $G$ and a player $(\tau, e)$, we define $\text{maxmin}(\tau(G), e):= \lbrace a'_i \in A_i : a_i \in \text{max}_{a_i \in A_i} \text{ min}_{\mu \in e} \text{ } \tau(G)(a_i, \mu) \rbrace$.

\end{definition}

[[\noindent Definition \ref{def:expected fitness} gives the expected evolutionary fitness of player $(\tau, e)$ against player $(\tau', e')$ in game $G$.\footnote{As already specified in the previous example, we assume that, whenever the set $\text{maxmin}(\tau(G), e)$ is not a singleton, the player $(\tau, e)$ will mix with equal probability among the actions in $\text{maxmin}(\tau(G), e)$.}
[OF COURSE we will postpone the definition of evolutionary fitness, it is only momentarily here!]
\begin{definition} \label{def:expected fitness}

Given a game $G \in \mathcal{G}$, a player $(\tau, e)$, and a player $(\tau', e')$, we define $\pi^G((\tau, e),(\tau', e')):= \pi^G(\text{maxmin}(\tau(G), e),\text{maxmin}(\tau'(G), e'))$. 

\end{definition}
[OF COURSE we will postpone the definition of evolutionary fitness, it is only momentarily here!]]]

It is then worth stressing here that $\kappa(\tau^{\pi}, \Delta(A_{-i}))$ turns out to be the objective
maximinimizer, $\kappa(\tau^{reg}, \Delta(A_{-i}))$ corresponds to the regret minimizer,
$\kappa(\tau^{\pi}, \overline{\mu})$ is the objective expected utility maximizer (with flat beliefs), and
$\kappa(\tau^{reg}, \overline{\mu})$ is the expected regret minimizer (with flat beliefs).
Moreover, different subjective preferences and belief types can give rise to identical action predictions,
either in a restricted class of games, or in general. This is also the case here.

\begin{table}[t]
  \centering
  \begin{tabular}{ccc}
    preference type & belief type & corresponding choice rule \\ \midrule
    $\tau^\pi$ & $\overline{\mu}$ & Laplace \\
    $\tau^\pi$ & $\Delta(A_{-i})$ & maximin \\ 
    $\tau^{reg}$ & $\overline{\mu}$ & Laplace \\
    $\tau^{reg}$ & $\Delta(A_{-i})$ & regret minimization \\ 
  \end{tabular}
  \caption{Overview relevant choice mechanisms and the corresponding (behaviorally equivalent) classical choice rule}
  \label{tab:CMs}
\end{table}


\begin{fact} \label{fact:maxEU-minReg} For any strategic game $G$ and any arbitrary
  probabilistic belief $\mu \in \Delta(A_{-i})$ about the opponents' behavior, the acts
  selected by maximization of expected utility for $G$ under $\mu$ are exactly the acts
  selected by the maximization of expected utility for the negative regret transform of $G$
  under $\mu$ \citep[e.g.][]{HalpernPass2012:Iterated-Regret}.
\end{fact}

\noindent In other words, given the same probabilistic belief, maximization of expected
(objective) payoff and minimization of expected (subjective) regret are behaviorally
equivalent. This holds for arbitrary games, as well as for solitary decision situations. On the
other hand, the following does not hold in general:

\begin{fact} \label{fact:equivalence2x2}
In the class of $2 \times 2$ symmetric games, players $(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{\pi}, \overline{\mu})$, and $(\tau^{reg},\overline{\mu})$ are behaviorally equivalent.
\end{fact} 




\begin{example}
  Consider the game in Figure \ref{coordgame1} again. When a player characterized
  by\footnote{To ease notation in the following we will refer to a player directly as a pair
    $(player \text{ } type \text{ } \tau,\text{ } epistemic\text{ } type \text{ } e)$.} player
  type $ \tau^{reg} $ and epistemic type $ \Delta(A_{-i}) $ faces the game in Figure
  \ref{coordgame1}, her player type gives rise to the representation $\tau^{reg}(G^C)$ in
  Figure \ref{coordgame1reg}. Then, maximinimizing expected utility over the probability
  simplex $ \Delta(A_{-i}) $ given the game in Figure \ref{coordgame1reg} corresponds to
  playing the standard maxmin strategy [REF] of that game.\footnote{Notice that this is the
    case for any game: given a game $G$, maximinimizing expected utility over the probability
    simplex $ \Delta(A_{-i}) $ amounts to playing the standard maxmin strategy of $G$.} This,
  in turn, corresponds to the regret minimizing action of the original game $G^C$ of Figure
  \ref{coordgame1}, namely action $II$, as we have seen in Section \ref{sec:regreTheory}. A
  player $(\tau^{reg},\overline{\mu})$ instead has subjective representation $\tau^{reg}(G^C)$
  of Figure \ref{coordgame1reg} and a flat probability measure
  $\overline{\mu}=(\frac{1}{2}I,\frac{1}{2}II)$ over the co-player's actions. Consequently,
  such a player would play action $II$, i.e., the action that maximinimizes expected utility
  given the flat probability measure $\overline{\mu}$.\footnote{As we already noticed,
    maximinimizing expected utility given a probability measure $\mu$ is equivalent to
    maximizing expected utility given $\mu$.}  Similarly, a player
  $(\tau^{\pi},\Delta(A_{-i}))$ will have the subjective representation coinciding with the
  objective fitness game, i.e., $ \tau^{\pi}(G^C)= G^C $. Then, both actions $I$ and $II$
  maximinimize expected utility over the probability simplex $ \Delta(A_{-i}) $. Hence, a
  player $(\tau^{\pi},\Delta(A_{-i}))$ will be indifferent between $I$ and $II$, and will
  randomize with equal probability $(\frac{1}{2}I,\frac{1}{2}II)$. Finally, a player
  $(\tau^{\pi},\overline{\mu})$ maximinimizes the objective fitness game $G^C$ of Figure
  \ref{coordgame1} with flat probability $\overline{\mu}$ over the co-player's actions, i.e.,
  maximizes objective utility given $\overline{\mu}$. In game $G^C$ this yields action $II$.

\end{example}


%\todo[inline]{should we explain here, or very soon, what the resulting types really are: regret minimizers, maximin players \dots}

Many other alternatives different from maxmin could produce behavior: choice rules like minmax, maxmax, and minmin would similarly generate behavior in case of a lack of precise probabilistic belief. Here we postulate that players use maxmin for two main reasons that we will expand in the next section. First, among other possible choice rules maxmin is not only intuitively appealing but also corroborated from an empirical point of view. Secondly, as it will be clear in the following, our main interest is the evolution of different patterns of subjective conceptualizations of games (player types), and we want to show that the veridical representation (objective type) is not necessarily the fittest and may not be evolutionarily stable. Assuming that all players use a fixed rule (maxmin) independent of their player type allows us to factor the choice rule out of the selection process and to focus on player types.



\bigskip
\bigskip

\todo[inline]{PAOLO rewrite up to here!!! BUT the last 2-3 paragraphs must be rearranged a bit more!}

\bigskip
\bigskip




In sum, we have so far considered four different alternatives, each differing along a
preference and a belief dimension. As for preference representations, agents could either
target objective payoffs, or consider non-veridical subjective regret transformations. Given a game $G$, we will denote by $ \tau^{reg}(G)$ the regret representation (corresponding to the negative regret transformation) of the game, and $\tau^{\pi}(G)$ the objective representation (corresponding to the identity transformation) of the game. \\
As for
beliefs, agents could either hold probabilistic belief or not. In the former case, we would
assume that they maximize expectations under whatever preference representation they hold; in
the latter, we would assume that they play maximin based on their preference
representations. There are doubtlessly many more possible preference and belief types, but
these are the main characters in the plot to follow. 

Before we turn to evolutionary competition between choice principles, we should justify our
focus on the two choice rules. In the following we argue that both the minimax rule, as well as the maximization of expected utility 
(and the Laplace rule as a special case) can be viewed as the same choice rule under different representations of
uncertainty. 


To sum up, here we consider the choice of an action as depending on two features: the subjective preference and the belief state. Definition \ref{def:choice rule} makes this point clear.

\begin{definition} \label{def:choice rule}

Given a set of subjective preferences $\Theta$ and a set of epistemic types $E$, we define \textit{choice rule} a function $\kappa: \Theta \times E  \rightarrow A_i$ that associates an action choice to a pair $(\theta, e) \in \Theta \times E$.

\end{definition}

According to what we argued in this section, we will assume that the choice rule that turns subjective preferences and beliefs into actions is maxmin expected utility.





\iffalse

\begin{fact} \label{fact:maxEU-minReg}

For any probability measure $\mu$, maximization of expected utility and minimization of expected regret are behaviorally equivalent.

\end{fact}

\noindent As we have seen in the previous example, this is not the case in the presence of (radical) uncertainty, in the sense of representation of belief state in terms of a set of probability, as in $ \Delta(A_{-i}) $.
\fi








\newpage

\section{Meta-games and the evolution of choice mechanisms}
\label{sec:model}

Evolutionary game theory has established itself as a useful tool for testing consequences of
sets of assumptions about complex evolutionary processes. However, the standard approach in
evolutionary game theory is not enough for our purposes, because it (i) fixes a single
\emph{stage game} (e.g., the prisoner's dilemma) and (ii) assumes that the object of
evolutionary selection is behavior for that very game. Following a similar line of argument
\citep[e.g.][]{FawcettHamblin2013:Exposing-the-be}, we therefore turn towards the evolutionary
competition of \emph{choice mechanisms}, i.e., general ways of selecting concrete behavior
across a range of different situations. As a consequence, the expected fitness of a choice
mechanism will not be determined by only a single (elementary) stage game, but it will be a
complex function of the statistical properties of the environment: we have to take into account
how frequently particular concrete situations of choice making are.

For concreteness, let us fix a class of strategic games $\mathcal{G}$ and a probability measure
$P \in \Delta(\mathcal{G})$ that gives the occurrence probability $P(G)$ of each game
$G \in \mathcal{G}$. Together, a class of games and a probability measure over it can be
thought of as a model of the environment in which different choice mechanisms compete. Let $C$
the a set of choice mechanisms that we care about. Then each choice mechanism $c \in C$ is a
associated with an \emph{action selection function} $S_c$ that maps any $G \in \mathcal{G}$
onto some action $S_c(G)$ of game $G$.\footnote{In all subsequent formal and simulation results
  we assume that, whenever a choice mechanisms is selects indifferently a set of actions, the
  agent will realize each with the same (flat) probability. For ease of parlor, we often do as
  if there was a tie-break rule.} Different choice mechanisms can select the same act in some
games. More importantly even, different choice mechanisms can have exactly the same action
selection functions on a particular class of games $\mathcal{G}$. This is because we think of
the object of evolutionary selection, not as the realized behavior but as the way behavior is
specified. (We will see below that this can make an important difference, e.g., in the light of
mutation.)

Imagine a huge, possibly infinite society of agents, each of which carries a choice mechanism
from $C$. For simplicity, let's assume that $C$ is finite and that $\mathcal{G}$ is
a finite set of two-player strategic games. The expected fitness $EF(c_1)$ of an agent with
choice principle $c_1$ is determined by the probability $P(c_2)$ of meeting an agent with
choice principle $c_2$, which we will assume is the frequency of $c_2$ in the population, the
probability $P(G)$ of playing game $G \in \mathcal{G}$ and the concrete payoff
$\pi^G(c_1 \mid c_2)$ that the player with $c_1$ obtains when playing against a player with
$c_2$ in $G$, which is uniquely defined by the associated action selection rules:
\begin{align*}
  EF(c_1) = \sum_{c_2 \in C} P(c_2) \cdot \sum_{G \in \mathcal{G}}  P(G) \cdot \pi^G(c_1 \mid c_2)\,.
\end{align*}
Similarly, the expected fitness of a $c_1$-agent, when playing against a $c_2$ agent is:
\begin{align}
  \label{eq:ExpectedPayoffs}
  EF(c_1 \mid c_2) = \sum_{G \in \mathcal{G}}  P(G) \cdot \pi^G(c_1 \mid c_2)\,.
\end{align}

Notice that expected fitness of a choice mechanism is defined entirely in parallel to the
expected fitness of an act in a fixed stage game, except that where the latter has a simple
payoff function, the former has an expectation over payoffs in a statistically rich
environment. That means that we can think of evolutionary competition between choice mechanisms
in terms of payoffs defined by a \emph{meta-game} over $\mathcal{G}$ whose ``acts'' are choice
mechanisms and whose ``payoffs'' are the expected payoffs in
Equation~(\ref{eq:ExpectedPayoffs}).\footnote{Allowing games in $\mathcal{G}$ that have
  different numbers of players makes matters more complicated, but we neglect these
  complications here, because we focus entirely on 2-player games.}


\section{The basic model}
\label{sec:basic-model-1}

To demonstrate the usefulness of a meta-game approach, let us concentrate first on a basic
model that contains the four choice mechanisms obtained from crossing the preference and belief
types introduced in Section~\ref{sec:basic-notions}.  For particular empirical purposes, one
could consult a more specific class $\mathcal{G}$ with appropriate, possibly empirically
determined probabilities $P(G)$ in order to match the natural environment of a given
population. But for our theoretical purposes we adopt another approach. For a first basic
model, let $\mathcal{G}$ be a set of symmetric 2-player games with $2$ acts. (We will look at
extensions and generalizations in Section~\ref{sec:extensions}). More precisely, we imagine the
class $\mathcal{G}$ to be generated from the generic form table of Figure \ref{generic form
  table}, where $a,b,c,d$ are i.i.d.~random variables sampled from a finite set of natural
numbers $ \lbrace 0, \dots, 10 \rbrace$.


\begin{figure}
\begin{center}%
\begin{tabular}{|c|c|c|}
\hline 
 & I & II\tabularnewline
\hline 
\hline 
I & a;a & b;c\tabularnewline
\hline 
II & c;b & d;d\tabularnewline
\hline 
\end{tabular}\end{center}

\protect\caption{generic form table for $2 \times 2$ symmetric games}
\label{generic form table}
\end{figure}



\subsection{Player types}

The set of all possible player types amouts to the set of all possible functions $ \tau: \mathcal{G} \rightarrow  \Theta $, i.e., $T=\mathbb{R}^{4^{\mathbb{R}^{4}}} $ , but for the sake of simplicity we will focus on types that are relevant and meaningful from a decision-theoretic point of view. We are mainly interested in two types: the objective type and the regret type.\\
Given a game $G$ and Definition \ref{defn:parwreg}, it is straightforward to define the
\textit{regret preference} $\theta^{reg}$ and the \textit{regret type} $\tau^{reg}$ as
follows. 

\todo[inline]{can some of these definitions not be written as one? e.g., regret preference and
  regret type as one definition? seems more elegant to me, because it's less repetitive (``for
  a game ...'')}

\begin{definition}[Regret preference] \label{defn:regpref}

For a game $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, we define the regret preference $ \theta^{reg}: A \rightarrow \mathbb{R} $ such that $ \theta^{reg}(a_i,a_{-i})=-\text{reg}(a_i,a_{-i}) $.\footnote{More precisely, this is the negative regret, whereas the (positive) regret, as defined in [halpass], would not have the minus sign at the beginning of the formula. Here it will be easier to define the regret in terms of negative regret.}

\end{definition}



\begin{definition}[Regret type] \label{defn:regtype}

Given $ \mathcal{G} $ the class of symmetric $ 2 \times 2 $ games, we call regret type the function $\tau^{reg}: \mathcal{G} \rightarrow  \Theta$ such that, for all $G \in \mathcal{G}$, $ \tau^{reg}(G)= \theta^{reg}$.

\end{definition}

\noindent Similarly, let us define the \textit{objective preference} $\theta^{\pi}$ and the \textit{objective type} $ \tau^{\pi} $, that we also call $\pi$\textit{-type}, as follows. 

\begin{definition}[Objective preference] \label{defn:objpref}

For a game $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, we define the objective preference $ \theta^{\pi}: A \rightarrow \mathbb{R} $ such that $ \theta^{\pi}(a_i,a_{-i})=\pi(a_i,a_{-i}) $.

\end{definition}


\begin{definition}[Objective type] \label{defn:objtype}

Given $ \mathcal{G} $ the class of symmetric $ 2 \times 2 $ games, we call objective type, or $\pi$-type, the function $\tau^{\pi}: \mathcal{G} \rightarrow  \Theta$ such that, for all $G \in \mathcal{G}$, $ \tau^{\pi}(G)= \theta^{\pi}$.

\end{definition}

\noindent In other words, we consider two prominent player types: a type who has a veridical subjective utility that perfectly matches the objective utility (i.e., the evolutionary fitness); and a type who has a non-veridical regret-based subjective utility. 


\section{Results for the basic model}
\label{sec:results:-basic-model}


In this section we present results of computer simulations and analytical proofs for our basic
model with the four player types introduced in the previous section. 

\subsection{Numerical simulations}
\label{sec:numer-simul}

Table~\ref{tab:ExpectedFitness_4Types} gives an approximation of expected fitness, obtained by
numerical simulation from averaging over $100,000$ randomly sampled games in $\mathcal{G}$ with
$a,b,c,d \in \lbrace 0, \dots, 10 \rbrace$. Concretely, games were sampled repeatedly by
choosing independently four integers $a,b,c,d \in \lbrace 0, \dots, 10 \rbrace$ uniformly at
random. For each game, a player type's action choices were determined and payoffs from all
pairwise encounters recorded. (Whenever a player type would not select a unique action in a
given game, we recorded unbiased averages over playing either action.) The number in each cell
of Table~\ref{tab:ExpectedFitness_4Types} is the average payoff for the row player type over
all 100,000 numbers for each pairing of player types obtained in this way. These averages
approximate the corresponding average evolutionary fitness of the row player when matched with
the column player.

\begin{table}[t]
\centering
\begin{tabular}{ccccc}
  \toprule
 & $\tau^{reg}, \Delta(A_{-i})$ 
 & $\tau^{\pi}, \Delta(A_{-i})$ 
 & $\tau^{reg}, \overline{\mu}$ 
 & $\tau^{\pi}, \overline{\mu}$ \\ 
  \midrule
  $\tau^{reg}, \Delta(A_{-i})$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
  $\tau^{\pi}, \Delta(A_{-i})$ & 6.486 & 6.484 & 6.486 & 6.486 \\ 
  $\tau^{reg}, \overline{\mu}$ & 6.663 & 6.662 & 6.663 & 6.663 \\  
  $\tau^{\pi}, \overline{\mu}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
   \bottomrule
\end{tabular}                    
\caption{Average evolutionary fitness in simulations of 100,000 symmetric $2 \times 2$ games}
\label{tab:ExpectedFitness_4Types}
\end{table}

Simulation results obviously reflect Fact~\ref{fact:equivalence2x2} in that all encounters in
which types $(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{reg}, \overline{\mu})$ or
$(\tau^{\pi}, \overline{\mu})$ are substituted for one another yield identical results. More
interestingly, Table~\ref{tab:ExpectedFitness_4Types} shows that the maximin strategy
$(\tau^{\pi}, \Delta(A_{-i}))$ is strictly dominated by either of the three other strategies:
in each column (i.e., for each kind of opponent), the maximin strategy is strictly worse
than any of the three behaviorally equivalent competitors. This has a number of interesting
consequences.

Let's begin with a restricted scenario and look at more and more complex cases afterwards. If
we restrict attention to a population of radically uncertain players, i.e., where only
epistemic type $\Delta(A_{-i})$ exists, the regret type $ \tau^{reg} $ is the only
\emph{evolutionarily stable state}. An evolutionarily stable state is a state which cannot be
invaded by a small number of mutants. More strongly, since maximin play is strictly dominated,
we expect selection that is driven by expected fitness to invariably weed out maximin behavior
in favor of regret minimization. In other words, when taking average payoffs over the class of
random games looked at here, regret minimization is a better choice rule than standard maxmin
from an evolutionary point of view, despite the fact that the former, but not the letter
entertains non-veridical subjective preferences.

Next, if we look at the competition between all four types represented in
Table~\ref{tab:ExpectedFitness_4Types}, $(\tau^{reg}, \Delta(A_{-i}))$ is no longer
evolutionarily stable. Rather, given behavioral equivalence (Fact~\ref{fact:equivalence2x2}),
types $(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{reg}, \overline{\mu})$, and
$(\tau^{\pi}, \overline{\mu})$ are all \emph{neutrally stable}
\citep{Maynard-Smith1982:Evolution-and-t}. But since $(\tau^{\pi}, \Delta(A_{-i}))$ is strictly
dominated and so disfavored by fitness-based selection, we are still drawn to conclude that
maximin behavior is weeded out in favor of a population with a random distribution of the
remaining three types.

Simulation results of the (discrete time) \emph{replicator dynamics}
\citep{TaylorJonker1978:Evolutionary-St} indeed show that random initial population
configurations are attracted to states with only three player types:
$\tuple{\tau^{reg}, \Delta(A)}$, $\tuple{\tau^{reg}, \overline{\mu}}$ and
$\tuple{\tau^{\pi}, \overline{\mu}}$. The relative proportions of these depend on the initial
population.\todo{check uniform notation for player types: round vs.~square brackets} This
variability disappears if we add a small mutation rate to the dynamics. We assume a fixed,
small mutation rate $\epsilon$ for the probability that a player's preference type \emph{or}
her epistemic type changes to another random preference type or epistemic type. The probability
that a player type randomly mutates into a completely different player type with altogether
different preference type and epistemic type would then be $\epsilon^2$. With these assumptions
about ``component-wise mutations'', numerical simulations of the (discrete time)
\emph{replicator mutator dynamics} \citep{Nowak2006:Evolutionary-Dy} show that already for very
small mutation rates almost all initial populations converge to a single fixed point in which
the majority of players are regret types. For instance, with $\epsilon = 0.001$, almost all
initial populations are attracted to a final distribution with proportions:\todo{recalculate
  these numbers for the actual 4x4 game!}

\begin{center}
  \begin{tabular}{ccc}
    $\tuple{\tau^{reg}, \Delta(A)}$ & $\tuple{\tau^{reg},
      \overline{\mu}}$ & $\tuple{\tau^{\pi}, \overline{\mu}}$ \\ \hline
    0.279  &   0.383 &    0.281 
  \end{tabular}
\end{center}

What this suggests is that, since biological evolution selects behavior-generating mechanisms,
not behavior as such, it need not be the case that behaviorally equivalent mechanisms are
treated equally all the while. If mutation probabilities are a function of individual
components of such behavior-generating mechanisms, it can be the case that certain components
are more strongly favored by a process of random mutation and selection. This is exactly the
case with regret-based subjective preferences in the present example. Since regret-based
subjective preferences are much better in connection with radical uncertainty than veridical
preferences are, the proportion of expected regret minimizers in the attracting state is
substantially higher than that of expected utility maximizers, even though these types are
behaviorally equivalent. In sum, component-wise mutation could have lead to a majority of
regret-based players even if they are behaviorally indiscernible from decision makers with
veridical preferences, because regret types are better off under radical uncertainty.

\subsection{Analytical results}
\label{sec:analytical-results}

Results based on the single meta-game in Table~\ref{tab:ExpectedFitness_4Types}, which was
obtained by averaging over numerical simulations, are not fully general and possibly spoiled by
random fluctuations in the sampling procedure. Fortunately, for the case of symmetric
$2 \times 2$ games, the main result that maximin types $(\tau^{\pi}, \Delta(A_{-i}))$ are
strictly dominated can also be shown analytically for generous general conditions on how likely
randomly sampled games are.

\begin{proposition} \label{proposition1}

  Let $\Lambda = \lbrace (\tau^{\pi}, \Delta(A_{-i})), (\tau^{reg}, \Delta(A_{-i})) \rbrace$ be
  the set of possible meta-types $(\tau, e)$ in the population, and let $\mathcal{G}$ be the class
  of symmetric $2 \times 2$ games with payoffs sampled from a finite, or compact and convex,
  set of i.i.d.~values. Then, $(\tau^{reg}, \Delta(A_{-i}))$ is the only strict Nash
  equilibrium in the resulting meta-game.

\end{proposition}

\begin{proof}
See Appendix.
\end{proof}

From Fact \ref{fact:equivalence2x2} and Proposition \ref{proposition1} the following corollary follows.

\begin{corollary} \label{corollary1}

  Fix $T = \lbrace \tau^{\pi}, \tau^{reg} \rbrace$,
  $E = \lbrace \Delta(A_{-i}), \overline{\mu} \rbrace$, $\Lambda= T \times E$, and
  $\mathcal{G}$ the class of symmetric $2 \times 2$ games with payoffs sampled from a finite,
  or compact and convex, set of i.i.d. values. Then, player type $(\tau^{\pi}, \Delta(A_{-i}))$
  is strictly dominated by $(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{\pi}, \overline{\mu})$, and
  $(\tau^{reg}, \overline{\mu})$.

\end{corollary}

\begin{proof}
See Appendix.
\end{proof}

Corollary~\ref{corollary1} tells us that the conclusions drawn in the previous section based on
the meta-game in Table~\ref{tab:ExpectedFitness_4Types} hold more generally for any class of
games in which the probability of encountering a game is the product of independently sampling
four identically distributed payoff values. As before, player types
$(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{\pi}, \overline{\mu})$, and
$(\tau^{reg}, \overline{\mu})$ will be neutrally stable, and fitness-based selection will tend
to weed out maximin play and leave us with a population of arbitrary frequency of
$(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{\pi}, \overline{\mu})$, and
$(\tau^{reg}, \overline{\mu})$ meta-types.

This shows that there is support for the main conceptual point that we wanted to make:
non-veridical subjective preference relations \emph{can}, under specific circumstances, persist
under evolutionary selection based on objective fitness. Moreover, regret-based preferences can
not only persist, but even be favored by natural selection if agents are radically
uncertain. This holds generally for playing arbitrary $2 \times 2$ games whose payoffs are
sampled identically and independently at random.

\section{Extensions}
\label{sec:extensions}

How do the basic results from the previous section carry over to more encompassing, richer
models? Section~\ref{sec:more-types} first considers further conceptually interesting
preference types that have been suggested in the recent literature. Section~\ref{sec:n-times-n}
then considers the case of $n \times n$ games for $n \ge 2$. Finally, to put our results into
the proper perspective, Section~\ref{sec:solitary-decisions} ends with a brief comparison of
game situations with cases of solitary decision making.

\subsection{More types}
\label{sec:more-types}


In evolutionary game theory other types of player have been investigated. A famous example is the \textit{altruistic type} [REF], introduced in the literature in order to explain the possible survival of altruistic behavior. In our framework, we can represent the altruistic type as follows. 

\begin{definition}[Altruistic preference] \label{defn:altpref}

For a game $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, we define the altruistic preference $ \theta^{alt}: A \rightarrow \mathbb{R} $ such that $ \theta^{alt}(a_i,a_{-i})=\pi(a_i,a_{-i}) + \sum_{j \neq i} \pi(a_{j},a_{-j})$.\footnote{A more general formulation [see: REFERENCES] would be to define an $ \alpha$-altruistic type, for $\alpha \in [0,1]$, with subjective utility $ \theta^{\alpha lt}(a_i,a_{-i})=\pi(a_i,a_{-i}) + \alpha \sum_{j \neq i} \pi(a_{j},a_{-j})$. Since we are not interested in the evolution of degrees of altruism here, we simply fix $ \alpha = 1 $.}

\end{definition}

\begin{definition}[Altruistic type] \label{defn:alttype}

Given $ \mathcal{G} $ the class of symmetric $ 2 \times 2 $ games, we call altruistic type the function $\tau^{alt}: \mathcal{G} \rightarrow  \Theta$ such that, for all $G \in \mathcal{G}$, $ \tau^{alt}(G)= \theta^{alt}$.

\end{definition}

\noindent As opposite to the altruistic type, it is possible to define a \textit{competitive type}. 

\begin{definition}[Competitive preference] \label{defn:compref}

For a game $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, we define the altruistic preference $ \theta^{com}: A \rightarrow \mathbb{R} $ such that $ \theta^{com}(a_i,a_{-i})=\pi(a_i,a_{-i}) - \sum_{j \neq i} \pi(a_{j},a_{-j})$.

\end{definition}

\begin{definition}[Competitive type] \label{defn:comtype}

Given $ \mathcal{G} $ the class of symmetric $ 2 \times 2 $ games, we call altruistic type the function $\tau^{com}: \mathcal{G} \rightarrow  \Theta$ such that, for all $G \in \mathcal{G}$, $ \tau^{com}(G)= \theta^{com}$.

\end{definition}

Similarly to the evolutionary competition that we simulated before, we can study the
evolutionary fitness of this extended set of players when they are all represented in the
population. Table~\ref{tab:ExpectedFitness_2x2_Full} shows simulation results that approximate the
expected fitness in the relevant meta-game.

\begin{table}[]
\centering
\footnotesize
\begin{tabular}{ccccccccc}
  \hline
 & $\tuple{\tau^{reg}, \Delta(A)}$ 
 & $\tuple{\tau^{\pi}, \Delta(A)}$ 
 & $\tuple{\tau^{com}, \Delta(A)}$
 & $\tuple{\tau^{alt}, \Delta(A)}$
 & $\tuple{\tau^{reg}, \overline{\mu}}$ 
 & $\tuple{\tau^{\pi}, \overline{\mu}}$ 
 & $\tuple{\tau^{com}, \overline{\mu}}$
 & $\tuple{\tau^{alt}, \overline{\mu}}$ \\ 
  \hline
  $\tuple{\tau^{reg}, \Delta(A)}$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $\tuple{\tau^{\pi}, \Delta(A)}$ & 6.486 & 6.484 & 6.088 & 6.703 & 6.486 & 6.486 & 6.088 & 6.875 \\
  $\tuple{\tau^{com}, \Delta(A)}$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $\tuple{\tau^{alt}, \Delta(A)}$ & 5.949 & 5.722 & 5.326 & 6.396 & 5.949 & 5.949 & 5.326 & 6.568 \\
  $\tuple{\tau^{reg}, \overline{\mu}}$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $\tuple{\tau^{\pi}, \overline{\mu}}$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $\tuple{\tau^{com}, \overline{\mu}}$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $\tuple{\tau^{alt}, \overline{\mu}}$ & 6.331 & 5.893 & 5.497 & 6.566 & 6.331 & 6.331 & 5.497 & 7.152 \\
   \hline                          
\end{tabular}                      
\caption{Average payoff in simulations of 100,000
  randomly generated $2 \times 2$ symmetric games.}
\label{tab:ExpectedFitness_2x2_Full}        
\end{table}   
 
\todo[inline]{is it true that competitive preferences types are behaviorally equivalent under
  either epistemic type? can we show that?}
 
The approximations in Table~\ref{tab:ExpectedFitness_2x2_Full} confirm basic intuitions about
altruistic and competitive types: averaging over randomly sampled games, everybody would like
to have an altruistic opponent and nobody would like to play against a competitive
opponent. Perhaps more surprisingly, altruistic types come up strictly dominated by competitive
types, but competitive types themselves are worse off against all opponent types except against
maximin players $\tuple{\tau^{\pi}, \Delta(A)}$ than any of the behaviorally equivalent types
$(\tau^{reg}, \Delta(A_{-i}))$, $(\tau^{\pi}, \overline{\mu})$, and
$(\tau^{reg}, \overline{\mu})$.  This is a noteworthy results in the light of the fact that
evolving altruistic preferences have been shown to support cooperative behavior in a single
stage game \myalert{[CITE]}. \todo{@Paolo: check whether this is true, please!} In contrast,
averaging over payoffs in multiple stage games, like we do here, makes altruistic preferences
prime victims of evolutionary eradication.

It is easy to see then that the previous result still obtains for the larger meta-game in
Table~\ref{tab:ExpectedFitness_2x2_Full}: $(\tau^{reg}, \Delta(A_{-i}))$,
$(\tau^{\pi}, \overline{\mu})$, and $(\tau^{reg}, \overline{\mu})$ are still neutrally stable;
all simulations of the (discrete-time) replicator dynamic on the $8 \times 8$ meta-game from
Table~\ref{tab:ExpectedFitness_2x2_Full} end up in populations consisting of only these types
in arbitrary proportion.

The following proposition (partially) confirms this result.\todo{rephrase to acknowledge all
  epistemic types?}

\begin{proposition} \label{proposition2}

Fix $\Lambda = \lbrace (\tau^{\pi}, \Delta(A_{-i})), (\tau^{reg}, \Delta(A_{-i})), (\tau^{alt}, \Delta(A_{-i})), (\tau^{com}, \Delta(A_{-i})) \rbrace$ and $\mathcal{G}$ the class of symmetric $2 \times 2$ games with payoffs sampled from a finite, or compact and convex, set of i.i.d. values. Then, $(\tau^{reg}, \Delta(A_{-i}))$ is the only evolutionarily stable player in the population.

\end{proposition}

\begin{proof}
See Appendix.\todo{is this there already?}
\end{proof}

In sum, the presence of other plausible preference types, such as competitive and altruistic
types does not undermine the previous results about the evolutionary drive towards regret-based
subjective preferences.


                                   
\subsection{$n \times n$ symmetric games}
\label{sec:n-times-n}

Results from Section~\ref{sec:results:-basic-model} relied heavily on
Fact~\ref{fact:equivalence2x2} that, for the special case of $2 \times 2$ symmetric games,
regret minimization $\tuple{\tau^{reg}, \Delta(A)}$ is behaviorally equivalent with expected
regret minimization $\tuple{\tau^{reg}, \overline{\mu}}$ and expected utility maximization
$\tuple{\tau^{\pi}, \overline{\mu}}$. This is no longer true when we look at arbitrary $n \times n$
games with $n \ge 2$. We should therefore see what happens in a broader class of games.

Table~\ref{tab:ExpectedFitness_10x10} therefore gives approximations for expected fitness in a
meta-game over a class of $n \times n$ symmetric games where $n$ is randomly drawn from
$\set{2, \dots, 10}$. Concretely, the numbers in Table~\ref{tab:ExpectedFitness_10x10} are
averages of payoffs obtained in 100,000 randomly sampled games, where each game was sampled by
first picking a number of acts $n \in \set{2, \dots, 10}$ uniformly at random, and then filling
the resulting $n \times n$ payoff matrix of the stage game with i.i.d.~sampled payoffs as
before.


\begin{table}[]
\centering
\footnotesize
\begin{tabular}{ccccccccc}
  \toprule
 & $\tuple{\tau^{reg}, \Delta(A)}$ 
 & $\tuple{\tau^{\pi}, \Delta(A)}$ 
 & $\tuple{\tau^{com}, \Delta(A)}$
 & $\tuple{\tau^{alt}, \Delta(A)}$
 & $\tuple{\tau^{reg}, \overline{\mu}}$ 
 & $\tuple{\tau^{\pi}, \overline{\mu}}$ 
 & $\tuple{\tau^{com}, \overline{\mu}}$
 & $\tuple{\tau^{alt}, \overline{\mu}}$ \\ 
  \midrule
  $\tuple{\tau^{reg}, \Delta(A)}$ & 6.567 & 6.570 & 5.650 & 6.992 & 6.564 & 6.564 & 5.593 & 7.409 \\
  $\tuple{\tau^{\pi}, \Delta(A)}$ & 6.476 & 6.483 & 5.896 & 6.818 & 6.484 & 6.484 & 5.850 & 7.124 \\
  $\tuple{\tau^{com}, \Delta(A)}$ & 6.468 & 6.647 & 5.512 & 7.169 & 6.578 & 6.578 & 5.577 & 7.354 \\
  $\tuple{\tau^{alt}, \Delta(A)}$ & 5.968 & 5.923 & 5.363 & 6.685 & 5.975 & 5.975 & 5.086 & 6.973 \\
  $\tuple{\tau^{reg}, \overline{\mu}}$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $\tuple{\tau^{\pi}, \overline{\mu}}$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $\tuple{\tau^{com}, \overline{\mu}}$ & 6.529 & 6.680 & 5.445 & 7.276 & 6.542 & 6.542 & 5.521 & 7.440 \\
  $\tuple{\tau^{alt}, \overline{\mu}}$ & 6.450 & 6.337 & 5.772 & 6.978 & 6.457 & 6.457 & 5.479 & 7.500 \\
   \bottomrule                         
\end{tabular}                      
\caption{Average payoff in simulations of 100,000
  randomly generated $n \times n$ symmetric games with $n$ randomly drawn from $\set{2, \dots, 10}$.}
\label{tab:ExpectedFitness_10x10}        
\end{table}

As is to be expected, average payoffs obtained in games with more than 2 acts are higher, as
can be seen by comparing Table~\ref{tab:ExpectedFitness_10x10} with
Table~\ref{tab:ExpectedFitness_2x2_Full}. But increases in approximate expected fitness are not
uniform. Most importantly, the regret minimizing type $\tuple{\tau^{reg}, \Delta(A)}$ is
strictly dominated by $\tuple{\tau^{reg}, \overline{\mu}}$ and by $\tuple{\tau^{\pi}, \overline{\mu}}$ in
the meta-game from Table~\ref{tab:ExpectedFitness_10x10}. This means that while regret
minimization \emph{can} thrive in some evolutionary contexts, there are also contexts where it
is demonstrably worse off.

Still, although this may be bad news for regret minimizing types
$\tuple{\tau^{reg}, \Delta(A)}$, it is not the case that regret types \emph{as such} are weeded
out by selection. Since, by Fact~\ref{fact:maxEU-minReg}, $\tuple{\tau^{reg}, \overline{\mu}}$ and
$\tuple{\tau^{\pi}, \overline{\mu}}$ are generally behaviorally equivalent, it remains that
selection based on meta-games constructed from $n \times n$ games will still not eradicate all
regret types. So, although regret types may not be selected for in this case, they are also not
selected against.

On the other hand, there are plenty of ways in which the basic insight from
Proposition~\ref{proposition1} that regret-based preferences can be strictly better than
veridical objective preferences in case of radical uncertainty could make for situations in
which evolution would select for regret types exclusively, even in $n \times n$ games. If, for
example, epistemic types of players are not what biological evolution selects, but rather
something that the particular choice situation would exogenously give us, then regret-based
preference \emph{can} again drive out veridical preferences altogether. For concreteness,
suppose that only preference types compete and that agents' epistemic types are exogenously
given, in such a way that agents hold flat probabilistic beliefs $\overline{\mu}$ with probability
$1-p$ and are of type $\Delta(A)$ with probability $p$. This transforms the meta-game from
Table~\ref{tab:ExpectedFitness_10x10} into the simpler $4 \times 4$ game in which the payoff
obtained for a preference type is the weighted average over the payoffs of that preference
types in Table~\ref{tab:ExpectedFitness_10x10}. Setting $p = 0.02$ for illustration, we get the
meta-game in Table~\ref{tab:ExogeneousEpistemics}. 

\begin{table}[]
\centering
\begin{tabular}{ccccc}
  \toprule
  & $\tau^{reg}$ 
  & $\tau^{\pi}$ 
  & $\tau^{com}$
  & $\tau^{alt}$ \\ 
  \midrule
  $\tau^{reg} $ & 6.926 & 6.926 & 5.942 & 7.757 \\ 
  $\tau^{\pi} $ & 6.924 & 6.924 & 5.948 & 7.751 \\ 
  $\tau^{com }$ & 6.566 & 6.570 & 5.481 & 7.434 \\ 
  $\tau^{alt} $ & 6.463 & 6.461 & 5.478 & 7.469 \\ 
   \bottomrule
\end{tabular}
\caption{Meta-game for the evolutionary competition between preference types when epistemic types are exogenously
  given}
\label{tab:ExogeneousEpistemics}
\end{table}

The only evolutionarily stable state of this meta-game is the regret type. Accordingly, all of
our simulation runs of the (discrete-time) replicator dynamic converged to monomorphic
regret-type populations. The reason why regret types prosper is because they have a substantial
payoff advantage if paired with radical uncertainty. If radical uncertainty is exogenously
given as something unavoidable that happens to agents (e.g., whenever they really have no
indication as to what the situation holds for them), and even if that happens only very
infrequently (i.e., for rather low $p$), regret types \emph{can} outperform veridical
preference types, as well as competitive and altruistic types.


\subsection{Solitary decisions}
\label{sec:solitary-decisions}

\todo[inline]{text in this section and the next is rough and unpolished, only intended to have
  the basic results recorded, so that we can reshape the article around the basic results}

To see how preference types behave in evolutionary competition based on solitary decision
problems, we approximated, much in the spirit of meta-games, average accumulated payoffs
obtained in randomly generated solitary decision problems. For our purposes, a decision problem
$\tuple{\States, \Acts, \Utils}$ consists of a set of world states $\States$, a set of acts
$\Acts$, and a utility function $\Utils \mycolon \States \times \Acts \rightarrow
\mathbb{R}$.
We generated arbitrary decision problems by selecting numbers of states and acts
$n_\state, n_\act \in \set{2, \dots, 10}$ and then filling the utility table, so to speak, by
independently picking a number $\Utils(\state, \act) \in \set{0, 10}$ uniformly at random, just
as we did before with symmetric games. Unlike with two-player games, we need to also sample the
actual state of the world, which we selected uniformly at random from the available states in
the current decision problem. Player types, consisting of epistemic types $\overline{\mu}$ and
$\Delta(\States)$ and preference types $\tau^{reg}$ and $\tau^{\pi}$, give rise to behavior in
the same way as before. (It is obviously not possible to carry altruistic or competitive
preference types over to solitary decision making.) As before, we recorded the behavioral
decisions for each of the four relevant player types in each sampled decision problem, and
determined the actual payoff obtained, given the actual state. Average payoff over 100,000 decision
problems are given in Table~\ref{tab:SolitaryDecisions}.

\begin{table}
  \centering
  \begin{tabular}{cccc}
    \toprule
   $\tuple{\tau^{reg}, \Delta(A)}$ 
 & $\tuple{\tau^{\pi}, \Delta(A)}$ 
 & $\tuple{\tau^{reg}, \overline{\mu}}$ 
 & $\tuple{\tau^{\pi}, \overline{\mu}}$ 
 \\ \midrule
    6.318 & 6.237 & 6.661 & 6.661 \\ \bottomrule
  \end{tabular}
  \caption{Average payoffs over 100,000 simulated solitary decision problems}
  \label{tab:SolitaryDecisions}
\end{table}

Facts~\ref{fact:maxEU-minReg} and \ref{fact:equivalence2x2} still apply:
$\tuple{\tau^{reg}, \overline{\mu}}$ and $\tuple{\tau^{\pi}, \overline{\mu}}$ are behaviorally equivalent
in general, and $\tuple{\tau^{reg}, \Delta(A)}$ is behaviorally equivalent to the former two in
decision problems with 2 states and 2 acts. \todo{show this?} We see this in part in the
results in Table~\ref{tab:SolitaryDecisions} in that the averages for $\tuple{\tau^{reg},
  \overline{\mu}}$ and $\tuple{\tau^{\pi}, \overline{\mu}}$ are identical. But since we included decision
problems with more acts and more states as well, the averages for regret minimizers
$\tuple{\tau^{reg}, \Delta(A)}$ are not identical. They are, in fact, lower, but not as low as
those of the minimax choosers. 

What that means is that basically every relevant result about game situations is also borne out
when reasoning about solitary decisions. Evolutionary selection based on objective fitness will
not select against regret-based subjective preferences, as these are indistinguishable from
veridical preferences when paired with probabilistic beliefs. But when paired with
non-probabilistic belief types, regret-based preferences actually outperform veridical
preferences. Consequently, if there is a chance, however small, that agents must fall back onto
non-probabilistic beliefs, evolution will actually positively select for non-veridical
regret-based preferences.

\subsection{Arbitrary probabilistic beliefs}
\label{sec:arbitr-prob-beli}

So far, we have assumed that epistemic types $\overline{\mu}$ hold an unbiased, flat belief. It is
worthwhile considering what happens when this assumption is relaxed and we allow agents with
probabilistic beliefs to make use of the full spectrum of probabilistic beliefs. It should be
clear that quality of beliefs directly impacts prospects of successful decision-making: if a
decision maker knows the actual state, or can put a high level of credence on the actual state,
accumulated fitness can be expected to be high. We see this, unsurprisingly, in numerical
simulations. We looked at the average payoff accumulated in 100,000 decision problems sampled
as before, except that for belief types with probabilistic beliefs we sampled a random
probabilistic belief $p$ (from an unbiased Dirichlet distribution) and chose the actual world
state with probability weights $p$. This effectively implements a bias for beliefs of the
decision maker that tend to put more weight on the actual state (although the procedure does
admittedly perverse the natural chicken-and-egg logic in this case that the actual world state
should come first and beliefs be a function of it). As a result, the average payoffs of types
$\tuple{\tau^{reg}, \overline{\mu}}$ and $\tuple{\tau^{\pi}, \overline{\mu}}$ are still the exact same
(because we compare what agents with different preference types would do under the same
beliefs; we are not interested in statistical fluctuations given one type better beliefs by
pure happenstance), but with $7.125$ notably higher than before. In effect, better
probabilistic beliefs lead to better decisions. If agents can learn, reason and extrapolate to
form better probabilistic beliefs, that will help them whenever they take their beliefs into
account. 

But this is orthogonal to the arguments in this paper, where the focus is on possibilities for
the evolution of non-veridical preferences. We know from Fact~\ref{fact:maxEU-minReg} that
regret types and veridical preference types, when paired with probabilistic belief types that
maximize/minimize expectations, are behaviorally equivalent, \emph{no matter what they believe},
as long as they belief the same. So, if learning, insight and statistical knowledge of a
recurrent situation \emph{can} be brought to bear, this will not make evolution select against
regret-based preferences. If, on the other hand, agents have no basis for probabilistic
beliefs, then we have shown that there are general and basic circumstances in which
regret-based preferences can actually be selected, despite their non-veridicality.

\section{Discussion}

\begin{itemize}
\item what we achieved
\item what could be done next
\end{itemize}




\appendix


\section{Proofs}

The proof of Proposition 1 relies on a partition of $\mathcal{G}$,
and on some lemmas. For brevity, let us denote the regret minimizer
by RM and the maximinimizer by MM. Since the proof is very long and
all the other cases are similar and easier to prove, we focus here
on the case of RM against MM players: we prove that RM is evolutionarily
stable against MM, but MM is not evolutionarily stable against RM. By $EU_{\mathcal{G}}(x,y)$ we denote the expected \textit{objective} utility (or payoff), in terms
of evolutionary fitness, of player type $x$ against player type $y$
with respect to the class of fitness games $\mathcal{G}$. 

\medskip{}


\textbf{Proof of Proposition 1.} By definition of evolutionary stability, we have to
show that in the class $\mathcal{G}$ of symmetric $2\times2$ games generated by i.i.d. sampling
from a finite or compact and convex set of values, it holds:
\begin{itemize}
\item[(i)] $EU_{\mathcal{G}}(RM,RM)>EU_{\mathcal{G}}(MM,RM);$
\item[(ii)] $EU_{\mathcal{G}}(MM,MM)<EU_{\mathcal{G}}(RM,MM).$
\end{itemize}


\noindent Let us consider the case of sampling from a finite set of values first. To show
the result we will use the following partition of $\mathcal{G}$.
\begin{enumerate}
\item Coordination games $\mathcal{G}^{C}$: $a>c$ and $d>b$;
\item Anti-coordination games $\mathcal{G}^{A}$: $a<c$ and $d<b$;
\item Strong dominance games $\mathcal{G}^{S}$: aut $(a>c$ and $b>d)$
aut $(a<c$ and $b<d)$;
\item Weak dominance games $\mathcal{G}^{W}$: aut $a=c$ aut $b=d$;
\item Indifferent games $\mathcal{G}^{I}$: $a=c$ and $b=d$.
\end{enumerate}
Before proving the lemmas, it is convenient to fix some notation.
We denote by $G^{C}$ a coordination game in $\mathcal{G}^{C}$ with
payoffs $a^{G^{C}}$, $b^{G^{C}}$, $c^{G^{C}}$, and $d^{G^{C}}$;
similarly, for $G^{A}$, $G^{S}$, $G^{W}$, and $G^{I}$. Let us
call $I_{G^{C}}^{RM}$ the event that a RM player plays action $I$
in the game $G^{C}$; and similarly for action $II$, for player MM,
and for games $G^{A}$, $G^{S}$, $G^{W}$ and $G^{I}$. Moreover,
let us recall the definition of evolutionarily stable strategy and
neutrally stable strategy.

\medskip{}

\begin{definition}[ESS-NSS]
A strategy $s^{\$}$ is \emph{evolutionarily stable
(ESS)} if for any other strategy $s$:
\begin{enumerate}
\item $\pi(s^{\$},s^{\$})>\pi(s,s^{\$})$, or
\item $\pi(s^{\$},s^{\$})=\pi(s,s^{\$})$ and $\pi(s^{\$},s)>\pi(s,s)$.
\end{enumerate}
\noindent A strategy $s^{\$}$ is \emph{neutrally stable (NSS)} if
for any other strategy $s$:
\begin{enumerate}
\item $\pi(s^{\$},s^{\$})>\pi(s,s^{\$})$, or
\item $\pi(s^{\$},s^{\$})=\pi(s,s^{\$})$ and $\pi(s^{\$},s)\geq\pi(s,s)$.
\end{enumerate}
\end{definition}

\medskip{}

\begin{lemma}
RM and MM perform equally well in $\mathcal{G}^{S}$
and in $\mathcal{G}^{I}$. 
\end{lemma}

\begin{proof}
By definition of regret minimization and maximin it
is easy to check that whenever in a game there is a strongly dominant
action $a^{\$}$, then $a^{\$}$ is both the maximin action and the
regret minimizing action. Then, for all the games in $\mathcal{G}^{S}$,
a RM chooses action $a$ if and only if MM chooses action $a$. Consequently,
RM and MM always perform equally (well) in $\mathcal{G}^{S}$. In
the case of $\mathcal{G}^{I}$ it is trivial to see that all the players
perform equally.
\end{proof}

\medskip{}

\begin{lemma}
In $\mathcal{G}^{W}$ RM is evolutionarily stable
against MM, but MM is not evolutionarily stable against RM.
\end{lemma}

\begin{proof}
Assume without loss of generality that $b=d$. We have
two cases then: (i) $c<b=d$; (ii) $c>b=d$. In the first case it
is easy to see that RM and MM perform equally well. Indeed, given
(i) both MM and RM choose action $I$ if $a>c$, and action $II$
if $c>a$. If case (ii) holds instead, we have that RM plays $I$
if $a>c$ and $II$ if $c>a$ as before, but MM plays $(\frac{1}{2}I;\frac{1}{2}II)$,
since both $I$ and $II$ maximize the minimal payoff. Consider now
a population of RM and MM playing games from the class $\mathcal{G}^{W}$.
Whenever (i) is the case both RM and MM perform equally. Suppose that
(ii) is the case and that $a>c$, without loss of generality. Then,
a RM population is evolutionarily stable against MM by condition 1
of ESS, since $a>\frac{1}{2}a+\frac{1}{2}c$, whereas a MM population
is not evolutionarily stable against RM, because clearly $\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d<\frac{1}{2}a+\frac{1}{2}b$.
Hence, we have that in general $EU_{\mathcal{G}^{W}}(RM,RM)>EU_{\mathcal{G}^{W}}(MM,RM),\mbox{ and }EU_{\mathcal{G}^{W}}(MM,MM)<EU_{\mathcal{G}^{W}}(RM,MM)$.
\end{proof}

\medskip{}


Since it is not difficult to see that both RM and MM are evolutionarily
stable in $\mathcal{G}^{C}$, and that both RM and MM are not evolutionarily
stable in $\mathcal{G}^{A}$ against each other, the main part of
the proof will be to show that RM is the only evolutionarily stable
player in the class $\mathcal{G}^{C}\cup\mathcal{G^{A}}$, that is:
\begin{itemize}
\item[(i')] $EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(RM,RM)>EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(MM,RM),$
\item[(ii')] $EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(MM,MM)<EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(RM,MM).$
\end{itemize}


\noindent This part needs some more lemmas to be proven, but firstly we introduce
the following bijective function $\phi$ between coordination and
anti-coordination games.

\medskip{}

\begin{definition}[$\phi$]
$\phi:\mathcal{G}^{C}\rightarrow\mathcal{G^{A}}$,
such that $\phi(a,b,c,d)=(c,d,a,b)$, is a bijection that for each
coordination game $G^{C}\in\mathcal{G}^{C}$ with payoffs $(a^{G^{C}},b^{G^{C}},c^{G^{C}},d^{G^{C}})$
gives the anti-coordination game $G^{A}\in\mathcal{G}^{A}$ with payoffs
$(a^{G^{A}},b^{G^{A}},c^{G^{A}},d^{G^{A}})=(c^{G^{C}},d^{G^{C}},a^{G^{C}},b^{G^{C}})$.
\end{definition}

\medskip{}

\begin{lemma}
Let $P(E)$ be the probability of event $E$, then
it holds that:
\begin{itemize}
\item $P(I_{G^{C}}^{RM})=P(II_{\phi(G^{C})}^{RM})$, and $P(II_{G^{C}}^{RM})=P(I_{\phi(G^{C})}^{RM})$;
\item $P(I_{G^{C}}^{MM})=P(II_{\phi(G^{C})}^{MM})$, and $P(II_{G^{C}}^{MM})=P(I_{\phi(G^{C})}^{MM})$.
\end{itemize}
\end{lemma}

\begin{proof}
It is easy to check that if $b^{G^{C}}-d^{G^{C}}>c^{G^{C}}-a^{G^{C}}$,
a RM player plays action $I$ in $G^{C}$; that if $b^{G^{C}}-d^{G^{C}}<c^{G^{C}}-a^{G^{C}}$,
RM plays $II$; and that if $b^{G^{C}}-d^{G^{C}}=c^{G^{C}}-a^{G^{C}}$,
a RM player is indifferent between $I$ and $II$ in $G^{C}$, and
we assume that randomizes with $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly,
if $a^{G^{A}}-c^{G^{A}}>d^{G^{A}}-b^{G^{A}}$, a RM player plays action
$I$ in $G^{A}$; if $a^{G^{A}}-c^{G^{A}}<d^{G^{A}}-b^{G^{A}}$, RM
plays $II$; and if $a^{G^{A}}-c^{G^{A}}=d^{G^{A}}-b^{G^{A}}$, a
RM player is indifferent between $I$ and $II$ in $G^{A}$, and randomizes
with $(\frac{1}{2}I;\frac{1}{2}II)$. Consequently, if $b^{G^{C}}-d^{G^{C}}>c^{G^{C}}-a^{G^{C}}$,
then $P(I_{G^{C}}^{RM})=1$, and by definition of $\phi$ we have
$P(II_{\phi(G^{C})}^{RM})=1$. Likewise, if $b^{G^{C}}-d^{G^{C}}<c^{G^{C}}-a^{G^{C}}$,
then $P(II_{G^{C}}^{RM})=1=P(I_{\phi(G^{C})}^{RM})$; and if $b^{G^{C}}-d^{G^{C}}=c^{G^{C}}-a^{G^{C}}$,
then $P(I_{G^{C}}^{RM})=P(II_{G^{C}}^{RM})=\frac{1}{2}=P(II_{\phi(G^{C})}^{RM})=P(I_{\phi(G^{C})}^{RM})$. \\
In the same way, in coordination games we have that if $b^{G^{C}}>c^{G^{C}}$,
a MM player plays $I$; if $c^{G^{C}}>b^{G^{C}}$, a MM player plays
$II$; and if $b^{G^{C}}=c^{G^{C}}$, MM is indifferent between $I$
and $II$, and plays $(\frac{1}{2}I;\frac{1}{2}II)$. In anti-coordination
games instead, if $a^{G^{A}}>d^{G^{A}}$, MM plays $I$; if $a^{G^{A}}<d^{G^{A}}$,
MM plays $II$; if $a^{G^{A}}=d^{G^{A}}$, MM plays $(\frac{1}{2}I;\frac{1}{2}II)$.
By definition of $\phi$: $P(I_{G^{C}}^{MM})=1=P(II_{\phi(G^{C})}^{MM})$
if $b^{G^{C}}>c^{G^{C}}$; $P(II_{G^{C}}^{MM})=1=P(I_{\phi(G^{C})}^{MM})$
if $c^{G^{C}}>b^{G^{C}}$; and $P(I_{G^{C}}^{MM})=P(II_{G^{C}}^{MM})=\frac{1}{2}=P(II_{\phi(G^{C})}^{MM})=P(I_{\phi(G^{C})}^{MM})$
if $b^{G^{C}}=c^{G^{C}}$.
\end{proof}

\medskip{}

\begin{lemma}
$P(\phi(G^{C}))=P(G^{C})$.
\end{lemma}

\begin{proof}
By definition, each game $G^{C}\equiv(a^{G^{C}},b^{G^{C}},c^{G^{C}},d^{G^{C}})$
is such that $a^{G^{C}}>c^{G^{C}}$ and $d^{G^{C}}>b^{G^{C}}$, and
each game $G^{A}\equiv(a^{G^{A}},b^{G^{A}},c^{G^{A}},d^{G^{A}})$
is such that $a^{G^{A}}<c^{G^{A}}$ and $d^{G^{A}}>b^{G^{A}}$. Since
$a,b,c,d$ come from i.i.d. sampling, by simple renaming of variables
it is clear that the probability of $(a^{G^{C}},b^{G^{C}},c^{G^{C}},d^{G^{C}})$
is equal to the probability of $(c^{G^{C}},d^{G^{C}},a^{G^{C}},b^{G^{C}})$.
Hence, $P(\phi(G^{C}))=P(G^{C})$.
\end{proof}

\medskip{}

\begin{lemma}
It holds that:
\begin{itemize}
\item $a^{G^{C}}>d^{G^{C}}\rightarrow(I_{G^{C}}^{MM}\subset I_{G^{C}}^{RM})$;
\item $a^{G^{C}}<d^{G^{C}}\rightarrow(II_{G^{C}}^{MM}\subset II_{G^{C}}^{RM})$;
\item $a^{G^{C}}=d^{G^{C}}\rightarrow I_{G^{C}}^{MM}=I_{G^{C}}^{RM}$.
\end{itemize}
\end{lemma}

\begin{proof}
If $b^{G^{C}}-d^{G^{C}}>c^{G^{C}}-a^{G^{C}}$, RM plays
$I$, and if $b^{G^{C}}-d^{G^{C}}=c^{G^{C}}-a^{G^{C}}$, RM plays
$(\frac{1}{2}I;\frac{1}{2}II)$, whereas if $b^{G^{C}}>c^{G^{C}}$,
MM plays $I$, and if $b^{G^{C}}=c^{G^{C}}$, MM plays $(\frac{1}{2}I;\frac{1}{2}II)$.
Then, $I_{G^{C}}^{RM}$ implies that $b^{G^{C}}-d^{G^{C}}\geq c^{G^{C}}-a^{G^{C}}$,
and $I_{G^{C}}^{MM}$ implies that $b^{G^{C}}\geq c^{G^{C}}$. Moreover,
on the assumption that $a^{G^{C}}>d^{G^{C}}$, it is easy to check
that $b^{G^{C}}\geq c^{G^{C}}$ implies $b^{G^{C}}-d^{G^{C}}>c^{G^{C}}-a^{G^{C}}$.
Hence, in any $G^{C}$ with $a^{G^{C}}>d^{G^{C}}$ it holds that $I_{G^{C}}^{MM}\mbox{ implies }I_{G^{C}}^{RM}$,
i.e., $a^{G^{C}}>d^{G^{C}}\rightarrow(I_{G^{C}}^{MM}\subseteq I_{G^{C}}^{RM})$.
Instead, it is possible that $a^{G^{C}}>d^{G^{C}}$, $b^{G^{C}}-d^{G^{C}}>c^{G^{C}}-a^{G^{C}}$
and $b^{G^{C}}<c^{G^{C}}$ hold simultaneously, so that $I_{G^{C}}^{MM}\nsupseteq I_{G^{C}}^{RM}$.
Consequently: $a^{G^{C}}>d^{G^{C}}\rightarrow(I_{G^{C}}^{MM}\subset I_{G^{C}}^{RM})$.
By symmetric argument it can be shown that $a^{G^{C}}<d^{G^{C}}\rightarrow(II_{G^{C}}^{MM}\subset II_{G^{C}}^{RM})$
too. \\
Finally, when $a^{G^{C}}=d^{G^{C}}$ it holds that: $b^{G^{C}}-d^{G^{C}}>c^{G^{C}}-a^{G^{C}}$
iff $b^{G^{C}}>c^{G^{C}}$; $b^{G^{C}}-d^{G^{C}}<c^{G^{C}}-a^{G^{C}}$
iff $b^{G^{C}}<c^{G^{C}}$; and $b^{G^{C}}-d^{G^{C}}=c^{G^{C}}-a^{G^{C}}$
iff $b^{G^{C}}=c^{G^{C}}$. Hence, $a^{G^{C}}=d^{G^{C}}\rightarrow I_{G^{C}}^{MM}=I_{G^{C}}^{RM}$.
\end{proof}

\medskip{}


We are now ready to prove that: 
\begin{itemize}
\item[(i')] $EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(RM,RM)>EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(MM,RM);$
\item[(ii')] $EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(MM,MM)<EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(RM,MM).$
\end{itemize}
We can rewrite $EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(RM,RM)>EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(MM,RM)$
more explicitly as:

\medskip{}

\noindent $\sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot c^{C}]+\sum_{G^{A}}P(G^{A})[P(I_{G^{A}}^{RM}\cap I_{G^{A}}^{RM})\cdot a^{A}+P(II_{G^{A}}^{RM}\cap II_{G^{A}}^{RM})\cdot d^{A}+P(I_{G^{A}}^{RM}\cap II_{G^{A}}^{RM})\cdot b^{A}+P(II_{G^{A}}^{RM}\cap I_{G^{A}}^{RM})\cdot c^{A}] > \sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot c^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot b^{C}]+\sum_{G^{A}}P(G^{A})[P(I_{G^{A}}^{RM}\cap I_{G^{A}}^{MM})\cdot a^{A}+P(II_{G^{A}}^{RM}\cap II_{G^{A}}^{MM})\cdot d^{A}+P(I_{G^{A}}^{RM}\cap II_{G^{A}}^{MM})\cdot c^{A}+P(II_{G^{A}}^{RM}\cap I_{G^{A}}^{MM})\cdot b^{A}]$
\medskip{}

\noindent Then, the next derivation follows. By Lemma 5 and 6, we can express
everything in terms of $G^{C}$ only: 
\begin{itemize}

\item $\sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot c^{C}]+\sum_{G^{A}}P(G^{A})[P(I_{G^{A}}^{RM}\cap I_{G^{A}}^{RM})\cdot a^{A}+P(II_{G^{A}}^{RM}\cap II_{G^{A}}^{RM})\cdot d^{A}+P(I_{G^{A}}^{RM}\cap II_{G^{A}}^{RM})\cdot b^{A}+P(II_{G^{A}}^{RM}\cap I_{G^{A}}^{RM})\cdot c^{A}] > \sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot c^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot b^{C}]+\sum_{G^{A}}P(G^{A})[P(I_{G^{A}}^{RM}\cap I_{G^{A}}^{MM})\cdot a^{A}+P(II_{G^{A}}^{RM}\cap II_{G^{A}}^{MM})\cdot d^{A}+P(I_{G^{A}}^{RM}\cap II_{G^{A}}^{MM})\cdot c^{A}+P(II_{G^{A}}^{RM}\cap I_{G^{A}}^{MM})\cdot b^{A}]$

\item $\sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot c^{C} +P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot c^{C}+P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})\cdot a^{C}] > \sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot c^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot c^{C}+P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot a^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot d^{C}]$

\item $\sum_{G^{C}}P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})) + b^{C} \cdot  (P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})) + c^{C} \cdot (P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) +P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}))]> \sum_{G^{C}}P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + b^{C} \cdot (P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})+P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + c^{C} \cdot (P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))]$

\end{itemize}

\noindent Now let us split into $a>d$ and $a<d$, and consider $a>d$ first.
Notice that, by Lemma 7, the case $a=d$ is irrelevant in order to
discriminate between RM and MM. If $a>d$, by Lemma 7 we can eliminate
the cases where RM plays $II$ and MM plays $I$:

\begin{itemize}

\item $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})) + b^{C} \cdot  (P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})) + c^{C} \cdot (P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) +P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}))]> 
\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM}) + b^{C} \cdot P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM}) + c^{C} \cdot (P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))]$

\item $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})- P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + b^{C} \cdot  (P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})- P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + c^{C} \cdot (P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) +P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))]> 0$

\end{itemize}

\noindent We now distinguish between two cases: (1) $ a-c = d-b $ and (2) $  a-c \neq d-b $. Notice that $P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM}) \neq 0$ if and only if case (1) obtains, and that $a>d$ and (1) imply $II_{G^{C}}^{MM}$. Then, from (1) we have:

\begin{itemize}

\item $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})) + b^{C} \cdot  (P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{RM}) + P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})) + c^{C} \cdot (P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) +P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))]> 0$

\item $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (\frac{1}{4}+\frac{1}{4}) + b^{C} \cdot  (\frac{1}{4}+\frac{1}{4}) + c^{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2}) + d^{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2})]> 0$

\item $\sum_{G_{a>d}^{C}} P(G^{C})[\frac{1}{2}a^{C}+ \frac{1}{2}b^{C} - \frac{1}{2}c^{C} - \frac{1}{2}d^{C}]> 0$

\end{itemize}

\noindent Since we have assumed $ a-c = d-b $, the last inequality is not satisfied. We have instead: 

$$ \sum_{G_{a>d}^{C}} P(G^{C})[\frac{1}{2}a^{C}+ \frac{1}{2}b^{C} - \frac{1}{2}c^{C} - \frac{1}{2}d^{C}]= 0 $$

\noindent This means that, when (1) is the case, RM and MM are neutrally stable. To prove the result we then have to show that the inequality is strict in favor of RM when case (2) obtains. \\
When (2) is the case, it is easy to check, by Lemma 7, that $P(I_{G^{C}}^{RM} \cap I_{G^{C}}^{RM})-P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})=P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})$
and $P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})=P(II_{G^{C}}^{RM} \cap II_{G^{C}}^{RM})$.
Consequently, we have:

\begin{itemize}

\item $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM}) - P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + b^{C} \cdot  (P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{RM})- P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + c^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{RM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))]> 0$

\item $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}) + b^{C} \cdot  P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}) - c^{C} \cdot P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}) - d^{C} \cdot P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})]> 0$

\item $\sum_{G_{a>d}^{C}} P(G^{C})[P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot (a^{C} + b^{C} - c^{C} - d^{C})]> 0$

\end{itemize}
\medskip{}

\noindent We know that $I_{G^{C}}^{RM}$ implies that $a^{G^{C}}-c^{G^{C}}\geq d^{G^{C}}-b^{G^{C}}$.
%and $II_{G^{C}}^{MM}$ implies that $b^{G^{C}}\leq c^{G^{C}}$. 
Since we have assumed that $a^{G^{C}}-c^{G^{C}}\neq d^{G^{C}}-b^{G^{C}}$, we have that $a^{G^{C}}-c^{G^{C}} > d^{G^{C}}-b^{G^{C}}$. Hence, the inequality 
$$\sum_{G_{a>d}^{C}} P(G^{C})[P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot (a^{C} + b^{C} - c^{C} - d^{C})]> 0$$
is satisfied. So, when (2) obtains, RM is evolutionarily stable against MM.
%From $b^{G^{C}}-d^{G^{C}}\geq c^{G^{C}}-a^{G^{C}}$, it is easy to see that $\sum_{G_{a>d}^{C}}P(G_{a>d}^{C})\cdot P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot[a^{G^{C}}-c^{G^{C}}]\geq\sum_{G_{a>d}^{C}}P(G_{a>d}^{C})\cdot P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot[d^{G^{C}}-b^{G^{C}}]$ holds. When the event $I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}$ obtains on the assumption that $a^{G^{C}}>d^{G^{C}}$, there are only three possible payoff orderings: $a>c\geq d>b$, $a>d>c>b$, and $a>d>c=b$. If games are randomly sampled from a finite set with i.i.d. values for $a,b,c$ and $d$, then $a>d>c=b$ occurs with positive probability, and the last formula of the derivation holds with strict inequality too. Hence, RM is evolutionarily stable when the value set for $a,b,c,d$ is finite. \\ 
%If i.i.d. values are sampled from a compact and convex set instead, it is again easy to see that $\forall a,d,c\ \exists\epsilon$ such that $\forall b\in(d,d+\epsilon)$ the above formula holds with strict inequality. We can simply take $\epsilon\equiv\frac{a-c}{2}$, and the interval $(d,d+\epsilon)$ always has positive probability. Hence, RM is evolutionarily stable when the value set for $a,b,c,d$ is a compact and convex set too. \\ 
Symmetrically, from $a<d$ and by distinguishing between the two cases (1) and (2) as before, in the end  we get:

\begin{itemize}

\item[(1)] $\sum_{G_{a<d}^{C}} P(G^{C})[-\frac{1}{2}a^{C}- \frac{1}{2}b^{C} + \frac{1}{2}c^{C} + \frac{1}{2}d^{C}]= 0$; and

\item[(2)] $\sum_{G_{a<d}^{C}} P(G^{C})[P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot (-a^{C} - b^{C} + c^{C} + d^{C})]> 0$.

\end{itemize}


\noindent Hence, we can conclude that RM is evolutionarily stable against MM in the class $\mathcal{G}^{C}\cup\mathcal{G^{A}}$. \\
In the case of sampling i.i.d. values from a real interval instead, games in $\mathcal{G}^I $ and $\mathcal{G}^W $ never arise, and the proof is the same for the remaining games in $\mathcal{G}^S $,  $\mathcal{G}^C $ and $\mathcal{G}^A $.

\medskip{}

The second half of the result is that MM is not evolutionarily stable
against RM in $\mathcal{G}^{C}\cup\mathcal{G^{A}}$: $EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(MM,MM)<EU_{\mathcal{G}^{C}\cup\mathcal{G^{A}}}(RM,MM)$.
As before, we write it more explicitly as:

\medskip{}


\noindent $\sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM})\cdot a^{C}+P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})\cdot d^{C}+P(I_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})\cdot b^{C}+P(II_{G^{C}}^{MM}\cap I_{G^{C}}^{MM})\cdot c^{C}]+\sum_{G^{A}}P(G^{A})[P(I_{G^{A}}^{MM}\cap I_{G^{A}}^{MM})\cdot a^{A}+P(II_{G^{A}}^{MM}\cap II_{G^{A}}^{MM})\cdot d^{A}+P(I_{G^{A}}^{MM}\cap II_{G^{A}}^{MM})\cdot b^{A}+P(II_{G^{A}}^{MM}\cap I_{G^{A}}^{MM})\cdot c^{A}] < \sum_{G^{C}}P(G^{C})[P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot a^{C}+P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot d^{C}+P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})\cdot b^{C}+P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})\cdot c^{C}]+\sum_{G^{A}}P(G^{A})[P(I_{G^{A}}^{RM}\cap I_{G^{A}}^{MM})\cdot a^{A}+P(II_{G^{A}}^{RM}\cap II_{G^{A}}^{MM})\cdot d^{A}+P(I_{G^{A}}^{RM}\cap II_{G^{A}}^{MM})\cdot b^{A}+P(II_{G^{A}}^{RM}\cap I_{G^{A}}^{MM})\cdot c^{A}]$.
\medskip{}

\noindent When $a>d$, similarly to the above derivation, we get: 

\medskip{}
\noindent $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) + P(I_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM}) - P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + b^{C} \cdot  (P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) + P(I_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM}) - P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + c^{C} \cdot (P(II_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) +P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})+P(II_{G^{C}}^{MM}\cap I_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))]< 0$
\medskip{}

\noindent We now distinguish between (1) $b=c$, (2) $b>c$, and (3) $b<c$. Notice that either (1) or (2), together with $a>d$, implies $I_{G^{C}}^{RM}$. Then we obtain: 
\begin{itemize}

\item[(1)] $\sum_{G_{a<d}^{C}} P(G^{C})[-\frac{1}{2}a^{C} - \frac{1}{2}b^{C} + \frac{1}{2}c^{C} + \frac{1}{2}d^{C}]< 0$;

\item[(2)] $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) - P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + b^{C} \cdot  (P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) - P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM}))]= 0$;

\item[(3)] $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + b^{C} \cdot  (- P(I_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + c^{C} \cdot (P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))] \leq 0$.

\end{itemize}

\noindent When $a<d$, the derivation proceeds symmetrically and we get: 

\begin{itemize}

\item[(1)] $\sum_{G_{a<d}^{C}} P(G^{C})[\frac{1}{2}a^{C} + \frac{1}{2}b^{C} - \frac{1}{2}c^{C} - \frac{1}{2}d^{C}]< 0$;

\item[(2)] $\sum_{G_{a>d}^{C}} P(G^{C})[a^{C} \cdot (P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) - P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + b^{C} \cdot  (P(I_{G^{C}}^{MM}\cap I_{G^{C}}^{MM}) - P(I_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + c^{C} \cdot (- P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM})) + d^{C} \cdot (- P(II_{G^{C}}^{RM}\cap I_{G^{C}}^{MM}))] \leq 0$

\item[(3)] $\sum_{G_{a>d}^{C}} P(G^{C})[c^{C} \cdot (P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM})) + d^{C} \cdot (P(II_{G^{C}}^{MM}\cap II_{G^{C}}^{MM})- P(II_{G^{C}}^{RM}\cap II_{G^{C}}^{MM}))] = 0$.

\end{itemize}


\noindent Finally, we can conclude that MM is
not evolutionarily stable against RM both when the value set for $a,b,c,d$
is finite, and when the value set for $a,b,c,d$ is a compact and
convex set. This concludes the proof. $\dashv$

\newpage

\textbf{Proof of Proposition 2.} First we prove that regret type is evolutionarily stable against altruistic type and that altruistic type is not evolutionarily stable against regret type, and second we show that regret type is evolutionarily stable against competitive type and that competitive type is not evolutionarily stable against regret type. We make use of the same partition of $\mathcal{G}$ introduced in the proof of Proposition 1. Again, to ease notation let us write ALT and COM in the place of $(\tau^{alt}, \Delta(A))$ and $(\tau^{com}, \Delta(A))$, respectively.

\begin{lemma}
In $\mathcal{G}^I$, RM and ALT perform equally well.
\end{lemma}

\begin{proof}
Trivial.
\end{proof}

\begin{lemma}
In $\mathcal{G}^S$, RM is evolutionarily stable against ALT, and ALT is not evolutionarily stable against RM. 
\end{lemma}

\begin{proof}
Let us assume, without loss of generality, that $a>c$ and $b>d$. We already know that RM always plays the dominant action $I$, then a population of RM players always coordinates on $I$ with a payoff of $a$. It is possible instead that an ALT player plays the dominated action $II$, for instance when $2d > 2a$ and $2a < b+c$. Then, the payoff of an ALT invader against a population of RMs is $c$. Since $a>c$ by assumption, we get that a RM population cannot be invaded by ALT players in this case. When $b+c < 2a$ and $b+c < 2d$, ALT will play $(\frac{1}{2}I;\frac{1}{2}II)$ for an expected payoff of $\frac{1}{2} a + \frac{1}{2} c$. Since $a > \frac{1}{2} a + \frac{1}{2} c$, a population of RMs is evolutionarily stable against ALT. \\
When $2d > 2a$ and $2a < b+c$, an ALT population plays action $II$ for a payoff of $d$, whereas a RM invader plays action $I$ for a payoff of $b$. By assumption $b>d$, hence a population of ALTs would not be evolutionarily stable in this case. When $b+c < 2a$ and $b+c < 2d$, the expected payoff of a population of ALTs is $\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d$, while the expected payoff of a RM invader is $\frac{1}{2}a+\frac{1}{2}b$. Since $\frac{1}{2}a+\frac{1}{2}b > \frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d$, we have that an ALT population is not evolutionarily stable against RM.
\end{proof}



\begin{lemma}
In $\mathcal{G}^W$, RM is evolutionarily stable against ALT, and ALT is not evolutionarily stable against RM. 
\end{lemma}

\begin{proof}
Let us assume, WLOG, that $b=d$ and $a>c$. We know that RM always plays the (weakly) dominant action $I$. Again, a RM population always coordinates on $I$ for a payoff of $a$, whereas when $2d > 2a$ and $2a < b+c$ an ALT invader plays $II$. Then, the payoff of an ALT invader against a population of RMs is $c$. Since $a>c$ by assumption, we get that a RM population cannot be invaded by ALT players in this case. When $b+c < 2a$ and $b+c < 2d$, ALT plays $(\frac{1}{2}I;\frac{1}{2}II)$ for an expected payoff of $\frac{1}{2} a + \frac{1}{2} c$. Since $a > \frac{1}{2} a + \frac{1}{2} c$, a population of RMs is evolutionarily stable against ALT. \\
When $2d > 2a$ and $2a < b+c$, an ALT population plays action $II$ for a payoff of $d$, whereas a RM invader plays action $I$ for a payoff of $b$. Since $b=d$, we have to check the second condition for ESS: an ALT against RM gets $c$, and RM against RM gets $a$. By assumption $a>c$, hence an ALT population would be invaded by RM players in this case. 
When $b+c < 2a$ and $b+c < 2d$, the expected payoff of a population of ALTs is $\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d$, while the expected payoff of a RM invader is $\frac{1}{2}a+\frac{1}{2}b$. Since $\frac{1}{2}a+\frac{1}{2}b > \frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d$, we again have that an ALT population is not evolutionarily stable against RM.
\end{proof}

\begin{lemma}
In $\mathcal{G}^C$, RM is evolutionarily stable against ALT, and ALT is not evolutionarily stable against RM. 
\end{lemma}

\begin{proof}
Let's assume, WLOG, that $a>d$. We know that RM plays $I$ if $ a-c > d-b $ and $II$ if $ a-c < d-b $, whereas  ALT plays $I$ if $2d<b+c$, otherwise ALT plays $(\frac{1}{2}I;\frac{1}{2}II)$. If $2d<b+c$, then $d-b<c-d$, hence $c>d$. Consequently, when $2d<b+c$ the expected payoff of an ALT population is $a$
\end{proof}






\printbibliography[heading=bibintoc]


\newpage

\section{Snippets}

[More precisely, from the meta-game perspective many possible subjective
transformations of the objective utility are eliminated by evolution, but we find some
evolutionarily stable subjective utility maximizers whose utility does not coincide with the
objective evolutionary utility. This result leaves some room in between the two extrema: we can
understand rationality in a normative way, given by evolution, without having to discard all
the possible subjective utility representations as irrational. CHANGE PLACE OF THIS MAYBE]



\subsection{Version 2}

The importance of aiming at a richer evolutionary game theory has been recently emphasized by many works in theoretical biology and behavioral ecology [Fawcett\&al., McNamara, etc.]. The biological argument to embrace this goal is related to the criticisms towards the so-called behavioral gambit [Fawcett\&al.]. The behavioral gambit denotes the standard approach in evolutionary game theory that puts the focus of attention on the expressed behavior, and neglects the underlying mechanisms that generate that behavior. From this point of view, the benefit and the reliability coming from the use of evolutionary game theory for biological purposes is also under attack. There are good reasons to claim that focusing on the general psychological mechanisms instead of on behavior itself as the phenotype under evolutionary selection can be more faithful and insightful for natural biology and ecology [Fawcett\&al.].
Standard evolutionary game theory normally models the agents as playing a single fixed game, and as genetically predetermined to play fixed strategies, that consequently are the only target of natural selection in those frameworks. Standard models of evolutionary game theory cannot take charge of this shift of perspective until they will keep modelling a single game at a time, and until they will keep focusing on actions considered genetically encoded and fixed. ''Instead we should expect animals to have evolved a set of psychological mechanisms which enable them to perform well on average across a range of different circumstances''[Fawcett\&al.].

Attempts to overcome the behavioral gambit have recently been developed in economic literature too. In the last twenty years economists have been using evolutionary game-theoretical models to study evolution of preferences [cite], instead of evolution of behavior only. This approach is called indirect evolutionary approach, because players' behavior and actions stem from their subjective preferences, and it is aimed at investigating what subjective preferences are evolutionarily better off and robust with respect to natural selection. This shift from a direct evolutionary approach about observed actions and behavior to an indirect evolutionary approach takes only partly into account the behavioral gambit criticized by biologists and ecologists. The reason is that the models of evolution of preferences usually consider only one game at a time. This means that there is a single type of interaction that is supposed to drive the whole evolution of preferences: preferences are selected on the basis of one possible interaction only. Clearly, this approach does not take into consideration the variety of possible circumstances that animals are supposed to face during their lifetime. 

The meta-game model we propose pays heed to both components of the behavioral gambit. We retain the indirect evolutionary approach, but our agents in the population play over a class of possible games $\mathcal{G}$. In this way we explicitly model a range of different circumstances, as wished by theoretical biologists.  Consequently, each agent in the population does not represent either a simple behavior or a subjective preference, but a more general player type that associates a subjective preference for each game in the class $\mathcal{G}$. The player type is supposed to encode the general psychological mechanisms that generate the behavior for any possible interaction in $\mathcal{G}$. These general psychological mechanisms then become the target of evolution, and they are selected based on the average fitness over a class of different interactions. 

In particular, in the following we mainly deal with two types of agents: players whose subjective preference coincides with the evolutionary fitness, and players whose subjective preference is defined in terms of regret. It turns out that psychological mechanisms and subjective conceptualizations based on regret can outperform more veridical conceptualizations coinciding with the evolutionary fitness.
This is just an example of the kind of insights and results that we can gain by adopting a meta-game perspective and by shifting the focus of our attention to psychological mechanisms and subjective conceptualizations as the phenotype under selection.


-maybe to add: difference with learning. Fawcett\&al. talk more about differences in learning, whereas we model differences in subjective conceptualizations.


[A pivotal element of economics and decision theory is the definition of rationality as
maximization of subjective preferences.]



\begin{itemize}
\item We present an approach to studying evolutionary selection of \emph{choice principles},
  i.e., ways of choosing consistently across several games. In other words, we would like to
  look at the problem of ``rational choice'' from an evolutionary point of view: what choice
  principles would be promoted, which demoted in evolutionary competition amongst each other?

\item This depends on the environment; it depends on what choice situations occur how
  frequently. Maybe some choice principle is better in some situations, while another one is
  better in others. 


\item We argue that this ``meta-evolutionary'' approach could eventually help explain attested
  deviations of human decision making from the classical ideal: choice behavior evolved to be
  an ``ecologically rational'', near-optimal adaptation to the environment
  \citep{Anderson1990:The-Adaptive-Ch,Anderson1991:Is-human-cognit,GigerenzerGoldstein1996:Reasoning-the-F,ChaterOaksford2000:The-Rational-An}. This
  is inline with recent aspirations to provide an evolutionary rationale for decision making
  processes
  \citep[e.g.][]{HammersteinStevens2012:Six-Reasons-for,FawcettHamblin2013:Exposing-the-be}. Here,
  we seek to contribute to the formal machinery, an extension of evolutionary game theory, that
  allows to capture statistical properties of the environment
  \citep[cf.][]{McNamara2013:Towards-a-Riche}.
\end{itemize}



\subsection{The environment: meta-games}

\begin{itemize}
\item imagine huge, virtually infinite population of agents, each carrying a choice principle
\item agents play a randomly sampled game against a randomly sampled opponent
\item fitness is based on the average payoff of choice principle A against choice principle B,
  taken over arbitrary games $g$ and weighted their occurrence probability $P(g)$:
  $\int_g P(g) \times U_g(\text{CP-A}, \text{CP-B}) \text{d}g$
\item the occurrence probability of games $g$ is the ``environment'' and, possibly, impossible to
  derive theoretically, or to measure empirically
\item here, use a relatively neutral approach instead that is practicable:
  \begin{itemize}
  \item games are individuated by their payoff matrices (no framing effects or similar)
  \item each entry in a payoff matrix is an i.~i.~d.~random variable
    \begin{itemize}
    \item approach still incorporates a bias (e.g., against strategic form representations of
      sequential games)
    \end{itemize}
  \end{itemize}
\item example: average payoffs from numerical simulation of ($2 \times 2$ symmetric games)
\item example: average payoffs from numerical simulation of ($n \times n$ symmetric games, with
  $n$ uniformly random (up to some upper-bound))
\end{itemize}


\begin{itemize}
\item Evolutionary game theory has become an established philosophical tool for probing into
  conceptual issues whose complexity requires mathematical modeling.
  \begin{itemize}
  \item helped structure debate about foraging, mating, evolution of cooperation,
    information-flow, deception etc.
  \end{itemize}
\item Most applications of EGT look at evolutionary selection of behavior in a single
  game. There are two crude simplifications here:
  \begin{enumerate}
  \item The stage game is usually considered fixed, closed and immutable.
    \begin{itemize}
    \item there are some notable exceptions where the game is open ended, e.g., by the
      invention of actions \citep{WordenLevin2007:Evolutionary-es,
        McKenzie-AlexanderSkymrs2012:Inventing-New-S}
    \end{itemize}
  \item What evolves are behavioral strategies for the stage game \citep[what][call the
    ``behavioral gamit'']{FawcettHamblin2013:Exposing-the-be} \myalert{(more references!
      Gigerenzer etc.)}.
  \end{enumerate}
\item It has been argued that we should rather study the evolutionary selection of \emph{choice
    mechanisms} that are adapted in a rich and variable environment
  \citep[e.g.][]{FawcettHamblin2013:Exposing-the-be,McNamara2013:Towards-a-Riche}.
\item However, studies on the adaptive value of choice mechanisms have focused, for the most
  part, on the evolutionary benefit of learning rules
  \citep[e.g.][]{ZollmanSmead2010:Plasticity-and-,SmeadZollman2013:The-Stability-o}. (\myalert{More references Harley 1981!!!, \dots}). 
    Moreover, only little attention has been paid to
  modeling the direct (game-theoretic) meta-competition between alternative choice mechanisms
  \citep[see][for related criticism]{FawcettHamblin2013:Exposing-the-be}.
\item Here, we would like to make two contributions to the literature on evolution of choice
  mechanisms:
  \begin{enumerate}
  \item On the conceptual side, we argue that paying attention to the evolution of (subjective
    representations of) preferences matters. Since almost any reasonable learning or
    decision-making process will make use of some (subjective, agent-internal (possibly
    unconscious)) representation of the quality of choice options, the question of which such
    representations are ecologically valuable (i.e., lead to high fitness) is central. In fact,
    the evolution of (subjective representations of) preferences has recently been studied in
    theoretical economics as well \myalert{add references}. However, these studies also suffer
    from the ``behavioral gambit'' and the usual shortcomings of the closedness of the stage game. 
  \item On the methodological side, we therefore introduce a conservative extension of the
    standard EGT that allows us to reuse notions such as evolutionary stability / evolutionary
    dynamics and yet helps us study the evolutionary competition between choice mechanisms in a
    statistically variable environment. Concretely, we look at the evolutionary competition of
    choice mechanisms in a ``meta-game'' that consists of the average expected payoffs of
    choice mechanisms when playing arbitrary games (from a given class). In this sense, a
    meta-game captures (the modeller's) assumptions about the relevant statistic of the
    environment in which evolutionary forces operate.
  \end{enumerate}
\item To drive home our conceptual point that preference representations can matter, we try to
  isolate, as good as we can, the evolution of preferences from considerations of learning or
  other factors feeding a decision mechanism (such as the acquisition of ecologically useful
  representations (think: beliefs)). (Obviously, eventually, we would like to study the
  evolution of all of these relevant components in one swoop.)
\item To demonstrate how this approach can be theoretically rewarding, we show that, under
  reasonable additional assumptions, evolutionary selection can support non-veridical
  representations of payoffs in terms of regret.
  \begin{itemize}
  \item We argue that this ``meta-evolutionary'' approach could eventually help explain
    attested deviations of human decision making from the classical ideal: choice behavior
    evolved to be an ``ecologically rational'', near-optimal adaptation to the environment
    \citep{Anderson1990:The-Adaptive-Ch,Anderson1991:Is-human-cognit,GigerenzerGoldstein1996:Reasoning-the-F,ChaterOaksford2000:The-Rational-An}. This
    is inline with recent aspirations to provide an evolutionary rationale for decision making
    processes
    \citep[e.g.][]{HammersteinStevens2012:Six-Reasons-for,FawcettHamblin2013:Exposing-the-be}.
  \item Non-veridical regret-based representations of payoffs are intuitively appealing (we do,
    at least on occasion, care about counterfactual or hypothetical losses), but could also
    help explain, for instance, that decision-makers may violate the independence axiom: human
    and non-human decision makers can favor $A$ when presented with a choice
    between $A$ and $B$, but prefer $B$ when presented with $A$, $B$ and $C$ \myalert{(for
      animals cite from Fawcett page 3)} \myalert{(add
      citation for human performance)}
  \item We need to be careful here: we do not want to stress that this approach here (the
    particular implemenation of a regret-based choice mechanism) explains these empirical
    findings (better than any other line of explanation). Our point is more general and
    methodological. Evolution of preferences is something that the literature on evolution of
    choice mechanisms should care about. We also presented a method that can be useful and an
    example of that method that should (succinctly) demonstrate our aforementioned conceptual
    point that evolution of preferences is conceptually important and can give non-trivial,
    unexpected and philosophically useful results.
  \end{itemize}
\item contributes to: (i) methodological reflection on evolutionary game theory, (ii) ecological
  rationality of choice behavior, (iii) evolution of preferences literature
\end{itemize}

\bigskip

\begin{itemize}
\item The meta-game perspective that we introduce in this paper can shed more light on the duality of descriptivism and normativism, and opens
some space in between the two extrema. 
By keeping the evolutionary approach we want to retain a
normative perspective on rationality, but our results show that (under certain conditions)
evolution does not necessarily lead to identifying rationality with maximization of objective
utility, contrary to the intuition of Alchian [cite] and Friedman [cite].

In the following we mainly deal with two types of agents: players whose subjective utility is defined
in terms of regret (as in DEFINITION), and players whose utility
function coincides with the objective utility $\pi_i$. 

The justification for this choice is both historical and behavioral. Among
the possible competitors of utility maximization, regret has always been in a prominent
position, both from a descriptive and normative viewpoint. Indeed, Bell82 writes:
\begin{quote}
  By explicitly incorporating regret, expected utility theory not only becomes a better
  descriptive predictor but also may become a more convincing guide for prescribing behavior to
  decision makers.
\end{quote}
In the same year, Graham Loomes and Robert Sugden \citet{LoomesSugden1982:Regret-Theory:-} show that a regret-based theory of choice can match
and explain all the main experimental results of prospect theory, and claim that
\begin{quote} [...] we shall challenge the idea that the conventional axioms [of decision
  theory] constitute the only acceptable basis for rational choice under uncertainty. We shall
  argue that it is no less rational to act in accordance with regret theory, and that
  conventional expected utility theory therefore represents an unnecessarily restrictive notion
  of rationality. \hfill \citep{LoomesSugden1982:Regret-Theory:-}
\end{quote}
\end{itemize}


\end{document}



