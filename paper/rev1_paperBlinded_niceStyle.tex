\documentclass[fleqn,reqno,11pt]{article}

%========================================
% Packages
%========================================

\usepackage{etex}



\RequirePackage{amsmath}            % Formeln
\RequirePackage{amsfonts}           % Fonts for Formulas
\RequirePackage{amssymb}
\RequirePackage{amsthm}
\RequirePackage{dsfont}             % double stroke fonts
\RequirePackage{graphicx}
\RequirePackage{booktabs}
\RequirePackage{enumerate}
\RequirePackage{paralist}
\RequirePackage[all]{xy}            
\RequirePackage{url}

\RequirePackage{txfonts} % for strict implication symbols
\RequirePackage{soul}
\RequirePackage{relsize} % provides command \relsize{+/-x} for relative
                     % font size changes
\RequirePackage[german,english]{babel}
\RequirePackage[utf8]{inputenc}
% \RequirePackage[T1]{fontenc} 
\RequirePackage{xypic}
\RequirePackage{multicol}
\RequirePackage{subcaption}
\RequirePackage{tikz}
\usetikzlibrary{arrows,shapes,automata,backgrounds,petri,fit,decorations.pathmorphing}
\RequirePackage{units}

\RequirePackage{dialogue}

\RequirePackage{setspace}

\RequirePackage[colorinlistoftodos,color=lightgray,bordercolor=blue,textsize=footnotesize]{todonotes}

\RequirePackage{xspace} % for \xspace in definition of acronyms etc.


\RequirePackage[final,            % override "draft" which means "no nothing"
            colorlinks,       % rather than outlining them in boxes
            linkcolor=black,   % override truly awful colour choices
            citecolor=black,   %   (ditto)
            urlcolor=black,    %   (ditto)
            plainpages=false, % to overcome complaints with multiple
            pdfpagelabels,    % multiple page 1-s due to preface
            hypertexnames=false % solves warning, but interferes with
                                % index and \autoref apparently
            ]{hyperref}


\usepackage[backend=bibtex,natbib=true,style=authoryear-comp,backend=bibtex,doi=false,url=false]{biblatex}

\bibliography{choicePrince,biblio}

\newtheoremstyle{Satz}
   {}                      %Space above
   {1em}                  %Space below
   {\normalfont}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\bfseries\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')

\theoremstyle{Satz}
\newtheorem{theorem}{Theorem} 
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{fact}{Fact}
\newtheorem{claim}{Claim} 
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}{Exercise} 
\newtheorem{problem}{Problem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\newtheoremstyle{Bsp}
   {}                      %Space above
   {1em}                  %Space below
   {\itshape}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')
\theoremstyle{Bsp}
% \newtheorem{example}[theorem]{Example}


% Math --------------------
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\tuple}[1]{\left \langle #1\right\rangle}
\newcommand{\card}[1]{\left \lvert \, #1 \, \right\rvert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\setbar}{\ensuremath{\thinspace \mid \thinspace}}
\newcommand{\probbar}{\ensuremath{\mid}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\df}{\rightarrow}
\newcommand{\es}{\emptyset}
\newcommand{\den}[1]{\left [\! \left [ #1 \right ]\! \right]}
% \newcommand{\den}[1]{\left \llbracket #1  \right \rrbracket} % this would use fourier package which makes 'cases' environment bad
\newcommand{\no}{\noindent}
\newcommand{\hin}{"$\Rightarrow$" }
\newcommand{\rueck}{"$\Leftarrow$" }
\newcommand{\exs}{\vspace{.15cm}}
\newcommand{\pow}[1]{\ensuremath{\mathcal{P}(#1)}}	% Powerset
\newcommand{\restr}{{\restriction}}
\newcommand{\implicates}{\ensuremath{\leadsto}}  % arrow for
                                % implicatures in examples
\newcommand{\update}[2]{\ensuremath{#1[#2]}}
\newcommand{\myts}{\ensuremath{\thinspace}}
\newcommand{\mycolon}{\ensuremath{\thinspace \colon \thinspace}}
\newcommand{\mydot}{\ensuremath{\thinspace . \thinspace}}


\makeatletter
\newcommand{\prob}{\@ifstar
  \simpleprob%
  \condprob%
}
\def\simpleprob(#1){\ensuremath{\Pr(#1)}}
\def\condprob(#1|#2){\ensuremath{\Pr(#1 \,|\, #2)}}
\makeatother

% General Text Markup--------------------
\newcommand{\runex}[1]{\begin{center}#1\end{center}}
\newcommand{\mydef}[1]{\textsc{#1}}  		% definitions
\newcommand{\markdef}[1]{\textsc{#1}}  		% definitions; alternative
\newcommand{\myment}[1]{\emph{#1}}  	% first mentions
\newcommand{\myword}[1]{\textbf{\texttt{#1}}}	% refering to the word
\newcommand{\myemph}[1]{\emph{#1}}			% emphasis
\newenvironment{exnonum}{
  \begin{list}{}{
    \setlength{\leftmargin}{2.5em}
    \setlength{\rightmargin}{2.5em}
    %\setlength{\itemindent}{-1.5em}
  }
}{
  \end{list}
}


% Slanted Fractions
\newcommand{\myslantfrac}[2]{\msf{#1}{#2}}
\newcommand{\msf}[2]{\ensuremath{\nicefrac{#1}{#2}}}
\newcommand{\msftext}[2]{\nicefrac{#1}{#2}}


% Symbols for Conditionals
\newcommand{\cond}{\ensuremath{>}}
\newcommand{\bicond}{\ensuremath{\Leftrightarrow}}
\newcommand{\strcondWill}{\ensuremath{\boxRight}}
\newcommand{\strcondMight}{\ensuremath{\DiamondRight}}

% Signaling Games & IBR
\newcommand{\sen}{\ensuremath{S}\xspace}		% Sender variable
\newcommand{\mysen}[1]{\ensuremath{\sen^{#1}}} % Sender of type XYZ
\newcommand{\rec}{\ensuremath{R}\xspace}		% Receiver variable
\newcommand{\myrec}[1]{\ensuremath{\rec_{#1}}} % Receiver of type XYZ
\newcommand{\States}{\ensuremath{T}\xspace}		% Set of States
\newcommand{\state}{\ensuremath{t}\xspace}		% single states
\newcommand{\mystate}[1]{\ensuremath{\state_{\text{#1}}}\xspace} %meaningful states
\newcommand{\Messgs}{\ensuremath{M}\xspace}		% Set of Messages
\newcommand{\messg}{\ensuremath{m}\xspace}		% single messages
\newcommand{\mymessg}[1]{\ensuremath{\messg_{\text{#1}}}\xspace} %meaningful messages
\newcommand{\cost}{\ensuremath\operatorname{C}} % cost function
\newcommand{\Acts}{\ensuremath{A}\xspace}		% Set of R-actions
\newcommand{\act}{\ensuremath{a}\xspace}		% single action
\newcommand{\myact}[1]{\ensuremath{\act_{\text{#1}}}\xspace} %meaningful
\newcommand{\Worlds}{\ensuremath{W}}		% Worlds
\newcommand{\world}{\ensuremath{w}}		% single world
\newcommand{\myworld}[1]{\ensuremath{\world_{\text{#1}}}} %named world
\newcommand{\util}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\Util}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\utils}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\Utils}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\RealUtil}{\ensuremath{\operatorname{V}}}	% material payoffs
\newcommand{\Sstrat}{\ensuremath{\sigma}} % Behav/Probab Sender strategy
\newcommand{\Sstrats}{\ensuremath{\mathcal{S}}}	% Set of S-strategies
\newcommand{\Spure}{\ensuremath{s}} % Pure sender strategy
\newcommand{\Spures}{\ensuremath{\mathsf{S}}} % Set of pure sen strategies
\newcommand{\Smixed}{\ensuremath{\tilde{s}}} % Mixed sender strategy
\newcommand{\Smixeds}{\ensuremath{\Delta(\Messgs^\States)}}
\newcommand{\SpuresW}{\ensuremath{\mathsf{S}}} 
\newcommand{\SpuresS}{\ensuremath{\mathsf{S}^{{\mathrm{S}}}}}
\newcommand{\Rstrat}{\ensuremath{\rho}} % Behav/Probab Receiver strategy
\newcommand{\Rstrats}{\ensuremath{\mathcal{R}}}	% Set of R-Strategies
\newcommand{\Rpure}{\ensuremath{r}} % Pure receiver strategy
\newcommand{\Rpures}{\ensuremath{\mathsf{R}}} % Set of pure rec strategies
\newcommand{\Rmixed}{\ensuremath{\tilde{r}}} % Mixed receiver strategy
\newcommand{\Rmixeds}{\ensuremath{\Delta(\Acts^\Messgs)}} 
\newcommand{\RpuresW}{\ensuremath{\mathsf{R}}} 
\newcommand{\RpuresS}{\ensuremath{\mathsf{R}^{\mathrm{S}}}} 
\newcommand{\PureBR}{\ensuremath{\operatorname{BR}}} % Set of pure best responses
\newcommand{\ProbBR}{\ensuremath{\operatorname{BR_{Prob}}}} % Set of mixed best responses
\newcommand{\bel}{\ensuremath{\pi}}
\newcommand{\Bels}{\ensuremath{\Pi}}
\newcommand{\Sbel}{\ensuremath{\pi_{\sen}}}
\newcommand{\Sbels}{\ensuremath{\Pi_{\sen}}}
\newcommand{\Rbel}{\ensuremath{\pi_{\rec}}}
\newcommand{\Rbels}{\ensuremath{\Pi_{\rec}}}
\newcommand{\EU}{\ensuremath{\operatorname{EU}}} % Expected Utility
\newcommand{\EV}{\ensuremath{\operatorname{EV}}} % Expected Response Utility
\newcommand{\BR}{\ensuremath{\operatorname{BR}}} % Best Response
\newcommand{\QR}{\ensuremath{\operatorname{QR}}} % Quantal Response
\newcommand{\WBR}{\ensuremath{\text{{\relsize{-1}W}BR}}} % Weak Best Response
\newcommand{\SBR}{\ensuremath{\text{{\relsize{-1}W}BR}}} % Strong Best Response
\newcommand{\interpr}{\ensuremath{\delta}} % Interpretation strategy

% OT Stuff
\newcommand{\Gen}{\ensuremath{\operatorname{Gen}}} % Generator
\newcommand{\Eval}{\ensuremath{\operatorname{Eval}}} % Evaluator
\newcommand{\Con}{\ensuremath{\operatorname{Con}}} % Constraints
\newcommand{\metsuc}{\ensuremath{\succ}} % Symbol for BiOT metric
\newcommand{\metsuceq}{\ensuremath{\succeq}} % Symbol for BiOT metric
\newcommand{\Go}[1]{\operatorname{Pool}_{#1}}
\newcommand{\Oo}[1]{\operatorname{Opt}_{#1}}
\newcommand{\Bo}[1]{\operatorname{Blo}_{#1}}
\newcommand{\Gr}[1]{\operatorname{GAM^{\rho}}_{#1}}
\newcommand{\Or}[1]{\operatorname{OPT^{\rho}}_{#1}}
\newcommand{\Br}[1]{\operatorname{BLO^{\rho}}_{#1}}

% Abbreviations/Acronyms:
\newcommand{\acro}[1]{\textsc{#1}\xspace}
\newcommand{\acros}[1]{\textsc{#1}{\relsize{-1}s}\xspace}
\newcommand{\bc}{\acro{bc}} % Biscuit Conditional(s)
\newcommand{\bcs}{\acros{bc}}
\newcommand{\cbc}{\acro{cbc}} % Counterfactual BCs
\newcommand{\cbcs}{\acros{cbc}}
\newcommand{\ibr}{\acro{ibr}} % IBR model
\newcommand{\iqr}{\acro{iqr}} % IQR model
\newcommand{\rsa}{\acro{rsa}} % RSA model
\newcommand{\ot}{\acro{ot}} % BiOT
\newcommand{\biot}{\acro{b{\relsize{-1}i}ot}} % BiOT
\newcommand{\tom}{\acro{t{\relsize{-1}o}m}} % ToM
\newcommand{\fc}{\acro{fc}} % Free Choice
\newcommand{\cp}{\acro{cp}} % Conditional Perfection
\newcommand{\uc}{\acro{uc}} % Unconditional Readings
\newcommand{\pbe}{\acro{pbe}}   % Perfect Bayesian Equil.
\newcommand{\pbes}{\acros{pbe}}
\newcommand{\forind}{\acro{fi}} % forward induction
\newcommand{\tcp}{\acro{tcp}} % truth ceteris paribus
\newcommand{\cmr}{\acro{cmr}} % credible message rationalizability
\newcommand{\cm}{\acro{cm}} % credible message (profile) (Rabin)
\newcommand{\condition}[2]{\acro{#1}{#2}} % conditions 
\newcommand{\br}{\acro{br}} % best response (property)
\newcommand{\wbr}{\acro{{\relsize{-1}w}br}} % weak best response (property)
\newcommand{\sbr}{\acro{{\relsize{-1}s}br}} % strong best response (property)
\newcommand{\curb}{\acro{curb}} % curb sets
\newcommand{\gtp}{\acro{gtp}} % game theoretic pragmatics
\newcommand{\sda}{\acro{sda}} % simplification of disjunctive antecedents
\newcommand{\decprob}{\ensuremath{\mathcal{D}}}
\newcommand{\ques}{\ensuremath{\mathfrak{Q}}}
\newcommand{\vsi}{\acro{vsi}}
\newcommand{\evsi}{\acro{evsi}}
\newcommand{\uv}{\acro{uv}}
\newcommand{\qud}{\acro{qud}}
\newcommand{\NE}{\acro{ne}}
\newcommand{\NEs}{\acros{ne}}
\newcommand{\SNE}{\acro{sne}}
\newcommand{\SNEs}{\acros{sne}}
\newcommand{\SG}{\acro{sg}}
\newcommand{\SGs}{\acros{sg}}
\newcommand{\KO}{\textsc{ko\relsize{-1}bs}} % Kennedy's observation
\newcommand{\EVP}{\acro{evp}}    % extreme-value principle
\newcommand{\illc}{\acro{illc}}

% Evolution
\newcommand{\EGT}{\acro{egt}}
\newcommand{\ESS}{\acro{ess}}
\newcommand{\ESSs}{\acros{ess}}
\newcommand{\NSS}{\acro{nss}}
\newcommand{\NSSs}{\acros{nss}}
\newcommand{\sigsys}{\textsc{SigSys}\xspace} % signaling system
\newcommand{\sigsyss}{\textsc{SigSys{\relsize{-1}s}}\xspace} % signaling system Plural

\newcommand{\fin}{\rule{0mm}{1mm}\hfill{\rule{1.5cm}{0.2pt}}}

%Properly typeset tilde for URLs
\def\urltilde{\kern -.15em\lower .7ex\hbox{\~{}}\kern .04em}


% Beamer footnote for references:
\newcommand{\beamfn}[1]{
  \vfill
      \begin{footnotesize}
        \leftskip 0.1in
        \parindent -0.1in
       \hspace{-0.3cm}\rule{2cm}{0.01cm}\\ \vspace{-0.15cm}
        #1
      \end{footnotesize}
}

\newcommand{\myvec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\transpose}[1]{\ensuremath{\operatorname{T}(#1)}}
\newcommand{\normalize}[1]{\ensuremath{\operatorname{N}(#1)}}

\newcommand{\dn}[1]{\draftnote{#1}}
\newcommand{\fn}[1]{\footnote{#1}}

\newcommand{\stateunmarked}{\ensuremath{\state}\xspace}
\newcommand{\statemarked}{\ensuremath{\state^*}\xspace}
\newcommand{\messgunmarked}{\ensuremath{\messg}\xspace}
\newcommand{\messgmarked}{\ensuremath{\messg^*}\xspace}
\newcommand{\actunmarked}{\ensuremath{\act}\xspace}
\newcommand{\actmarked}{\ensuremath{\act^*}\xspace}

\newcommand{\sunmarked}{\ensuremath{\state}\xspace}
\newcommand{\smarked}{\ensuremath{\state^*}\xspace}
\newcommand{\munmarked}{\ensuremath{\messg}\xspace}
\newcommand{\mmarked}{\ensuremath{\messg^*}\xspace}
\newcommand{\aunmarked}{\ensuremath{\act}\xspace}
\newcommand{\amarked}{\ensuremath{\act^*}\xspace}

\newcommand{\ssome}{\mystate{\exists\neg\forall}}
\newcommand{\sall}{\mystate{\forall}}
\newcommand{\msome}{\mymessg{some}}
\newcommand{\mall}{\mymessg{all}}
\newcommand{\asome}{\myact{\exists\neg\forall}}
\newcommand{\aall}{\myact{\forall}}

% for repeating examples with gb4e

\newcounter{myexememory}
\newenvironment{exer}[1]
{
\setcounter{myexememory}{\value{exx}}
\setcounter{exx}{\getrefnumber{#1}}
\addtocounter{exx}{-1}  
\begin{exe}
}
{
\end{exe}
\setcounter{exx}{\value{myexememory}}
}

\newenvironment{nakedlist}{
  \begin{list}{\quad}{}
}
{
  \end{list}
}

\DefineNamedColor{named}{mycol}{cmyk}{0.6,0.6,0,0}
\DefineNamedColor{named}{mygray}{cmyk}{0.05,0.05,0.05,0.05}
\DefineNamedColor{named}{mygraylight}{cmyk}{0.017,0.017,0.017,0.017}
\DefineNamedColor{named}{mycol2}{cmyk}{0.8,0,0.8,0.2}

\newcommand{\mymark}[1]{{\color{mycol}{#1}}}

\DeclareMathOperator{\expo}{exp}

\newcommand{\mygray}[1]{{\textcolor{gray}{#1}}}
\newcommand{\mycol}[1]{{\textcolor{mycol}{#1}}}
\newcommand{\mycolh}[1]{{\textcolor{mycol2}{#1}}}


% \usepackage{calc}
\newsavebox\CBox
\newcommand\msout[2][0.5pt]{%
  \ifmmode\sbox\CBox{$#2$}\else\sbox\CBox{#2}\fi%
  \makebox[0pt][l]{\usebox\CBox}%  
  \rule[0.5\ht\CBox-#1/2]{\wd\CBox}{#1}}

\newcommand{\greensquare}{\raisebox{1.5pt}{\textcolor{Green}{\Large{\ensuremath{\blacksquare}}}}}
\newcommand{\bluecircle}{\textcolor{blue}{\Huge{\ensuremath{\bullet}}}}
\newcommand{\greencircle}{\textcolor{Green}{\Huge{\ensuremath{\bullet}}}}

\newcommand{\greensquareS}{\raisebox{1.5pt}{\textcolor{Green}{\normalsize{\ensuremath{\blacksquare}}}}}
\newcommand{\greensquareSS}{\raisebox{1.5pt}{\textcolor{Green}{\footnotesize{\ensuremath{\blacksquare}}}}}
\newcommand{\bluecircleS}{\textcolor{blue}{\Large{\ensuremath{\bullet}}}}
\newcommand{\greencircleS}{\textcolor{Green}{\Large{\ensuremath{\bullet}}}}

\newcommand{\soc}{\ensuremath{\theta}\xspace}

\usepackage{todonotes}


\usetikzlibrary{positioning,arrows,calc,fit}
%========================================
% Standard Layout
%========================================

% \usepackage{bigints}
% \usepackage{MnSymbol}

\usepackage{titlesec}

\usepackage{geometry}
 % \geometry{
 % a4paper,
 % left=300mm,
 % right=250mm,
 % top=400mm,
 % bottom=400mm
 % }

% \pagenumbering{gobble} 
% % left justification:
% \raggedright

%\usepackage{authblk}

% misc
\usepackage{babel}
\usepackage{amsmath}

\makeatletter
\makeatother

% Customisations specific to Philosophy of Science:

% paper size, margins, etc:



% Itemize
\renewcommand{\labelitemi}{\large{$\mathbf{\cdot}$}}    % itemize symbols
\renewcommand{\labelitemii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiv}{\large{$\mathbf{\cdot}$}}
% Description
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\textsc{#1}}

% \renewcommand{\footnotesize}{\normalsize}

\usepackage{setspace} % load setspace before footmisc

\usepackage{footmisc}
\setlength{\footnotesep}{\baselineskip}

\usepackage{setspace}
% \AtBeginCaption{\doublespacing} 

% \renewcommand{\footnotelayout}{\doublespacing}


% Figure Captions
\usepackage{caption} % use corresponding myfiguresize!
\setlength{\captionmargin}{20pt}
\renewcommand{\captionfont}{\normalsize}
\setlength{\belowcaptionskip}{7pt} % standard is 0pt

\newcommand{\myalert}[1]{\textcolor{red}{#1}}

\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{calc}

\definecolor{Red}{RGB}{178,34,34}
\newcommand{\mf}[1]{\textcolor{Red}{[mf: #1]}} 

\title{Smart Representations: {R}ationality and Evolution in a Richer  Environment}
\author{Paolo Galeazzi and Michael Franke}

\date{}

%\titleformat{\subsection}
%       {\normalfont\fontfamily{phv}\fontsize{12}{17}\itshape}{\thesubsection}{1em}{}
     
     
\usepackage{titlesec}
\titleformat*{\section}{\bf}
\titleformat*{\subsection}{\it}
\titleformat*{\subsubsection}{\it}

% indentation
% \setlength\parindent{12pt}

% \usepackage[osf]{palatino}
% \usepackage[T1]{fontenc}

\usepackage{fontspec,xltxtra}
\setromanfont[Mapping=tex-text]{Baskerville}
% \fontspec[Mapping=tex-text, Ligatures={Common, Rare, Historic}]{Palatino}

\begin{document}

\maketitle

\begin{abstract}
  \normalsize Standard applications of evolutionary game theory look at a single, fixed game
  and focus on the evolution of behavior for that game alone. Instead, this paper uses tools
  from evolutionary game theory to study the evolutionary competition between \emph{choice
    mechanisms} in a rich and variable multi-game environment. A choice mechanism is a way of subjectively
  representing a decision situation, paired with a method for choosing an act based on this
  subjective representation. We demonstrate the usefulness of this approach by a case study
  that shows how subjective representations in terms of regret that differ from the
  actual fitness can be evolutionarily advantageous.
\end{abstract}

\section{Introduction}
\label{sec:intr--motiv}

% \begin{quotation} \textit{This is what I aim at, because the point of philosophy is to start with something so simple as not to seem worth stating, and to end with something so paradoxical that no one will believe it.} \citep{Russ18} \end{quotation}

%Evolutionary game theory has become an established philosophical tool for probing into
%conceptual issues whose complexity requires mathematical modelling at a high level of
%abstraction.  Abstraction entails simplifying assumptions. 

If agents deal with a rich and variable environment, they have to face many different choice
situations.  Standard evolutionary game models frequently simplify reality in at least two
ways. Firstly, the environment is represented as a fixed \emph{stage game}; secondly, the focus
of evolutionary selection is behavior for that stage game alone. In contrast, some argue for
studying the evolutionary competition of general \emph{choice mechanisms} in a rich and
variable environment
\citep[e.g.,][]{FawcettHamblin2013:Exposing-the-be,McNamara2013:Towards-a-Riche,HammStev12}. In
response to this and adding to recent like-minded approaches, this paper introduces a general
\emph{meta-game} model that conservatively extends the scope of evolutionary game theory to
deal with evolutionary selection of choice mechanisms in variable environments \citep[see
also][]{Harley1981:Learning-the-Ev,ZollmanSmead2010:Plasticity-and-,SmeadZollman2013:The-Stability-o,OConnor2016:Evolving-to-Gen,RayoBecker07,Zollman2008,SkyrmsZollman10,BednarPage07}.\footnote{Some
  of these contributions are very closely related to ours. \citet{BednarPage07} use a
  multi-game framework, composed of a fixed selection of six possible games, to study the
  emergence of different cultural behaviors, and model agents as finite-state automata playing
  games from the fixed selection. \citet{Zollman2008} explains seemingly ``irrational'' fair
  behavior in social dilemmas (like the Ultimatum game) by means of a model where agents have
  to play the Ultimatum game together with the Nash bargaining game, but they are constrained
  to choose the same strategy for both games. Finally, \citet{RayoBecker07} consider, in a more
  decision-theoretic setting, what subjective utility function a cognitively limited agent
  should be endowed with in order to maximize her evolutionary fitness. Our framework can then
  be viewed as a generalization of those models, mainly in that here players do not necessarily
  have any specific cognitive limitations, and we allow for larger and possibly variable
  classes of games.}

A choice mechanism associates decision situations with action choices. A crucial part of a
choice mechanism is the \emph{subjective representation} of the decision situation, in
particular the manner of forming preferences and beliefs about a possibly uncertain world. To
show the usefulness of the meta-game approach, this paper asks: which preference and belief
representations are ecologically valuable and lead to high fitness? The evolution of
preferences has been subject of recent interest in theoretical economics
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. Here, we argue that questions of preference
evolution should take variability in uncertainty representation into account as well. We
demonstrate that if agents have \emph{imprecise} probabilistic beliefs
\citep[e.g.,][]{gardsah82,levi74,walley96}, faithful and objective representations in terms of
true evolutionary fitness can be outperformed by subjective (e.g., regret-based)
preference representations that deviate from the true fitness that natural selection operates
on.

% To show this, we study the evolutionary competition of choice mechanisms, or more
% specifically the competition of subjective representations of choice situations, in a
% variable environment. Concretely, we look at evolutionary ``meta games'' whose ``acts'' are
% choice mechanisms and whose ``payoffs'' represent the average expected fitness that different
% choice mechanisms would accrue when playing arbitrary games (from a given class, with a given
% occurrence probability). In this sense, a meta game captures (the modeller's) assumptions
% about the relevant statistics of the environment in which evolutionary forces operate, while
% we are still able to use standard methods from evolutionary game theory, like stability or
% evolutionary dynamics.

The paper is organized as follows. Section \ref{sec:rati--subj} sets the scene by reviewing
different perspectives on rational choice. Section~\ref{sec:subj-ut,bel,dec-rules} introduces
the meta-game approach. In doing so, it covers key notions such as choice mechanisms, decision
rules and subjective representations, all with an eye towards the evolutionary application of
Section~\ref{sec:model}. Section~\ref{sec:results} contains the main results for that
application, and Section \ref{sec:extensions} discusses some interesting extensions. Finally,
Section~\ref{sec:conclusion} concludes.

\section{Rationality and Subjective Representations}
\label{sec:rati--subj}

The standard textbook definition of \textit{rationality} in economics and decision theory
traces back to the seminal work by \citet{deFinetti37}, \citet{Neumannvon-NeumannMorgenstern1944:Theory-of-Games}
and \citet{Savage1954:The-Foundations}. It says that a choice is rational only if it maximizes
(subjective) expected utility.


\noindent Expected utility is subjective in the sense that it is a function of subjective
beliefs and subjective preferences of the decision maker (DM). To wit, a choice can be
rational, i.e., the best choice from the DM's point of view, even if based on peculiar beliefs
and/or aberrant preferences. % (Indeed, while
% \citeauthor{Neumannvon-NeumannMorgenstern1944:Theory-of-Games} assumed that probabilities were
% objectively given, it was \citet{Savage1954:The-Foundations} who famously extended von Neumann
% and Morgenstern's axiomatization to include a subjective notion of probabilistic beliefs on top
% of the subjective notion of preference that von Neumann and Morgenstern started out with.)

If beliefs and preferences are subjective, there is room for \emph{rationalization} or
\emph{redescriptionism} of observable behavior. For example,
% \citet{KahnemannTversky1979:Prospect-Theory} famously demonstrated that what appear to be
% violations of rationality norms in human decision making can be explained on the assumption
% that subjects' beliefs and preferences deviate systematically from the objectively given
% parameters in the presented choice task. Similarly
in the case of social decision making, including considerations of fairness allows us to
describe as rational empirically observed behavior, such as in experimental Prisoner's Dilemmas
or public goods games, that might otherwise appear irrational
\citep[e.g.,][]{fehrschmidt99,charrab02}.

The main objection to redescriptionism is that, without additional constraints, the notion of
rationality is likely to collapse, as it seems possible to deem rational almost everything that
is observed, given the freedom to adjust beliefs and preferences at will. \emph{Normativism}
therefore emphasizes that there are many ways in which ascriptions of beliefs and preferences
should be constrained by normative considerations of rationality as well: e.g., subjective
beliefs should reflect objective chance where possible; subjective preferences should be
oriented towards tracking objective fitness. For instance, profit maximization seems a
necessary requirement for evolution in a competitive market because only firms behaving
according to profit maximization will survive in the long run \citep[e.g.,][]{alch50,Fried53}.

An alternative view on rationality of choice is \emph{adaptationism}
\citep[e.g.,][]{Anderson1991:Is-human-cognit,ChaterOaksford2000:The-Rational-An,HagenChater2012:Decision-Making}. Adaptationism
aims to explain rational behavior by appealing to evolutionary considerations: DMs have
acquired choice mechanisms that have proved to be adaptive with respect to the variable
environment where they have evolved. A choice mechanism can be a set of distinct heuristics
(the DM's \emph{adaptive toolbox}) that have little in common
\citep[e.g.,][]{TverskyKahnemann1981:The-Framing-of-,GigerenzerGoldstein1996:Reasoning-the-F,ScheibehenneRieskamp2013:Testing-the-Ada}. But
to closely relate to the literature on evolution of preferences and to the philosophical debate
about the nature of rational choice, we here suggest to think of a choice mechanism as a map
from choice situations to action choices which includes an explicit level of subjective
representation of the situation. Specifically, a \emph{subjective representation} is a general
way of forming preferences and beliefs about the choice situation. We are most interested in
the question which subjective representations, and which choice mechanisms in general, are
better than others from an evolutionary point of view.


\section{Choice Mechanisms and Meta-Games} 
\label{sec:subj-ut,bel,dec-rules}


We view a choice mechanism as the combination of three different things: a \textit{subjective
  utility} (or preference), a \textit{subjective belief} and a \textit{decision rule}. In
general, the agent's action choice will depend both on the agent's utility at different
possible outcomes of the choice situation and on the agent's beliefs about the realization of
these outcomes. The decision rule then combines the agent's subjective utility and belief, and
dictates how the agent should act: a decision rule is a function that associates an action
choice with the agent's utility and beliefs:
$$ \text{Decision Rule: Utility }\times\text{ Beliefs } \rightarrow \text{ Actions.}  $$
The subjective utility of an agent can be formally expressed by a function
$u:W \times A \rightarrow \mathbb{R} $, where $A$ stands for a (finite) set of actions
available to the agent and $W$ is a (finite) set of possible states of the world. There are
many different ways to describe beliefs, but for concreteness of later applications we here
assume that the agent's beliefs are represented in terms of a (possibly singleton) convex compact set of
probability functions $\Gamma \subseteq \Delta(W) $ over the possible states of the
world. Given a utility $u$ and a belief $\Gamma$, examples of well-known decision rules from the literature that we will encounter later are:
\begin{enumerate}

\item \textbf{Maxmin}: $$ a^*(u,\Gamma)= \argmax_{a \in A} \min_{\mu \in \Gamma} \sum_{w\in W} u(w,a)  \mu(w)$$

\item \textbf{Maximax}: $$ a^*(u,\Gamma)= \argmax_{a \in A} \max_{\mu \in \Gamma} \sum_{w\in W} u(w,a)  \mu(w)$$

\item \textbf{Laplace rule}: $$ a^*(u,\Gamma)= \argmax_{a \in A} \sum_{w\in W} \frac{1}{|W|}  u(w,a) $$

\item \textbf{Expected utility maximization} (for $\Gamma$ singleton): $$ a^*(u,\Gamma)= \argmax_{a \in A} \sum_{w\in W} u(w,a)  \mu(w)$$

\end{enumerate}

It is worth noticing that both maxmin and maximax boil down to expected utility maximization
when the set $\Gamma$ is a singleton, and in turn expected utility maximization reduces to the
Laplace rule when the belief $\mu$ is a uniform probability over the states.

As mentioned previously, for a choice mechanism to prescribe an action, the decision rule needs
to be given a specific utility $u$ and belief $\Gamma$ as input. We call the pair $(u,\Gamma)$
a \textit{subjective representation} of the decision situation. In the following, we investigate
the evolutionary fitness of general and systematic ways of forming such subjective
representations across many different decision situations.

% The literature on evolution of preferences (REFERENCES) mainly focuses on the evolutionary
% selection of subjective utilities uniquely. Instead, it will be clear from our model that
% different subjective beliefs may have a fundamental role in determining the evolutionary
% success of subjective utilities, and therefore the interplay between utilities and beliefs
% needs to be taken into account explicitly.  So, here we are most interested in the question
% which subjective representations are better than others from an evolutionary point of
% view. In order to investigate this issue we are going to use the machinery of evolutionary
% game theory.

A \emph{fitness game} is an interactive decision situation.  For a given fitness game
$G=\langle N, (A_i,\pi^G_i)_{i \in N} \rangle$, let us denote the evolutionary payoff, or
\textit{fitness}, of player $i$ by the function
$\pi^G_i: \Pi_{i \in N} A_i \rightarrow \mathbb{R}$, where $A_i$ is player $i$'s (finite) set
of actions. For simplicity of exposition we assume that all games that are played are symmetric
two-player games where $N:=\{1,2\}$, $A_1 = A_2 $ and
$\pi^G_1(a_1, a'_2)= \pi^G_2(a'_1, a_2)=:\pi^G(a,a')$.\footnote{Since payoff functions are
  symmetric, we simply write $\pi^G(a, a')$ for $\pi^G_1(a_1, a'_2)$ and $A:=A_1=A_2$, as
  usual. However, notice that all definitions and results can be extended to more general
  cases.} The fitness of a choice mechanism $c$ with decision rule $a_{c}^{*}$ and subjective representation $(u_c,\Gamma_c)$ is
measured in terms of the expected evolutionary payoff of $c$.
Formally, the fitness of choice mechanism $c$ against choice mechanism $c'$ in a
symmetric two-player game $G=\langle \{1,2\}, A, \pi^G \rangle$ is given by:

$$
F_G(c,c')= \pi^G(a_c^*(u_c^G,\Gamma_c),a_{c'}^*(u_{c'}^G,\Gamma_{c'})). \footnote{Whenever a choice mechanism
  would not select a unique action, we assume that the player chooses one of the equally
  optimal actions at random. I.e., $F_G(c,c')=
  \sum_{a \in a_c^*(u_c^G,\Gamma_c)} \sum_{a' \in a_{c'}^*(u_{c'}^G,\Gamma_{c'})}
  \frac{1}{|a_c^*(u_c^G,\Gamma_c)|} \frac{1}{|a_{c'}^*(u_{c'}^G,\Gamma_{c'})|} \pi^G(a,a')$.}
$$
Given the game theoretic setting,
the subjective utility $u^{G}_c$ is now a function $u_c^G:A \times A \rightarrow \mathbb{R}$, and
the subjective belief $\Gamma_c$ is a set of probability functions over the co-player's
actions, $ \Gamma_c \subseteq \Delta(A)$.

Going beyond a single fixed fitness game, we consider a \textit{class} of possible games. For
concreteness, let $\mathcal{G}$ be a class of two-player symmetric games, together with a
probability measure $P_{\mathcal{G}}(G)$ for the occurrence probability of game
$G \in \mathcal{G}$. Intuitively, the probability $P_{\mathcal{G}}$ encodes the statistical
properties of the environment. A \textit{meta-game} is then a tuple
$ MG=\langle CM, \mathcal{G}, P_{\mathcal{G}},F \rangle$, where $CM$ is a set of choice
mechanisms, $\mathcal{G}$ is a class of possible games, $P_{\mathcal{G}}(G)$ is the probability
of game $G$ to occur, and $F:CM \times CM \rightarrow \mathbb{R}$ is the (meta-)fitness
function, defined as:
\begin{align}
  \label{eq:FittnessChoiceMechGamePairwise}
  F(c, c') = \int P_{\mathcal{G}}(G) \  F_G(c,c') \text{ d} G \,.
\end{align}
Hence, $F(c,c')$ determines the evolutionary payoff of choice mechanism $c$ against $c'$ in the
meta-game. The set $CM$ can be thought of as the set of choice mechanisms that are present within a given
population playing the games from the class $\mathcal{G}$. Consequently, it is possible
to compute the average fitness of $c$ against the population, that is given by:
\begin{align}
  \label{eq:FittnessChoiceMechGame}
  F(c) = \int P_{M}(c') \ F(c,c') \text{ d} c' = \int \int P_{\mathcal{G}}(G) \  P_{M}(c') \  F_G(c,c') \text{ d} c' \text{ d} G  \,,
\end{align}
where $P_{M}(c')$ is the probability of
encountering a co-player with choice mechanism $c'$.

Meta-games are then abstract models for the evolutionary competition between choice mechanisms
in interactive decision making contexts.  Standard notions of evolutionary game theory apply to
meta-games as well. For example, a choice mechanism $c$ is a strict Nash equilibrium if
$F(c,c) > F(c',c)$ for all $c'$; it is evolutionarily stable if for all $c'$ either (i)
$F(c,c) > F(c',c)$ or (ii) $F(c,c) = F(c',c)$ and $F(c,c') > F(c',c')$; it is neutrally stable
if for all $c'$ either (i) $F(c,c) > F(c',c)$ or (ii) $F(c,c) = F(c',c)$ and
$F(c,c') \ge F(c',c')$ \citep{Maynard-Smith1982:Evolution-and-t}. Similarly, evolutionary
dynamics can be applied to meta-games. Later we will also turn towards a dynamical analysis in terms of replicator dynamics
\citep{TaylorJonker1978:Evolutionary-St} and replicator mutator dynamics
\citep[e.g.,][]{Nowak2006:Evolutionary-Dy}.

\section{Evolution of Preferences} \label{sec:model}

To demonstrate the usefulness of a meta-game approach, we compare a
selection of general ways of forming belief and preference representations against each
other. As for subjective preferences, consider initially:
\begin{enumerate}
\item the \textit{objective} utility, defined by: for all $G \in \mathcal{G}$,
$$\text{obj}^G(a,a')=\pi^G(a,a');$$
\item the \textit{regret}, defined by: for all $G \in \mathcal{G}$,
$$\text{reg}^G(a,a') =\pi^G(a,a')- \max_{a''\in A} \pi^G(a'',a').$$
\end{enumerate}

As motivation for this comparison, it is to be stressed that regret minimization is one of the main alternatives to utility (or value) maximization in the literature on decision criteria (see also \cite{bleWakker15}).
For a start, the subjective beliefs that we take into consideration are also two:
\begin{enumerate}
\item $prc$: a precise uniform belief $\overline{\mu}$ such that $\overline{\mu}(a)=\frac{1}{|A|}$ for all $a\in A$;
\item $imp$: a maximally imprecise belief $\overline{\Gamma}=\Delta(A)$.
\end{enumerate}

Although a thorough discussion of this issue goes beyond the scope of this work, let us say
that these two kinds of belief underlie two different and alternative views on
uncertainty. Faced with uncertain events, a strict Bayesian will always form a precise belief,
specified by a \textit{single} probability $\mu$. In the absence of any information about
future uncertain events, the Bayesian would mostly invoke the \textit{principle of insufficient
  reason}, and accordingly choose a uniform probability over the possible outcomes. In
contrast, others have argued against the obligation of representing a belief by means of a
single probability measure, opposite to the Bayesian paradigm (e.g., \cite{GilbMarin13}). They
argue instead in favor of a more encompassing account, according to which uncertainty can be
\emph{unmeasurable}, and represented by a (convex and compact) set of probabilities (e.g.,
\cite{gilsch89}). This line of thought has its origin in decision theory, motivated by
Ellsberg's famous paradoxes \citep{ells61}, and appears extremely relevant in game-theoretic
contexts too. Indeed, in a recent paper \citet{BattCerrMM15} write:

\begin{quote}
  Such [unmeasurable] uncertainty is inherent in situations of strategic interaction. This is quite obvious
  when such situations have been faced only a few times. (p.~646)
\end{quote}

\noindent In evolutionary game theory, for instance, players obviously face uncertainty
about the composition of the population that they are part of, and consequently about the (type of)
co-player that they are randomly paired with at each round and about the co-player's action. In
case of complete lack of information about the composition of the population, a non-Bayesian player would thus entertain maximal
unmeasurable uncertainty, i.e., a maximally imprecise belief.\footnote{Such a radical
  uncertainty could ensue, for example, if agents have no conception of their co-player or her
  preferences. Unsophisticated agents, as considered in evolutionary game theory, might be
  entirely unaware of the fact that they are engaged in social decision making (see \cite{HeifetzMeier2009:Dynamic-Unaware}, for
  game-theoretic models of unawareness). It is therefore
  not ludicrous to consider radical uncertainty first and tend to more
  sophisticated ways of forming beliefs later (more on this below).} As already anticipated, we
will see that the way agents form beliefs, and the possibility of holding imprecise beliefs
in particular, can have a fundamental impact on their evolutionary success.

As for the decision rule, we assume that players use the maxmin rule. This is in line with many
representation results of decision making under unmeasurable uncertainty
\citep[e.g.,][]{gilsch89,GhirMar02}, and seems corroborated by empirical findings
too. Ellsberg's paradoxes are prominent examples \citep{ells61}, and evidence from experimental
literature suggests that agents are generally averse to unmeasurable uncertainty
\citep[e.g.,][]{TrautKuil16}.

Finally, note that when the maxmin rule acts on subjective representations of type (obj, imp),
i.e., objective preferences and imprecise beliefs, the generated behavior corresponds to the
classic maxmin strategy \citep{neumorg44}. When the maxmin rule acts on subjective
representation (reg, imp), the agent's behavior is known as \textit{regret
  minimization}.\footnote{The notion of regret in decision theory dates back at least to the
  work by \citet{Savage1951:The-theory-of-s}, and has later been developed by \citet{bell82},
  \citet{Fishburn1982} and \citet{loosug82} independently. Recently,
  \citet{HalpernPass2012:Iterated-Regret} showed how the use of regret minimization can give
  solutions to game-theoretic puzzles (like the Traveller's dilemma and the Centipede game) in
  a way that is closer to everyday intuition and empirical data. In this paper the notion of
  regret defined earlier is the same as in \citet{HalpernPass2012:Iterated-Regret}.} Two facts
follow from these observations.  The first is related to our focus on different types of
uncertainty that players may entertain.
\begin{fact} \label{fact:maxEU-minReg} 
%Under an identical precise (singleton set) belief, the
%  acts selected by maximization of expected evolutionary payoff in game $G$ are exactly the acts selected
%  by the maximization of expected utility for the negative regret transformation of $G$
  For any precise (Bayesian) belief $\mu$, maximization of expected (objective) utility based
  on $\mu$ and minimization of expected regret based on $\mu$ are behaviorally
  equivalent. %\citep[e.g.,][]{HalpernPass2012:Iterated-Regret}.
\end{fact}
\noindent The second fact highlights another behavioral equivalence, which we will make use of
shortly in the following section.
\begin{fact} \label{fact:equivalence2x2} In the class of $2 \times 2$ symmetric games, the acts
  selected by the Laplace rule are exactly the acts selected by regret minimization.
\end{fact} 

Here is a simple example that shows these choice mechanisms in action. Consider the
coordination fitness game $G$ depicted in figure \ref{coordgame1}. Since the
game is symmetric, it suffices to specify the evolutionary payoffs for the row player. Figure
\ref{coordgame1} also represents the objective utility $\text{obj}^G$, since
$\text{obj}^G= \pi^G$ by definition, whereas figure \ref{coordgame1reg} pictures the
representation of $G$ in terms of regret-based utilities. While classic maxmin is indifferent
between $I$ and $II$ (figure \ref{coordgame1}), regret minimization uniquely selects $II$
(figure \ref{coordgame1reg}).
\begin{figure}
\begin{center}
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 1 & 0 \\
      II & 0 & 2\\
      \bottomrule
    \end{tabular}
    \caption{Coordination game $G$.}
    \label{coordgame1}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 0 & -2 \\
      II & -1 & 0\\
      \bottomrule
    \end{tabular}
    \caption{Regret-based representation of $G$.}
    \label{coordgame1reg}
  \end{subfigure}
  \caption{A coordination game (left) and the associated regret representation (right).}
    \label{coordgame1mainFig}
    \end{center}
\end{figure}


\section{Results}\label{sec:results}

\subsection{Simulation Results}
\label{sec:simulation-results}

Since for now we keep the decision rule fixed to maxmin, a player's choice mechanism will only depend on the player's subjective representation $ (u,\Gamma) $. For brevity, from now on we will refer to the pair $ (u,\Gamma)$, like $(\text{reg}, \text{imp})$ or $(\text{obj}, \text{prc})$, as the \textit{type} of the player. Sometimes we will also distinguish types by referring to the subjective utility only, for instance $(\text{reg}, \text{imp})$ and $(\text{reg}, \text{prc})$ are regret types.

As observed earlier, meta-games factor in statistical properties of the environment. For particular
empirical purposes, one could consult a specific class of games $\mathcal{G}$ with appropriate,
maybe empirically informed probability $P_{\mathcal{G}}$ in order to match the natural
environment of a given population. For our present purposes, let $\mathcal{G}$ be a set of symmetric
two-player fitness games with two acts for a start. Each game $G \in \mathcal{G}$ is then individuated solely by its payoff
function, i.e., by a quadruple of numbers $G=(a,b,c,d)$. As for the occurrence probability
$P_{\mathcal{G}}(G)$ of game $G$, we imagine that the values $a,b,c,d$ are i.i.d.~random
variables sampled from the set $ \lbrace 0, \dots, 10 \rbrace$ according to uniform probability
$P_{V}$. Using Monte Carlo simulations, we can then approximate the values of
equation~(\ref{eq:FittnessChoiceMechGamePairwise}) to construct meta-game payoffs. Results
based on $100.000$ randomly sampled games are given in
table~\ref{tab:ExpectedFitness_4Types}.\footnote{Concretely, $100.000$ games were sampled
  repeatedly by choosing independently four integers between 0 and 10 uniformly at random. For
  each game, the action choices of all four choice mechanisms were determined and payoffs from
  all pairwise encounters recorded. The number in each cell of
  table~\ref{tab:ExpectedFitness_4Types} is the average payoff for the choice mechanism listed
  in the row when matched with the choice mechanism in the column.}

\begin{table}[t]
\centering
\begin{tabular}{ccccc}
  \toprule
 & $\text{reg}, \text{imp}$ 
 & $\text{obj}, \text{imp}$ 
 & $\text{reg}, \text{prc}$ 
 & $\text{obj}, \text{prc}$ \\ 
  \midrule
  $\text{reg}, \text{imp}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
  $\text{obj}, \text{imp}$ & 6.486 & 6.484 & 6.486 & 6.486 \\ 
  $\text{reg}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\  
  $\text{obj}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
   \bottomrule
\end{tabular}                    
\caption{Average evolutionary fitness from Monte Carlo simulations of 100.000 symmetric $2 \times 2$ games.}
\label{tab:ExpectedFitness_4Types}
\end{table}

Simulation results obviously reflect Fact~\ref{fact:equivalence2x2} in that all encounters in
which types $(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ or
$(\text{obj}, \text{prc})$ are substituted for one another yield identical results. More
interestingly, table~\ref{tab:ExpectedFitness_4Types} shows that $(\text{obj}, \text{imp})$,
the maxmin strategy, is strictly dominated by the three other types: in each column (i.e.,
for each type of co-player), the maxmin strategy is strictly worse than any of the three
competitors. This has a number of interesting consequences.

If we restrict attention to subjective representations with imprecise beliefs only, then a monomorphic state in which every agent has
regret-based preferences is the only \emph{evolutionarily stable state}. More strongly, since
$(\text{obj}, \text{imp})$ is strictly dominated by $(\text{reg}, \text{imp})$, we expect
selection that is driven by (expected) fitness to invariably weed out maxmin players $(\text{obj}, \text{imp})$ in favor of $(\text{reg}, \text{imp})$, regret minimization. 
In terms of choice rules, this means that regret
minimization is evolutionarily better than maxmin over the class of games considered. In terms of subjective preferences, it shows that players using the objective representation that directly looks at fitness (possibly money, or profit) are outperformed by non-veridical (regret) representations, when players' beliefs are imprecise.

Next, if we look at the competition between all four types represented in
table~\ref{tab:ExpectedFitness_4Types}, $(\text{reg}, \text{imp})$ is no longer evolutionarily
stable. Given behavioral equivalence (Fact~\ref{fact:equivalence2x2}), types
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$, and $(\text{obj}, \text{prc})$ are all
\emph{neutrally stable} \citep{Maynard-Smith1982:Evolution-and-t}. But since
$(\text{obj}, \text{imp})$ is strictly dominated and so disfavored by fitness-based selection,
we are still drawn to conclude that maxmin behavior is weeded out in favor of a population with
a random distribution of the remaining three types.

Simulation results of the (discrete time) \emph{replicator dynamics}
\citep{TaylorJonker1978:Evolutionary-St} indeed show that random initial population
configurations are attracted to states with only three player types:
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. The
relative proportions of these depend on the initial shares in the population. This variability fully disappears if
we add a small mutation rate to the dynamics. Take a fixed, small mutation rate $\epsilon$ for
the probability that a player's subjective utility \emph{or} her subjective belief changes to another
utility or belief. The probability that a player's subjective representation randomly mutates into a
completely different representation with altogether different utility and belief would
then be $\epsilon^2$. With these assumptions about ``component-wise mutations,'' numerical
simulations of the (discrete time) \emph{replicator mutator dynamics}
\citep{Nowak2006:Evolutionary-Dy} show that already for very small mutation rates almost all
initial population states converge to a single fixed point in which the majority of players have
regret-based utility. For instance, with $\epsilon = 0.001$, almost all initial populations are
attracted to a final distribution with proportions:

\begin{center}
  \begin{tabular}{cccc}
    $(\text{reg}, \text{imp})$ & $(\text{obj}, \text{imp})$ & $(\text{reg},
      \text{prc})$ & $(\text{obj}, \text{prc})$ \\ \hline
    0.289  & 0.021 &   0.398 &    0.289 
  \end{tabular}
\end{center}

What this suggests is that, if biological evolution selects behavior-generating mechanisms, not
behavior as such, it need not be the case that behaviorally equivalent mechanisms are treated
equally all the while. If mutation probabilities are a function of individual components, it can be the case that
certain components of such behavior-generating mechanisms are more strongly favored by a process of random mutation and
selection. This is exactly the case with regret-based preferences. Since regret-based preferences are much better in connection with imprecise
beliefs than veridical preferences are, the proportion of expected regret minimizers,
$(\text{reg}, \text{prc})$, in the attracting state is substantially higher than that of
expected utility maximizers, $(\text{obj}, \text{prc})$, even though these types are
behaviorally equivalent.



\subsection{Analytical Results}
\label{sec:analytical-results}

Results based on the single meta-game in table~\ref{tab:ExpectedFitness_4Types} are not fully general and possibly spoiled by random
fluctuations in the sampling procedure. Fortunately, for the case of $2 \times 2$ symmetric
games, the main result that maxmin types $(\text{obj}, \text{imp})$ are strictly dominated by regret minimizers can
also be shown analytically for considerably general conditions.

\begin{proposition} \label{proposition1}
  Let $\mathcal{G}$ be the class of $2 \times 2$ symmetric games $G=(a,b,c,d)$ generated by i.i.d.~sampling $a,b,c,d$ from a
  set of values with at least three elements in the support. Then,
  $(\text{reg}, \text{imp})$ strictly dominates $(\text{obj}, \text{imp})$ in the resulting
  meta-game.
\end{proposition}

\begin{proof}
All proofs are in Appendix~\ref{sec:proofs}.
\end{proof}

\begin{corollary} \label{corollary1} Let $\mathcal{G}$ be as in Proposition~\ref{proposition1}. If we only consider imprecise belief types, $(\text{obj}, \text{imp})$ and
  $(\text{reg}, \text{imp})$, then the unique evolutionarily stable state is a monomorphic population of
  $(\text{reg}, \text{imp})$ players.
\end{corollary}

%Obviously, $(\text{reg}, \text{imp})$,
%$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ players will be neutrally stable, as before. 
%and fitness-based selection will tend to weed out maxmin play and leave us with a population of
%arbitrary frequency of $(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
%$(\text{reg}, \text{prc})$ players. 
The result shows that there is support for the main conceptual
point that we wanted to make: 
%non-veridical representations \emph{can}, under
%specific circumstances, persist under evolutionary selection based on objective
%fitness. Moreover,
objective preference representations are not necessarily favored by natural selection;
objective preferences are outperformed by non-veridical regret preferences if agents
have imprecise beliefs.  This tells us that the main conclusions drawn in the previous section
based on the approximated meta-game of table~\ref{tab:ExpectedFitness_4Types} hold more
generally for arbitrary $2 \times 2$ symmetric games with i.i.d. sampled payoffs.

This result presupposes at least occasional imprecise beliefs. The assumed imprecise beliefs do not need to be maximally uncertain, however.  Let
the uncertainty held by a player be defined by a convex compact set of probabilities
$ [s,t] \subseteq \Delta(A) $ over the co-player's actions, where $s$ is the lower probability
and $t$ is the upper probability of action $II$.  We can then prove the following proposition,
which is the analogue of Proposition~\ref{proposition1} for any possible (not necessarily
maximal) degree of uncertainty $[s,t]$, with $s \neq t$. There is only one difference: we are
now going to require i.i.d. drawing of a \emph{continuous} random variable. This is due to the
fact that, for arbitrarily small intervals $[s,t]$, objective players $([s,t],\text{obj})$ and
regret players $([s,t],\text{reg})$ can behave as holding a unique probability measure (precise
belief) if the underlying payoff space is not dense. The reason for this technical requirement
will become clearer from the proof.

\begin{proposition} \label{proposition2} In the class of $2\times2$ symmetric games generated
  by i.i.d. drawing of a continuous random variable from a distribution with density $P_V$ and
  support $(\underline{r},\overline{r}) \subseteq \mathbb{R}$, for any imprecise
  belief $[s,t]$, the only evolutionarily stable state of a population with regret players
  $([s,t],\text{reg})$ and objective players $([s,t],\text{obj})$ is a monomorphic state of
  $([s,t],\text{reg})$ players.
\end{proposition}

This tells us that regret-based preferences can outperform objective preference
representations when agents are also capable of learning or otherwise restricting their assumptions
about the co-player's behavior as long as there is, at least on occasion, some imprecision in
their beliefs. We will enlarge on the issue of belief formation after having covered some more
relevant extensions in the next section.

\section{Extensions}
\label{sec:extensions}

How do the basic results from the previous section carry over to richer models?
Section~\ref{sec:more-types} first introduces further conceptually interesting subjective
representations that have been considered in the literature. Section~\ref{sec:n-times-n} then
addresses the case of symmetric two-player $n \times n$ games for $n \ge 2$. Finally,
Section~\ref{sec:solitary-decisions} ends with a brief comparison to the case of solitary
decision making.

\subsection{More Preference Types}
\label{sec:more-types}

The space of possible preference types is enormous, and we have only compared regret
and objective types so far. Let us now
look at two other types of subjective preferences that have been investigated, especially
in behavioral economics and in evolutionary game theory. A famous example is the
\textit{altruistic} preference \citep[e.g.,][]{Beck76,BestGuth98}, summoned to explain the
possibility of altruistic behavior. At the other end of the spectrum, the \emph{competitive}
preference is located. The two subjective utilities are defined as follows:
\begin{enumerate}
\item \textit{altruistic} utility: for all $G \in \mathcal{G}$, $\text{alt}^G(a,a') = \pi^G(a,a') + \pi^G(a',a)$;\footnote{A
    more general formulation would be to define $ \alpha$-altruistic utility, for
    $\alpha \in [0,1]$,
    $ u^G_\alpha(a, a')=\pi^G(a,a') + \alpha \pi^G(a',a)$. Since we are not
    interested in the evolution of degrees of altruism, here we simply fix $ \alpha = 1 $. Analogously for $\alpha$-competitive utilities too.}
\item \textit{competitive} utility: for all $G \in \mathcal{G}$, $\text{com}^G(a,a') = \pi^G(a,a') - \pi^G(a',a)$.
\end{enumerate}


\begin{table}[]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
  \hline
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{com}, \text{imp})$
 & $(\text{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\text{com}, \text{prc})$
 & $(\text{alt}, \text{prc})$ \\ 
  \hline
  $(\text{reg}, \text{imp})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{imp})$ & 6.486 & 6.484 & 6.088 & 6.703 & 6.486 & 6.486 & 6.088 & 6.875 \\
  $(\text{com}, \text{imp})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\text{alt}, \text{imp})$ & 5.949 & 5.722 & 5.326 & 6.396 & 5.949 & 5.949 & 5.326 & 6.568 \\
  $(\text{reg}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{com}, \text{prc})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\text{alt}, \text{prc})$ & 6.331 & 5.893 & 5.497 & 6.566 & 6.331 & 6.331 & 5.497 & 7.152 \\
   \hline                          
\end{tabular}    
}                  
\caption{Average evolutionary fitness from Monte Carlo simulations of 100.000 symmetric $2 \times 2$ games.}
\label{tab:ExpectedFitness_2x2_Full}        
\end{table}   
 
Table~\ref{tab:ExpectedFitness_2x2_Full} shows results of Monte Carlo simulations that
approximate the expected fitness in the relevant meta-game with all the subjective representations considered so
far. These results confirm basic intuitions about altruistic and competitive types: everybody would like to have an altruistic co-player and nobody
would like to play against a competitive player. Perhaps more surprisingly, $(\text{alt}, \text{imp})$
comes up strictly dominated by $(\text{com}, \text{imp})$, but competitive types themselves are worse off
against all types except against maxmin players $(\text{obj}, \text{imp})$ than any of
the behaviorally equivalent types $(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
$(\text{reg}, \text{prc})$.
\iffalse This is a noteworthy results in the light of the fact
that evolving altruistic preferences have been shown to support cooperative behavior in a
single stage game \myalert{[CITE]}. In contrast, averaging over payoffs in multiple stage
games, like we do here, makes altruistic preferences prime victims of evolutionary eradication.
\fi
It is thus easy to see that the previous results still obtain for the larger meta-game in
table~\ref{tab:ExpectedFitness_2x2_Full}: $(\text{reg}, \text{imp})$,
$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ are still neutrally stable;
simulation runs of the (discrete-time) replicator dynamics on the $8 \times 8$ meta-game from
table~\ref{tab:ExpectedFitness_2x2_Full} end up in population states consisting of only these
three types in variable proportion.

In sum, the presence of other subjective representations, such as those based on altruistic or competitive utilities, does not undermine, but rather strengthens our previous results.


                                   
\subsection{More Actions}
\label{sec:n-times-n}

Results from Section~\ref{sec:results} relied heavily on Fact~\ref{fact:equivalence2x2}, which
is no longer true when we look at arbitrary $n \times n$ games.
Table~\ref{tab:ExpectedFitness_10x10} gives approximations of expected fitness in the class of
$n \times n$ symmetric games. Concretely, the numbers in table~\ref{tab:ExpectedFitness_10x10}
are averages of evolutionary payoffs obtained in 100.000 randomly sampled symmetric games,
where each fitness game $G$ was sampled by first picking a number of acts $n^G \in \set{2, \dots, 10}$
uniformly at random, and then filling the necessary $n^G \times n^G$ payoff matrix with
i.i.d.~sampled numbers, as before.


\begin{table}[]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
  \toprule
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{com}, \text{imp})$
 & $(\text{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\text{com}, \text{prc})$
 & $(\text{alt}, \text{prc})$ \\ 
  \midrule
  $(\text{reg}, \text{imp})$ & 6.567 & 6.570 & 5.650 & 6.992 & 6.564 & 6.564 & 5.593 & 7.409 \\
  $(\text{obj}, \text{imp})$ & 6.476 & 6.483 & 5.896 & 6.818 & 6.484 & 6.484 & 5.850 & 7.124 \\
  $(\text{com}, \text{imp})$ & 6.468 & 6.647 & 5.512 & 7.169 & 6.578 & 6.578 & 5.577 & 7.354 \\
  $(\text{alt}, \text{imp})$ & 5.968 & 5.923 & 5.363 & 6.685 & 5.975 & 5.975 & 5.086 & 6.973 \\
  $(\text{reg}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{obj}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{com}, \text{prc})$ & 6.529 & 6.680 & 5.445 & 7.276 & 6.542 & 6.542 & 5.521 & 7.440 \\
  $(\text{alt}, \text{prc})$ & 6.450 & 6.337 & 5.772 & 6.978 & 6.457 & 6.457 & 5.479 & 7.500 \\
   \bottomrule                         
\end{tabular}  
}                    
\caption{Average evolutionary fitness for 100.000 randomly generated $n \times n$ symmetric games with $n$ randomly drawn from $\set{2, \dots, 10}$.}
\label{tab:ExpectedFitness_10x10}        
\end{table}

The most important result is that the regret minimizing type $(\text{reg}, \text{imp})$ is
strictly dominated by $(\text{reg}, \text{prc})$ and by $(\text{obj}, \text{prc})$ in the
meta-game from table~\ref{tab:ExpectedFitness_10x10}. This means that while simple regret
minimization can thrive in some evolutionary contexts, there are also contexts where it is
demonstrably worse off. While this may be bad news for regret minimizing types
$(\text{reg}, \text{imp})$, it is not the case that regret types \emph{as such} are weeded out
by selection. Since, by Fact~\ref{fact:maxEU-minReg}, $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are behaviorally equivalent in general, it remains that selection
based on meta-games constructed from $n \times n$ games will still not eradicate regret
preferences. %So, although regret types may not be selected for in this case, they are also not selected against.

On the other hand, there are plenty of ways in which the basic insights from
Propositions~\ref{proposition1} and \ref{proposition2} can make for situations in which
evolution would favor regret types, even in $n \times n$ games. If, for example, the belief of
a player is a trait that biological evolution has no bite on, but rather something that the
particular choice situation would exogenously give us (possibly because of the different amount
of information available in different choice situations), then regret-based preferences can
again drive out veridical preferences altogether. For example, suppose that only preference representations
compete and that agents' beliefs are exogenously given, in such a way that both players hold
precise (Bayesian) uniform beliefs with probability $p$ and they both have maximally imprecise
beliefs otherwise. This transforms the meta-game from table~\ref{tab:ExpectedFitness_10x10}
into a simpler $4 \times 4$ meta-game in which the payoff obtained by a subjective preference is
the weighted average over the payoffs of the subjective representations including that preference
in table~\ref{tab:ExpectedFitness_10x10}. Setting $p = 0.98$ for illustration, we get the
meta-game in table~\ref{tab:ExogeneousEpistemics}.
\begin{table}[]
\centering
\begin{tabular}{ccccc}
  \toprule
  & $\text{reg}$ 
  & $\text{obj}$
  & $\text{com}$
  & $\text{alt}$ \\ 
  \midrule
  $\text{reg} $ & 6.926 & 6.926 & 5.942 & 7.757 \\ 
  $\text{obj} $ & 6.924 & 6.924 & 5.948 & 7.751 \\ 
  $\text{com }$ & 6.566 & 6.570 & 5.481 & 7.434 \\ 
  $\text{alt} $ & 6.463 & 6.461 & 5.478 & 7.469 \\ 
   \bottomrule
\end{tabular}
\caption{Meta-game for the evolutionary competition between subjective utilities when beliefs are exogenously
  given (see main text).}
\label{tab:ExogeneousEpistemics}
\end{table}
The only evolutionarily stable state of this meta-game is again a monomorphic population of regret types. Accordingly, all
our simulation runs of the (discrete-time) replicator dynamics converge to monomorphic
regret-type populations. The reason why regret-based utilities prosper is because they have a substantial
fitness advantage when paired with imprecise beliefs (Propositions~\ref{proposition1} and \ref{proposition2}). If unmeasurable uncertainty is exogenously given
as something that happens to agents because of the information available in some choice situations, and even if that happens only very infrequently
(i.e., for rather low $p$), regret preferences will outperform objective preferences, as
well as competitive and altruistic preferences.


\subsection{Solitary Decisions}
\label{sec:solitary-decisions}

To see how different choice mechanisms behave in evolutionary competition based on solitary
decision making, we approximated, much in the spirit of meta-games, average accumulated
fitness obtained in randomly generated solitary decision problems. For our purposes, a decision
problem $D=\tuple{W, \Acts, \pi^D}$ consists of a set of states of the world $W$, a set of acts $\Acts$, and
a payoff function $\pi^D: W \times \Acts \rightarrow \mathbb{R}$.  We generate arbitrary
decision problems by selecting, uniformly at random, numbers of states and acts
$n^{D}_w, n^{D}_\act \in \set{2, \dots, 10}$ and then filling the payoff table, so to speak, by
i.i.d.~samples for each $\pi^D(w, \act) \in \set{0, 10}$. Unlike with
two-player games, we need to also sample the actual state of the world, which we selected
uniformly at random from the available states in the current decision problem. Accordingly, the fitness of choice mechanism $c$ in decision problem $D$ is given by:
$$F_D(c)= \pi^D(w,a_c^*(u_c^D,\Gamma_c))\overline{\mu}(w),$$ with $ \overline{\mu}(w)= \frac{1}{n_w^D}$ for all $w$.
As subjective representations,
we considered the original cast of four from table~\ref{tab:ExpectedFitness_4Types}, since altruistic and
competitive types are meaningless in solitary decision situations. As before, the
relevant fitness measure, defined in equation~(\ref{eq:FittnessChoiceMechSolitary}), was
approximated by Monte Carlo simulations, the results of which are given in
table~\ref{tab:SolitaryDecisions}.
\begin{align}
  \label{eq:FittnessChoiceMechSolitary}
  F(c) = \int P_{\mathcal{D}}(D) \  F_D(c) \text{ d} D 
\end{align}
\begin{table}
  \centering
  \begin{tabular}{cccc}
    \toprule
   $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 \\ \midrule
    6.318 & 6.237 & 6.661 & 6.661 \\ \bottomrule
  \end{tabular}
  \caption{Expected fitness of choice mechanisms approximated from 100.000 simulated solitary
    decision problems (see main text).}
  \label{tab:SolitaryDecisions}
\end{table}
Facts~\ref{fact:maxEU-minReg} and \ref{fact:equivalence2x2} still apply:
$(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$ are behaviorally equivalent in
general, and $(\text{reg}, \text{imp})$ is behaviorally equivalent to the former two in
decision problems with two states and two acts. This shows in the results from
table~\ref{tab:SolitaryDecisions} in that the averages for $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are identical. But since we included decision problems with more
acts and more states as well, the average for regret minimizers $(\text{reg}, \text{imp})$ is
not identical to the one of $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. It
is, in fact, lower, but again not as low as that of $(\text{obj}, \text{imp})$.

This means that every relevant result we have seen about game situations is also borne out for
solitary decisions. Evolutionary selection based on objective fitness will not select against
regret preferences, as these are indistinguishable from veridical preferences
when paired with precise beliefs. But when paired with imprecise beliefs, regret-based
utilities outperform objective utilities. Consequently, if there is a chance, however
small, that agents fall back on imprecise beliefs, evolution will actually positively select
for non-veridical regret-based preferences.


\subsection{Sophisticated Beliefs}

Since one of our main purposes was to illustrate the usefulness of a meta-game approach by the
case study of objective and regret preferences, we have partially neglected an important and
interesting issue, namely the evolution of ways of forming beliefs about co-players' behavior
or the actual state of the world. For reasons of space we must, unfortunately, leave a deeper
exploration of belief type evolution to another occasion. Two remarks are in order
nonetheless. Firstly, belief type evolution can be studied without conceptual hurdles in the
meta-game framework, so that there is no principled argument against the main methodological
contribution of this paper. Secondly, our results regarding the comparison between regret and
objective types remain to be informative, even if we allow agents to learn or reason
strategically.\footnote{Some research has recently been done along these lines. See in
  particular \cite{RobalinoRobson16,Mengel12,Mohlin12}.} This is because we know from
Fact~\ref{fact:maxEU-minReg} that regret and objective preferences come up behaviorally
equivalent when paired with precise probabilistic beliefs (given identical decision rule). This
holds no matter what the content of that belief is. So, if learning, reasoning or statistical
knowledge about a recurrent situation can be brought to bear, this will not make evolution
select against regret-based preferences. If, on the other hand, agents resort to imprecise
beliefs at least occasionally (e.g., when they are unaware of the co-player or her utilities,
or when strategic reasoning cannot reduce all uncertainty about the co-player's choice), then
regret-based preferences can be favored by natural selection over objective preferences.

\section{Conclusion} \label{sec:conclusion}


The assumption that players and decision makers maximize their (subjective) utility is central
through in the economics literature, and the maximization of actual (objective) payoffs is
often justified by appealing to evolutionary arguments and natural selection. In contrast to
the standard view, we showed the existence of player types with subjective utilities different
from the actual evolutionary payoffs that can outperform types whose subjective utilities
coincide with the evolutionary payoffs. The claim is not that regret preferences are the best
on the market, but rather that utilities that perfectly mirror evolutionary fitness can be
outclassed by subjective utilities that differ from the objective fitness.  While the
literature on evolution of preferences has focused on fixed games, we have adopted a more
general approach here. We suggested that attention to “meta-games” is crucial, because what may
be a good subjective representation in one type of game (e.g., cooperative preferences in the
Prisoner’s Dilemma class) need not be generally beneficial. Taken together, we presented a
variety of plausible circumstances in which evolutionary competition between choice mechanisms
on a larger class of games can favor non-veridical preference representations focusing on
regret.

%Meta-games are useful tools for probing into conceptual questions about ecological rationality. The approach is rich and general, with many possible further applications imaginable beyond the case study presented here. For example, the evolutionary investigation of different risk and ambiguity attitudes when competing against each other may bring new insights on both behavioral and theoretic decision making under uncertainty, as well as on many issues in theoretical biology and behavioral ecology. Moreover, this paper only considered one decision rule (maxmin), leaving it to future work to explore evolutionary competition of different rules as well. Finally, while this paper only used dummy models of diverse decision environments, the meta-game approach also allows for empirically informed models of rich and variable environments.


\appendix

\iffalse
\section{Background on Imprecise Probabilities}
\label{sec:impr-prob-beli}

Consider the following problem. I have, on the desk in front of me, a bag containing either red
or black marbles. There is no further information about the distribution of the two
colors. What is then the probability that I will draw a red marble? As in \citet{walley96}, we
are not talking about physical probability, but rather about epistemic probability, i.e., the
subjective probability that an agent attributes to an event.  In the literature there are two
traditional ways of answering this. According to standard Bayesianism, one should always be
able to determine a precise betting rate, that immediately translates into a precise
probability distribution. In the case
of the bag on my desk, a strict Bayesian, in the absence of any further information, would
probably rely on the \textit{principle of insufficient reason} and answer that the DM should
have a uniform measure that considers all the outcomes equipossible.


The second answer involves imprecise probabilities and is related to the literature about
unmeasurable uncertainty. Many authors argued in favor of this kind of approach
\citep[e.g.,][]{levi74,gardsah82,walley96}. Since it is consistent with the information
available that all the marbles are red, or that no marble is red, the resulting model should
take it into account and specify a lower probability of red $\underline{P}(R)$ and an upper
probability of red $\overline{P}(R)$ that define an interval of possible probabilistic beliefs
[$\underline{P}(R), \overline{P}(R)$]. Given the information available in this case we would
have $\underline{P}(R)=0$ and $\overline{P}(R)=1$. This approach appears particularly relevant
in a game theoretical context. Indeed, in a recent paper \citet{BattCerrMM15} write:

\begin{quote}
  Such uncertainty is inherent in situations of strategic interaction. This is quite obvious
  when such situations have been faced only a few times. (p.~646)
\end{quote}

\noindent In evolutionary game theory, for instance, players in a population obviously face
uncertainty about the composition of the population that they are part of, and consequently
about the co-player that they are randomly paired with at each round and about the co-player's
action. 

We therefore represent maximal uncertainty in two different ways that correspond to the two
approaches discussed in the main text: a Bayesian uniform measure and an imprecise probability
set. The latter represents uncertainty for players who are not able to narrow down the set of
probability measures to a single one and they consider all of them as possible. The former
precise belief type conceptualizes the uncertainty about the population by using the principle
of insufficient reason and ascribes equal probability to all the possible alternatives. As we
have seen, both options are reasonable and justifiable ways to deal with situations of
uncertainty.

In decision theory, the imprecise probability model is in line with most of the representation
results of decision making under uncertainty \citep[e.g.,][]{gilsch89,KlibMarMuk05,GhirMar02},
and seems justified by empirical observations too. A prominent example is Ellsberg's paradox
(\citet{ells61}). A bag contains 90 marbles, 30 are red and the remaining 60 are either black
or yellow. A marble is to be drawn and DM can win 100€ if she guesses the color of the drawn
marble. DM is offered two different bets. In the first one she can choose to bet either on
red (R) or on yellow (Y), while in the second she can bet either on red or black (RB) or on
yellow or black (YB), as shown in table \ref{Ellsberg}.

\definecolor{yellow}{RGB}{255,188,1}


\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\cmidrule(l){2-4}
\multicolumn{1}{c}{} & {\color{red}R}   & {\color{yellow}Y}   & B   \\ \cmidrule(l){2-4} 
$f_{{\color{red}R}}$              & 100 & 0   & 0   \\
$f_{{\color{yellow}Y}}$              & 0   & 100 & 0   \\
$f_{{\color{red}R}B}$            & 100 & 0   & 100 \\
$f_{{\color{yellow}Y}B}$             & 0   & 100 & 100 \\ \bottomrule
\end{tabular}
\caption{Ellsberg's paradox}
\label{Ellsberg}
\end{table}

The pattern consistently observed in experiments is:
$f_{{\color{red}R}} \succ f_{{\color{yellow}Y}}$ and
$f_{{\color{red}R}B} \prec f_{{\color{yellow}Y}B}$. This is clearly incompatible with any
precise probabilistic belief about the proportion of black and yellow marbles. This kind of
behavior has been famously axiomatized by \citet{gilsch89}. The
choice pattern is normally explained in terms of aversion to unmeasurable uncertainty, and the standard
representation is given by means of a set of probability measures together with maxmin rule. We
can think of DM in Ellsberg's example as maximinimizing expected utility over the
probability set
$
[\underline{P}(R)=\overline{P}(R)=\frac{1}{3}, \underline{P}(Y)= \underline{P}(B)=0, \overline{P}(Y)= \overline{P}(B)=\frac{2}{3}]
$.
This amounts to calculating for any probability measure contained in the probability set the
expected utility of any available action, and then to choosing the action that guarantees the
highest minimal expected utility. 

Evidence from experimental literature suggests that agents are mostly uncertainty averse \citep[e.g.,][]{TrautKuil16}. In line with empirical data, we assume that players in the population
are uncertainty averse and conform to maxmin expected utility. The resulting behavior is then
produced by maximinimizing the subjective utility given by the player's preference type over
the set of probabilities given by the belief type.

\fi

\section{Proofs}
\label{sec:proofs}

The proof of Proposition \ref{proposition1} relies on a partition of $\mathcal{G}$, and on some
lemmas. For brevity, let us denote the regret minimizer $(\text{reg}, \text{imp})$ by $R$ and
the maximinimizer $(\text{obj}, \text{imp})$ by $M$. Following
equation~(\ref{eq:FittnessChoiceMechGamePairwise}), let $F_{\mathcal{G}}(X,Y)$ denote the
expected payoff of choice mechanism $X$ against choice mechanism $Y$ on the possibly restricted
class of fitness games $\mathcal{G}$.

\vspace{.5cm}


\noindent \textbf{Proof of Proposition \ref{proposition1}.} By definition of strict dominance,
we have to show that in the class $\mathcal{G}$ of symmetric $2\times2$ games with payoffs
sampled from a set of i.i.d. values with at least 3 elements in the support, it holds that:
\begin{multicols}{2}
  \begin{itemize}
  \item[(i)] $F_{\mathcal{G}}(R,R)>F_{\mathcal{G}}(M,R);$
  \item[(ii)] $F_{\mathcal{G}}(M,M)<F_{\mathcal{G}}(R,M).$
  \end{itemize}
\end{multicols}

\noindent To show this we use the following partition of $\mathcal{G}$, based on payoffs parametrized as
follows:
\begin{center}
  \begin{tabular}{ccc}
    \toprule
    & $I$ & $II$ \\ \midrule
    $I$ & $a$ & $b$ \\
    $II$ & $c$ & $d$ \\ \bottomrule
  \end{tabular}
\end{center}
\begin{enumerate}
\item Coordination games $\mathcal{C}$: $a>c$ and $d>b$;
\item Anti-coordination games $\mathcal{A}$: $a<c$ and $d<b$;
\item Strong dominance games $\mathcal{S}$: aut $(a>c$ and $b>d)$
aut $(a<c$ and $b<d)$;
\item Weak dominance games $\mathcal{W}$: aut $a=c$ aut $b=d$;
\item Boring games $\mathcal{B}$: $a=c$ and $b=d$.
\end{enumerate}
Before proving the lemmas, it is convenient to fix some notation. Let us call $x,y,z$ the 3
elements in the support, and without loss of generality suppose that $ x > y > z $.  We denote
by $C$ a coordination game in $\mathcal{C}$ with payoffs $a_{C}$, $b_{C}$, $c_{C}$, and
$d_{C}$; similarly for games $A \in \mathcal{A}$, $S \in \mathcal{S}$, $W \in \mathcal{W}$, and
$B \in \mathcal{B}$.  Let us denote by $I_{RC}$ the event that a $R$-player plays action $I$ in
the game $C$; and similarly for action $II$, for player $M$, and for games $A$, $S$, $W$ and
$B$. We first consider the case of i.i.d. sampling with finite support.

\begin{lemma} \label{lemma:S-B games} $R$ and $M$ perform equally well in $\mathcal{S}$ and in
  $\mathcal{B}$.
\end{lemma}

\begin{proof}
  By definition of regret minimization and maxmin it is easy to check that whenever in a game
  there is a strongly dominant action $a^{\$}$, then $a^{\$}$ is both the maxmin action and the
  regret minimizing action. Then, for all the games in $\mathcal{S}$, $R$ chooses action $a$ if
  and only if $M$ chooses action $a$. Consequently, $R$ and $M$ always perform equally (well)
  in $\mathcal{S}$. In the case of $\mathcal{B}$ it is trivial to see that all the players
  perform equally.
\end{proof}

\begin{lemma} \label{lemma:W games} In $\mathcal{W}$, $R$ strictly dominates $M$.
\end{lemma}

\begin{proof}
  Assume without loss of generality that $b=d$, and that $ a>c $. There are two cases that we have to check:
  (i) $c < b=d$ and (ii) $c \geq b=d$. In the first case it is easy to see that $R$ and $M$
  perform equally: act $I$ is the choice of both $R$ and $M$. In the case of (ii) instead we have that $I$ is the regret minimizing action, whereas both actions have the same minimum and $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$, since both $I$ and
  $II$ maximize the minimal payoff. Consider now a population of $R$ and $M$ playing games from
  the class $\mathcal{W}$.  Whenever (i) is the case $R$ and $M$ perform equally well. But suppose $W \in \mathcal{W}$ and (ii) is the case. Then,
  $\pi_W(R,R)=a>\frac{1}{2}a+\frac{1}{2}c=\pi_W(M,R)$, whereas
  $\pi_W(M,M)=\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d<\frac{1}{2}a+\frac{1}{2}b=\pi_W(R,M)$.
  Hence, we have that in general
  $F_{\mathcal{W}}(R,R)>F_{\mathcal{W}}(M,R),\mbox{ and
  }F_{\mathcal{W}}(M,M)<F_{\mathcal{W}}(R,M)$.
\end{proof}

\noindent Since it is not difficult to see that both $(R,R)$ and $(M,M)$ are strict Nash equilibria in
$\mathcal{C}$, and that $(R,R)$ and $(M,M)$ are not Nash equilibria in $\mathcal{A}$, the main
part of the proof will be to show that $R$ strictly dominates $C$ in the class
$\mathcal{C}\cup\mathcal{A}$, that is:
\begin{multicols}{2}
  \begin{itemize}
  \item[(i')] $F_{\mathcal{C}\cup\mathcal{A}}(R,R)>F_{\mathcal{C}\cup\mathcal{A}}(M,R),$
  \item[(ii')] $F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M).$
  \end{itemize}
\end{multicols}

\noindent This part needs some more lemmas to be proven, but firstly we introduce
the following bijective function $\phi$ between coordination and
anti-coordination games.

\begin{definition}[$\phi$] \label{def:bijection phi} The permutation $\phi(a,b,c,d)=(c,d,a,b)$
  defines a bijective function $\phi:\mathcal{C}\rightarrow\mathcal{A}$ that for each
  coordination game $C\in\mathcal{C}$ with payoffs $(a_{C},b_{C},c_{C},d_{C})$ gives the
  anti-coordination game $A\in\mathcal{A}$ with payoffs
  $(a_{A},b_{A},c_{A},d_{A})=(c_{C},d_{C},a_{C},b_{C})$. Essentially, $\phi$ swaps rows
  in the payoff matrix.
\end{definition}

\begin{lemma} \label{lemma:probabilities coord-ant}
Occurrence probability of $C$ equals that of $\phi(C)$: $P(\phi(C))=P(C)$.
\end{lemma}

\begin{proof}
  By definition, each game $C\equiv(a_{C},b_{C},c_{C},d_{C})$ is such that $a_{C}>c_{C}$ and
  $d_{C}>b_{C}$, and each game $A\equiv(a_{A},b_{A},c_{A},d_{A})$ is such that $a_{A}<c_{A}$
  and $d_{A}<b_{A}$. Given that $a,b,c,d$ are i.i.d. random variables and that a sequence of
  i.i.d. random variables is exchangeable, it is clear that the probability of
  $(a_{C},b_{C},c_{C},d_{C})$ equals the probability of $(c_{C},d_{C},a_{C},b_{C})$.  Hence,
  $P(\phi(C))=P(C)$.
\end{proof}

\begin{lemma} \label{lemma:probabilities actions coord-ant} Let $P(E)$ be the probability of
  event $E$, e.g., $P(I_{RC})$ is the probability that a random $R$-player plays act $I$ in coordination game $C$, which is either 0, $.5$ or 1.  It then holds that:
\begin{itemize}
\item $P(I_{RC})=P(II_{R\phi(C)})$, and $P(II_{RC})=P(I_{R\phi(C)})$;
\item $P(I_{MC})=P(II_{M\phi(C)})$, and $P(II_{MC})=P(I_{M\phi(C)})$.
\end{itemize}
\end{lemma}

\begin{proof}
  It is easy to check that if $b_{C}-d_{C}>c_{C}-a_{C}$, a $R$-player plays action $I$ in $C$;
  that if $b_{C}-d_{C}<c_{C}-a_{C}$, $R$ plays $II$; and that if $b_{C}-d_{C}=c_{C}-a_{C}$, a
  $R$-player is indifferent between $I$ and $II$ in $C$, and so randomizes with
  $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly, if $a_{A}-c_{A}>d_{A}-b_{A}$, a $R$-player plays
  action $I$ in $A$; if $a_{A}-c_{A}<d_{A}-b_{A}$, $R$ plays $II$; and if
  $a_{A}-c_{A}=d_{A}-b_{A}$, a $R$-player is indifferent between $I$ and $II$ in $A$, and
  randomizes with $(\frac{1}{2}I;\frac{1}{2}II)$. Consequently, if $b_{C}-d_{C}>c_{C}-a_{C}$,
  then $P(I_{RC})=1$, and by definition of $\phi$ we have $P(II_{R\phi(C)})=1$. Likewise, if
  $b_{C}-d_{C}<c_{C}-a_{C}$, then $P(II_{RC})=1=P(I_{R\phi(C)})$; and if
  $b_{C}-d_{C}=c_{C}-a_{C}$,
  then $P(I_{RC})=P(II_{RC})=\frac{1}{2}=P(II_{R\phi(C)})=P(I_{R\phi(C)})$. \\
  In the same way, in coordination games we have that if $b_{C}>c_{C}$, a $M$-player plays $I$;
  if $c_{C}>b_{C}$, a $M$-player plays $II$; and if $b_{C}=c_{C}$, $M$ is indifferent between
  $I$ and $II$, and plays $(\frac{1}{2}I;\frac{1}{2}II)$. In anti-coordination games instead,
  if $a_{A}>d_{A}$, $M$ plays $I$; if $a_{A}<d_{A}$, $M$ plays $II$; if $a_{A}=d_{A}$, $M$
  plays $(\frac{1}{2}I;\frac{1}{2}II)$.  By definition of $\phi$:
  $P(I_{MC})=1=P(II_{M\phi(C)})$ if $b_{C}>c_{C}$; $P(II_{MC})=1=P(I_{M\phi(C)})$ if
  $c_{C}>b_{C}$; and $P(I_{MC})=P(II_{MC})=\frac{1}{2}=P(II_{M\phi(C)})=P(I_{M\phi(C)})$ if
  $b_{C}=c_{C}$.
\end{proof}

\begin{lemma} \label{lemma:action implications}
It holds that:
  \begin{itemize}
  \item $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$;
  \item $a^{C}=d^{C}\rightarrow I_{MC}=I_{RC}$.
  \item $a_{C}<d_{C}\rightarrow(II_{MC}\subseteq II_{RC})$;
  \end{itemize}
\end{lemma}

\begin{proof}
The event that $R$ plays action $I$, $I_{RC}$, with positive probability is the event that $b_{C}-d_{C} \geq c_{C}-a_{C}$: if $b_{C}-d_{C}>c_{C}-a_{C}$, $R$ plays $I$, and if $b_{C}-d_{C}=c_{C}-a_{C}$, $R$ plays
  $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly, the event that $I_{MC}$ has positive occurrence is the event that $b_{C} \geq c_{C}$: if $b_{C}>c_{C}$, $M$ plays $I$, and if
  $b_{C}=c_{C}$, $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$. 
Then, $I_{RC}$ implies that
  $b_{C}-d_{C}\geq c_{C}-a_{C}$, and $I_{MC}$ implies that $b_{C}\geq c_{C}$. Moreover, on the
  assumption that $a_{C}>d_{C}$, it is easy to check that $b_{C}\geq c_{C}$ implies
  $b_{C}-d_{C}>c_{C}-a_{C}$.  Hence, in any $C$ with $a_{C}>d_{C}$ it holds that
  $I_{MC}\mbox{ implies }I_{RC}$, i.e., $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$.
Instead, it is possible that $a_{C}>d_{C}$, $b_{C}-d_{C}>c_{C}-a_{C}$ and $b_{C}<c_{C}$ hold
  simultaneously, so that $I_{MC}\nsupseteq I_{RC}$.
By a symmetric argument it can be shown that
  $a_{C}<d_{C}\rightarrow(II_{MC}\subseteq II_{RC})$ too. Finally, when $a_{C}=d_{C}$ it holds
  that: $b_{C}-d_{C}>c_{C}-a_{C}$ iff $b_{C}>c_{C}$; $b_{C}-d_{C}<c_{C}-a_{C}$ iff
  $b_{C}<c_{C}$; and $b_{C}-d_{C}=c_{C}-a_{C}$ iff $b_{C}=c_{C}$. Hence,
  $a_{C}=d_{C}\rightarrow I_{MC}=I_{RC}$.
\end{proof}

\vspace{.5cm}

\noindent We are now ready to prove that
$F_{\mathcal{C}\cup\mathcal{A}}(R,R)>F_{\mathcal{C}\cup\mathcal{A}}(M,R)$. With notation like
$P(I_{RC} \cap I_{RC})$ denoting the probability that a random $R$-player plays $I$ and another
$R$-player plays $I$ as well in game $C$, rewrite the inequality as:
\begin{align*}
   \sum_{C \in \mathcal{C}}P(C) & [P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot
  d_{C}+P(I_{RC}\cap II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C}] 
  \\ + \sum_{A \in
    \mathcal{A}}P(A) & [P(I_{RA}\cap I_{RA})\cdot a_{A}+P(II_{RA}\cap II_{RA})\cdot
  d_{A}+P(I_{RA}\cap II_{RA})\cdot b_{A}+P(II_{RA}\cap I_{RA})\cdot c_{A}]\\
  > \sum_{C \in
    \mathcal{C}}P(C) & [P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot
  d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C}] \\
   + \sum_{A \in
    \mathcal{A}}P(A) & [P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot
  d_{A}+P(I_{RA}\cap II_{MA})\cdot c_{A}+P(II_{RA}\cap I_{MA})\cdot b_{A}]
\end{align*}
\noindent By Lemma \ref{lemma:probabilities coord-ant} and Lemma \ref{lemma:probabilities
  actions coord-ant}, we can express everything in terms of $C$ only:
\begin{align*}
  & \textstyle{\sum_{C}} P(C)[P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot d_{C}+P(I_{RC}\cap
  II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C} + \\
  & \ \ \ \ P(II_{RC}\cap II_{RC})\cdot c_{C}+P(I_{RC}\cap I_{RC})\cdot b_{C}+P(II_{RC}\cap
  I_{RC})\cdot d_{C}+P(I_{RC}\cap
  II_{RC})\cdot a_{C}] \\
  > & \textstyle{\sum_{C}}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot
  d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C} + \\
  & \ \ \ \ P(II_{RC}\cap II_{MC})\cdot c_{C}+P(I_{RC}\cap I_{MC})\cdot b_{C}+P(II_{RC}\cap
  I_{MC})\cdot a_{C}+P(I_{RC}\cap II_{MC})\cdot d_{C}]
\end{align*}
This simplifies to:
\begin{align*}
  & \textstyle{\sum_{C}}P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C}
  \cdot (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) + 
\\ & \ \ \ \ c_{C} \cdot
  (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap
  II_{RC})+P(II_{RC}\cap I_{RC}))] 
\\ > & \textstyle{\sum_{C}}P(C)[a_{C} \cdot (P(I_{RC}\cap
  I_{MC})+P(II_{RC}\cap I_{MC})) + b_{C} \cdot (P(II_{RC}\cap I_{MC})+P(I_{RC}\cap I_{MC})) + \\
  & \ \ \ \ c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap
  II_{MC})+P(I_{RC}\cap II_{MC}))]
\end{align*}
\noindent Now let us split into $a>d$ and $a<d$, and consider $a>d$ first.  Notice that, by
Lemma \ref{lemma:action implications}, the case $a=d$ is irrelevant in order to discriminate
between $R$ and $M$. If $a>d$, by Lemma \ref{lemma:action implications} we can eliminate the
cases where $R$ plays $II$ and $M$ plays $I$:
\begin{align*}
  & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot
  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) \\ 
  & \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC})
  +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC}))] \\
  > & 
  \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot P(I_{RC}\cap I_{MC}) + b_{C} \cdot P(I_{RC}\cap I_{MC}) +
  c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) \\ 
  & \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap
  II_{MC})+P(I_{RC}\cap II_{MC}))]
\end{align*}
Rewrite:
\begin{align*}
  & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})-
  P(I_{RC}\cap I_{MC})) \\ 
  & \ \ \ \ + b_{C} \cdot (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})-
  P(I_{RC}\cap I_{MC})) \\ 
  & \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})-
  P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\ 
  & \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap
  II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
\noindent We now distinguish between two cases: (1) $ a-c = d-b $ and (2) $  a-c \neq d-b $. Notice that $P(I_{RC}\cap II_{RC}) \neq 0$ if and only if case (1) obtains, and that $a>d$ and (1) imply $II_{MC}$. Then, from (1) we have\footnote{Note that when we have only 3 elements in the support it is not guaranteed that case (1), together with $a>d$, may arise in a coordination game, whereas it is guaranteed that case (2), together with $a>d$, occurs with some positive probability. If we take for instance $x= 5, y= 2, z= 1$, then case (1) cannot obtain, whereas if we take $x= 3, y= 2, z= 1$, both (1) and (2) may obtain ($a=3, b=1, c=2, d=2$ for case (1), and $a=3, b=1, c=2, d=2$ for case (2)). Moreover, under the assumption that $a>d$, having 3 elements in the support is a necessary and sufficient condition for case (2) to have positive occurrence in a coordination game. As it will be clear in the following, a positive occurrence of case (2) only is enough for the theorem to hold.}:

\begin{align*} 
& \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC}))\\
& \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))\\
& \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
that is
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (\frac{1}{4}+\frac{1}{4}) + b_{C} \cdot  (\frac{1}{4}+\frac{1}{4}) + c_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2}) + d_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2})]> 0
\end{align*}
%\begin{align*}
%\textstyle{\sum_{C_{a>d}}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]> 0
%\end{align*}
\noindent Since we have assumed $ a-c = d-b $, the last inequality is not satisfied. We have instead:

$$ \sum_{C_{a>d}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]= 0 $$

\noindent This means that where $a_{C}>d_{C}$ and
where (1) is the case, $R$ and $M$ are equally fit. This changes when we turn to (2). In that
case, since $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$ by Lemma \ref{lemma:action implications}, we have that
$P(I_{RC} \cap I_{RC})-P(I_{RC}\cap I_{MC})=P(I_{RC}\cap II_{MC})$. Moreover, when $a_{C}>d_{C}$, $b_{C}\geq c_{C}$ implies
  $b_{C}-d_{C}>c_{C}-a_{C}$ (see Lemma \ref{lemma:action implications}). Consequently, when $M$ plays either $I$ or $  (\frac{1}{2}I;\frac{1}{2}II)$, $R$ always plays $I$. Hence, whenever $a_{C}>d_{C}$ and (2) obtain, it also holds that $P(II_{RC}\cap II_{MC})=P(II_{RC} \cap II_{RC})$. In this case we can simplify:
\begin{align*}
& \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{RC}\cap I_{RC})- P(I_{RC}\cap I_{MC})) \\
& \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\
& \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap II_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot P(I_{RC}\cap II_{MC}) + b_{C} \cdot  P(I_{RC}\cap II_{MC}) \\- c_{C} \cdot P(I_{RC}\cap II_{MC}) - d_{C} \cdot P(I_{RC}\cap II_{MC})]> 0
\end{align*}
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0
\end{align*}
\noindent We know that $I_{RC}$ implies that $a_{C}-c_{C}\geq d_{C}-b_{C}$.
% and $II_{MC}$ implies that $b_{C}\leq c_{C}$.
Since we have assumed that $a_{C}-c_{C}\neq d_{C}-b_{C}$, we have that
$a_{C}-c_{C} > d_{C}-b_{C}$. Hence, the inequality
$$\sum_{C_{a>d}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0$$
is satisfied. So, when $a_{C}>d_{C}$, $R$
strictly dominates $M$. Symmetrically, from $a<d$ and by distinguishing between the two cases
(1) and (2) as before, in the end we get:
\begin{itemize}
\item[(1)] $\sum_{C_{a<d}} P(C)[-\frac{1}{2}a_{C}- \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]= 0$; and
\item[(2)] $\sum_{C_{a<d}} P(C)[P(II_{RC}\cap I_{MC})\cdot (-a_{C} - b_{C} + c_{C} + d_{C})]> 0$.
\end{itemize}
\noindent Hence, we can conclude that $R$ strictly dominates $M$ in the class
$\mathcal{C}\cup\mathcal{A}$. Notice that in case of i.i.d. sampling with continuous support,
games in $\mathcal{B} $ and $\mathcal{W} $ never arise, but the proof is the same for the
remaining games in $\mathcal{S}$, $\mathcal{C} $ and $\mathcal{A}$.

\medskip{}

\noindent It remains to be shown that
$F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M)$. As before, spell
this out as:
\begin{align*}
& \sum_{C}P(C)[P(I_{MC}\cap I_{MC})\cdot a_{C}+P(II_{MC}\cap II_{MC})\cdot d_{C}+P(I_{MC}\cap II_{MC})\cdot b_{C}+P(II_{MC}\cap I_{MC})\cdot c_{C}] \\
+ & \sum_{A}P(A)[P(I_{MA}\cap I_{MA})\cdot a_{A}+P(II_{MA}\cap II_{MA})\cdot d_{A}+P(I_{MA}\cap II_{MA})\cdot b_{A}+P(II_{MA}\cap I_{MA})\cdot c_{A}] \\
< & \sum_{C}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot b_{C}+P(II_{RC}\cap I_{MC})\cdot c_{C}]\\ 
+ & \sum_{A}P(A)[P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot d_{A}+P(I_{RA}\cap II_{MA})\cdot b_{A}+P(II_{RA}\cap I_{MA})\cdot c_{A}]
\end{align*}
\noindent When $a>d$, similarly to the above derivation, we get: 
\begin{align*} & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC}))\\ & + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC}))\\ & + c_{C} \cdot (P(II_{MC}\cap I_{MC}) +P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\ & + d_{C} \cdot (P(II_{MC}\cap II_{MC})+P(II_{MC}\cap I_{MC})- P(II_{RC}\cap II_{MC}))]< 0
\end{align*}
\noindent We now distinguish between (1) $b=c$, (2) $b>c$, and (3) $b<c$. Notice that either
(1) or (2), together with $a>d$, implies $I_{RC}$. Then we obtain:\footnote{Note that here, when
  we only have 3 elements in the support, case (2) is impossible, but cases (1) and (3) can
  occur with positive probability, and this enough for our purpose.}
\begin{itemize}

\item[(1)] $\sum_{C_{a>d}} P(C)[-\frac{1}{2}a_{C} - \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC}))]= 0$;

\item[(3)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (- P(I_{RC}\cap II_{MC})) + b_{C} \cdot  (- P(I_{RC}\cap II_{MC})) + c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] \leq 0$.

\end{itemize}
\noindent When $a<d$, the derivation proceeds symmetrically and we get: 
\begin{itemize}

\item[(1)] $\sum_{C_{a<d}} P(C)[\frac{1}{2}a_{C} + \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a<d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + c_{C} \cdot (- P(II_{RC}\cap I_{MC})) + d_{C} \cdot (- P(II_{RC}\cap I_{MC}))] \leq 0$;

\item[(3)] $\sum_{C_{a<d}} P(C)[c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] = 0$.

\end{itemize}
\noindent Finally, we can conclude that
$F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M)$. As before, notice
that, when we have i.i.d. sampling with continuous support, games in $\mathcal{W}$ and
$\mathcal{B}$ never occur, but the proof is the same for all the other cases. Hence, both when
the support of $a,b,c,d$ is finite, and when the support is infinite, $R$ strictly dominates
$M$ under the conditions assumed. \hfill $\dashv$


\vspace{.5cm}


\noindent \textbf{Proof of Proposition \ref{proposition2}.} Take an arbitrary real interval $(\underline{r},\overline{r})$, where
$\underline{r}$ is the infimum and $\overline{r}$ is the supremum,
and let $P$ be the PDF over $(\underline{r},\overline{r})$. Given
Lemma \ref{lemma:probabilities coord-ant}, we focus on the case of coordination games. Now fix a set
of probabilities $[s,t]$, corresponding to the uncertainty of the
players about the co-player's action. As we have seen, objective and
regret types will behave exactly the same when $s=t$ and $[s,t]$
reduces to a single probability. Notice that objective and regret
will also be indistinguishable when there is no game such that $s<\frac{a-c}{a-c+d-b}<t$.
It is then a necessary condition for regret types to be better than
objective types that the two types behave differently at least in
some games. Geometrically, the ratio $\frac{a-c}{a-c+d-b}$ denotes
the intersection point of the lines corresponding to action $I$ and
action $II$ (figure~\ref{fig:graphstrictinf1-2})
\begin{figure}
\centering\begin{tikzpicture}[scale=.9]


%
\draw [-, very thick]  (0,0) to (5,0);
%
\draw [-, very thick] (0,0) to (0,5);

%
\draw [-, very thick] (5,0) to (5,5);



%
\node (zero) [] at (-.75,-.5) {$P(II)=0$};

%
\node (s) [] at (2,0)
  [label=below:{$s$}] {|};
%
\node (t) [] at (4,0)
  [label=below:{$t$}] {|};
%
%\node (st) [] at (3,0)
 % [label=below:{$s + \nicefrac{\epsilon}{2}$}] {};


\draw [dashed] (2,0) -- (2,5);
\draw [dashed] (4,0) -- (4,5);

\draw [dotted] (3,0) -- (3,5);

%
\node (a') [] at (1.8,2.6)
  [label=left:{}] {$a'$};
\node (c') [] at (2.2,2)
  [label=left:{}] {$c'$};
\node (d') [] at (4.2,4)
  [label=left:{}] {$d'$};
\node (b') [] at (4.2,2.6)
  [label=left:{}] {$b'$};

%
\node (0y) [] at (0,0)
  [label=left:{$c$}] {};
\node (1y) [] at (0,1)
  [label=left:{}] {-};
\node (2y) [] at (0,2)
  [label=above right:{$I$}] [label=left:{$a$}] {-};

\node (3y) [] at (0,3)
  [label=left:{}] {-};
\node (4y) [] at (0,4)
  [label=left:{}] {-};
\node (5y) [] at (0,5)
  [label=left:{}] {-};
%
\node (1yy) [] at (5,1)
  [label=left:{}] {-};
\node (2yy) [] at (5,2)
  [label=left:{}] {-};

\node (3yy) [] at (5,3)
  [label=right:{$b$}] {-};
\node (4yy) [] at (5,4)
  [label=left:{}] {-};
\node (5yy) [] at (5,5)
  [label=left:{$II$}] [label=right:{$d$}] {-};
%

\node (p1) [] at (5,0)
  [label=below:{$1$}] {};

%


%
\draw [-, very thick] (0,2) to (5,3);
%
\draw [-, very thick] (0,0) to (5,5);
%


\draw[decorate,decoration={brace,amplitude=10}](5,5)--(5,3)node [black,midway,xshift=25pt] {\footnotesize $d-b$};
%
\draw[decorate,decoration={brace,amplitude=10}](0,0)--(0,2)node [black,midway,xshift=-25pt] {\footnotesize $a-c$};
%

\end{tikzpicture}\protect\caption{Example of strictly informative coordination game.}
\label{fig:graphstrictinf1-2}
\end{figure}
. If there is no game where such an intersection point falls into
the interval $[s',t']$, for some $s'\neq t'$, then whenever players
hold the set of probabilities $[s',t']$ they will perceive any game
that can arise as having a dominant action, i.e., as a strong dominance
game in partition cell $\mathcal{S}$. Note also that the ratio $\frac{a-c}{a-c+d-b}$
takes values in the interval $(0,1)$, since to get $\frac{a-c}{a-c+d-b}=0$
we shall have $a=c$ and for $\frac{a-c}{a-c+d-b}=1$ we need $d=b$,
which have no probability of occurrence for continuous random variables.
Furthermore, the ratio $\frac{a-c}{a-c+d-b}$ is dense in $(0,1)$,
and consequently in any interval $[s,t]\subset(0,1)$. Indeed, for
any rational number $\frac{m}{n}\in[s,t]$, we can always find $a,b,c,d\in(\underline{r},\overline{r})$
such that $\frac{a-c}{a-c+d-b}=\frac{m}{n}$ as follows. It is not
difficult to see that for this to be the case we must have 
\[
a-(a-b)\frac{m}{n}=c+(d-c)\frac{m}{n}.
\]
This equation reduces to
\begin{equation}
\frac{n-m}{n}a+\frac{m}{n}b=\frac{n-m}{n}c+\frac{m}{n}d,\label{eq:hit the point}
\end{equation}
which is an equality between two convex combinations with the same
weights $\frac{n-m}{n}$ and $\frac{m}{n}$. Then, whatever $a,b\in(\underline{r},\overline{r})$
we pick it is always possible to find (infinitely many) $c,d\in(a,b)\subset(\underline{r},\overline{r})$
such that equation (\ref{eq:hit the point}) is satisfied. Indeed,
for any $x\in(0,1)$ we have that $c=(\frac{n-m}{n}a+\frac{m}{n}b)x+a(1-x)$
and $d=(\frac{n-m}{n}a+\frac{m}{n}b)x+b(1-x)$ satisfy equation (\ref{eq:hit the point}).
This shows that for any rational number $\frac{m}{n}\in[s,t]$ we
can always find $c,d\in(a,b)\subset(\underline{r},\overline{r})$
such that $\frac{a-c}{a-c+d-b}=\frac{m}{n}$, and so we can always
find $a,b,c,d\in(\underline{r},\overline{r})$ such that $\frac{a-c}{a-c+d-b}=\frac{m}{n}$.
Since rational numbers are dense in $\mathbb{R}$, they are also dense
in any real interval $[s,t]\subset(0,1)$. Hence, the ratio $\frac{a-c}{a-c+d-b}$
is dense in $[s,t]$.

Once we have proven the density of $\frac{a-c}{a-c+d-b}$ for coordination
games $G=(a,b,c,d)$ in any real interval $[s,t]\subset(0,1)$, the
rest of the proof is connected to the proof of Proposition \ref{proposition1}. Specifically,
for a given interval $[s,t]$ we define
\begin{equation}
\begin{array}{ccccc}
a' & := & (1-s)a+sb & = & a+s(b-a)\\
b' & := & (1-t)a+tb & = & a+t(b-a)\\
c' & := & (1-s)c+sd & = & c+s(d-c)\\
d' & := & (1-t)c+td & = & c+t(d-c)
\end{array}\label{eq:a'b'c'd'}
\end{equation}
Substituting $ a',b',c',d'$ for $a,b,c,d$, Lemmas \ref{lemma:S-B games}, \ref{lemma:W games}, \ref{lemma:probabilities coord-ant}, \ref{lemma:probabilities actions coord-ant} and \ref{lemma:action implications} as well as the rest of the proof of Proposition \ref{proposition1} hold unchanged. \hfill $\dashv$



\newpage

\printbibliography[heading=bibintoc]

\end{document}



