\documentclass[fleqn,reqno,12pt]{article}

%========================================
% Packages
%========================================

\usepackage{etex}

\RequirePackage{amsmath}            % Formeln
\RequirePackage{amsfonts}           % Fonts for Formulas
\RequirePackage{amssymb}
\RequirePackage{amsthm}
\RequirePackage{dsfont}             % double stroke fonts
\RequirePackage[final]{graphicx}
\RequirePackage{booktabs}
\RequirePackage{enumerate}
\RequirePackage{paralist}
\RequirePackage[all]{xy}            
\RequirePackage{url}

\RequirePackage{txfonts} % for strict implication symbols
\RequirePackage{soul}
\RequirePackage{relsize} % provides command \relsize{+/-x} for relative
                     % font size changes
\RequirePackage[german,english]{babel}
\RequirePackage[utf8]{inputenc}
\RequirePackage[T1]{fontenc} 
\RequirePackage{xypic}
\RequirePackage{multicol}
\RequirePackage{subcaption}
\RequirePackage{tikz}
\usetikzlibrary{arrows,shapes,automata,backgrounds,petri,fit,decorations.pathmorphing}
\RequirePackage{units}

\RequirePackage{dialogue}

\RequirePackage{setspace}

\RequirePackage[colorinlistoftodos,color=lightgray,bordercolor=blue,textsize=footnotesize]{todonotes}

\RequirePackage{xspace} % for \xspace in definition of acronyms etc.


\RequirePackage[final,            % override "draft" which means "no nothing"
            colorlinks,       % rather than outlining them in boxes
            linkcolor=black,   % override truly awful colour choices
            citecolor=black,   %   (ditto)
            urlcolor=black,    %   (ditto)
            plainpages=false, % to overcome complaints with multiple
            pdfpagelabels,    % multiple page 1-s due to preface
            hypertexnames=false % solves warning, but interferes with
                                % index and \autoref apparently
            ]{hyperref}


\usepackage[backend=bibtex,natbib=true,style=authoryear-comp,backend=bibtex,doi=false,url=false]{biblatex}

\bibliography{choicePrince,biblio}

\newtheoremstyle{Satz}
   {}                      %Space above
   {1em}                  %Space below
   {\normalfont}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\bfseries\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')

\theoremstyle{Satz}
\newtheorem{theorem}{Theorem} 
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{fact}{Fact}
\newtheorem{claim}{Claim} 
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}{Exercise} 
\newtheorem{problem}{Problem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\newtheoremstyle{Bsp}
   {}                      %Space above
   {1em}                  %Space below
   {\itshape}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')
\theoremstyle{Bsp}
% \newtheorem{example}[theorem]{Example}


% Math --------------------
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\tuple}[1]{\left \langle #1\right\rangle}
\newcommand{\card}[1]{\left \lvert \, #1 \, \right\rvert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\setbar}{\ensuremath{\thinspace \mid \thinspace}}
\newcommand{\probbar}{\ensuremath{\mid}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\df}{\rightarrow}
\newcommand{\es}{\emptyset}
\newcommand{\den}[1]{\left [\! \left [ #1 \right ]\! \right]}
% \newcommand{\den}[1]{\left \llbracket #1  \right \rrbracket} % this would use fourier package which makes 'cases' environment bad
\newcommand{\no}{\noindent}
\newcommand{\hin}{"$\Rightarrow$" }
\newcommand{\rueck}{"$\Leftarrow$" }
\newcommand{\exs}{\vspace{.15cm}}
\newcommand{\pow}[1]{\ensuremath{\mathcal{P}(#1)}}	% Powerset
\newcommand{\restr}{{\restriction}}
\newcommand{\implicates}{\ensuremath{\leadsto}}  % arrow for
                                % implicatures in examples
\newcommand{\update}[2]{\ensuremath{#1[#2]}}
\newcommand{\myts}{\ensuremath{\thinspace}}
\newcommand{\mycolon}{\ensuremath{\thinspace \colon \thinspace}}
\newcommand{\mydot}{\ensuremath{\thinspace . \thinspace}}


\makeatletter
\newcommand{\prob}{\@ifstar
  \simpleprob%
  \condprob%
}
\def\simpleprob(#1){\ensuremath{\Pr(#1)}}
\def\condprob(#1|#2){\ensuremath{\Pr(#1 \,|\, #2)}}
\makeatother

% General Text Markup--------------------
\newcommand{\runex}[1]{\begin{center}#1\end{center}}
\newcommand{\mydef}[1]{\textsc{#1}}  		% definitions
\newcommand{\markdef}[1]{\textsc{#1}}  		% definitions; alternative
\newcommand{\myment}[1]{\emph{#1}}  	% first mentions
\newcommand{\myword}[1]{\textbf{\texttt{#1}}}	% refering to the word
\newcommand{\myemph}[1]{\emph{#1}}			% emphasis
\newenvironment{exnonum}{
  \begin{list}{}{
    \setlength{\leftmargin}{2.5em}
    \setlength{\rightmargin}{2.5em}
    %\setlength{\itemindent}{-1.5em}
  }
}{
  \end{list}
}


% Slanted Fractions
\newcommand{\myslantfrac}[2]{\msf{#1}{#2}}
\newcommand{\msf}[2]{\ensuremath{\nicefrac{#1}{#2}}}
\newcommand{\msftext}[2]{\nicefrac{#1}{#2}}


% Symbols for Conditionals
\newcommand{\cond}{\ensuremath{>}}
\newcommand{\bicond}{\ensuremath{\Leftrightarrow}}
\newcommand{\strcondWill}{\ensuremath{\boxRight}}
\newcommand{\strcondMight}{\ensuremath{\DiamondRight}}

% Signaling Games & IBR
\newcommand{\sen}{\ensuremath{S}\xspace}		% Sender variable
\newcommand{\mysen}[1]{\ensuremath{\sen^{#1}}} % Sender of type XYZ
\newcommand{\rec}{\ensuremath{R}\xspace}		% Receiver variable
\newcommand{\myrec}[1]{\ensuremath{\rec_{#1}}} % Receiver of type XYZ
\newcommand{\States}{\ensuremath{T}\xspace}		% Set of States
\newcommand{\state}{\ensuremath{t}\xspace}		% single states
\newcommand{\mystate}[1]{\ensuremath{\state_{\text{#1}}}\xspace} %meaningful states
\newcommand{\Messgs}{\ensuremath{M}\xspace}		% Set of Messages
\newcommand{\messg}{\ensuremath{m}\xspace}		% single messages
\newcommand{\mymessg}[1]{\ensuremath{\messg_{\text{#1}}}\xspace} %meaningful messages
\newcommand{\cost}{\ensuremath\operatorname{C}} % cost function
\newcommand{\Acts}{\ensuremath{A}\xspace}		% Set of R-actions
\newcommand{\act}{\ensuremath{a}\xspace}		% single action
\newcommand{\myact}[1]{\ensuremath{\act_{\text{#1}}}\xspace} %meaningful
\newcommand{\Worlds}{\ensuremath{W}}		% Worlds
\newcommand{\world}{\ensuremath{w}}		% single world
\newcommand{\myworld}[1]{\ensuremath{\world_{\text{#1}}}} %named world
\newcommand{\util}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\Util}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\utils}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\Utils}{\ensuremath{\operatorname{U}}}	% Utility function
\newcommand{\RealUtil}{\ensuremath{\operatorname{V}}}	% material payoffs
\newcommand{\Sstrat}{\ensuremath{\sigma}} % Behav/Probab Sender strategy
\newcommand{\Sstrats}{\ensuremath{\mathcal{S}}}	% Set of S-strategies
\newcommand{\Spure}{\ensuremath{s}} % Pure sender strategy
\newcommand{\Spures}{\ensuremath{\mathsf{S}}} % Set of pure sen strategies
\newcommand{\Smixed}{\ensuremath{\tilde{s}}} % Mixed sender strategy
\newcommand{\Smixeds}{\ensuremath{\Delta(\Messgs^\States)}}
\newcommand{\SpuresW}{\ensuremath{\mathsf{S}}} 
\newcommand{\SpuresS}{\ensuremath{\mathsf{S}^{{\mathrm{S}}}}}
\newcommand{\Rstrat}{\ensuremath{\rho}} % Behav/Probab Receiver strategy
\newcommand{\Rstrats}{\ensuremath{\mathcal{R}}}	% Set of R-Strategies
\newcommand{\Rpure}{\ensuremath{r}} % Pure receiver strategy
\newcommand{\Rpures}{\ensuremath{\mathsf{R}}} % Set of pure rec strategies
\newcommand{\Rmixed}{\ensuremath{\tilde{r}}} % Mixed receiver strategy
\newcommand{\Rmixeds}{\ensuremath{\Delta(\Acts^\Messgs)}} 
\newcommand{\RpuresW}{\ensuremath{\mathsf{R}}} 
\newcommand{\RpuresS}{\ensuremath{\mathsf{R}^{\mathrm{S}}}} 
\newcommand{\PureBR}{\ensuremath{\operatorname{BR}}} % Set of pure best responses
\newcommand{\ProbBR}{\ensuremath{\operatorname{BR_{Prob}}}} % Set of mixed best responses
\newcommand{\bel}{\ensuremath{\pi}}
\newcommand{\Bels}{\ensuremath{\Pi}}
\newcommand{\Sbel}{\ensuremath{\pi_{\sen}}}
\newcommand{\Sbels}{\ensuremath{\Pi_{\sen}}}
\newcommand{\Rbel}{\ensuremath{\pi_{\rec}}}
\newcommand{\Rbels}{\ensuremath{\Pi_{\rec}}}
\newcommand{\EU}{\ensuremath{\operatorname{EU}}} % Expected Utility
\newcommand{\EV}{\ensuremath{\operatorname{EV}}} % Expected Response Utility
\newcommand{\BR}{\ensuremath{\operatorname{BR}}} % Best Response
\newcommand{\QR}{\ensuremath{\operatorname{QR}}} % Quantal Response
\newcommand{\WBR}{\ensuremath{\text{{\relsize{-1}W}BR}}} % Weak Best Response
\newcommand{\SBR}{\ensuremath{\text{{\relsize{-1}W}BR}}} % Strong Best Response
\newcommand{\interpr}{\ensuremath{\delta}} % Interpretation strategy

% OT Stuff
\newcommand{\Gen}{\ensuremath{\operatorname{Gen}}} % Generator
\newcommand{\Eval}{\ensuremath{\operatorname{Eval}}} % Evaluator
\newcommand{\Con}{\ensuremath{\operatorname{Con}}} % Constraints
\newcommand{\metsuc}{\ensuremath{\succ}} % Symbol for BiOT metric
\newcommand{\metsuceq}{\ensuremath{\succeq}} % Symbol for BiOT metric
\newcommand{\Go}[1]{\operatorname{Pool}_{#1}}
\newcommand{\Oo}[1]{\operatorname{Opt}_{#1}}
\newcommand{\Bo}[1]{\operatorname{Blo}_{#1}}
\newcommand{\Gr}[1]{\operatorname{GAM^{\rho}}_{#1}}
\newcommand{\Or}[1]{\operatorname{OPT^{\rho}}_{#1}}
\newcommand{\Br}[1]{\operatorname{BLO^{\rho}}_{#1}}

% Abbreviations/Acronyms:
\newcommand{\acro}[1]{\textsc{#1}\xspace}
\newcommand{\acros}[1]{\textsc{#1}{\relsize{-1}s}\xspace}
\newcommand{\bc}{\acro{bc}} % Biscuit Conditional(s)
\newcommand{\bcs}{\acros{bc}}
\newcommand{\cbc}{\acro{cbc}} % Counterfactual BCs
\newcommand{\cbcs}{\acros{cbc}}
\newcommand{\ibr}{\acro{ibr}} % IBR model
\newcommand{\iqr}{\acro{iqr}} % IQR model
\newcommand{\rsa}{\acro{rsa}} % RSA model
\newcommand{\ot}{\acro{ot}} % BiOT
\newcommand{\biot}{\acro{b{\relsize{-1}i}ot}} % BiOT
\newcommand{\tom}{\acro{t{\relsize{-1}o}m}} % ToM
\newcommand{\fc}{\acro{fc}} % Free Choice
\newcommand{\cp}{\acro{cp}} % Conditional Perfection
\newcommand{\uc}{\acro{uc}} % Unconditional Readings
\newcommand{\pbe}{\acro{pbe}}   % Perfect Bayesian Equil.
\newcommand{\pbes}{\acros{pbe}}
\newcommand{\forind}{\acro{fi}} % forward induction
\newcommand{\tcp}{\acro{tcp}} % truth ceteris paribus
\newcommand{\cmr}{\acro{cmr}} % credible message rationalizability
\newcommand{\cm}{\acro{cm}} % credible message (profile) (Rabin)
\newcommand{\condition}[2]{\acro{#1}{#2}} % conditions 
\newcommand{\br}{\acro{br}} % best response (property)
\newcommand{\wbr}{\acro{{\relsize{-1}w}br}} % weak best response (property)
\newcommand{\sbr}{\acro{{\relsize{-1}s}br}} % strong best response (property)
\newcommand{\curb}{\acro{curb}} % curb sets
\newcommand{\gtp}{\acro{gtp}} % game theoretic pragmatics
\newcommand{\sda}{\acro{sda}} % simplification of disjunctive antecedents
\newcommand{\decprob}{\ensuremath{\mathcal{D}}}
\newcommand{\ques}{\ensuremath{\mathfrak{Q}}}
\newcommand{\vsi}{\acro{vsi}}
\newcommand{\evsi}{\acro{evsi}}
\newcommand{\uv}{\acro{uv}}
\newcommand{\qud}{\acro{qud}}
\newcommand{\NE}{\acro{ne}}
\newcommand{\NEs}{\acros{ne}}
\newcommand{\SNE}{\acro{sne}}
\newcommand{\SNEs}{\acros{sne}}
\newcommand{\SG}{\acro{sg}}
\newcommand{\SGs}{\acros{sg}}
\newcommand{\KO}{\textsc{ko\relsize{-1}bs}} % Kennedy's observation
\newcommand{\EVP}{\acro{evp}}    % extreme-value principle
\newcommand{\illc}{\acro{illc}}

% Evolution
\newcommand{\EGT}{\acro{egt}}
\newcommand{\ESS}{\acro{ess}}
\newcommand{\ESSs}{\acros{ess}}
\newcommand{\NSS}{\acro{nss}}
\newcommand{\NSSs}{\acros{nss}}
\newcommand{\sigsys}{\textsc{SigSys}\xspace} % signaling system
\newcommand{\sigsyss}{\textsc{SigSys{\relsize{-1}s}}\xspace} % signaling system Plural

\newcommand{\fin}{\rule{0mm}{1mm}\hfill{\rule{1.5cm}{0.2pt}}}

%Properly typeset tilde for URLs
\def\urltilde{\kern -.15em\lower .7ex\hbox{\~{}}\kern .04em}


% Beamer footnote for references:
\newcommand{\beamfn}[1]{
  \vfill
      \begin{footnotesize}
        \leftskip 0.1in
        \parindent -0.1in
       \hspace{-0.3cm}\rule{2cm}{0.01cm}\\ \vspace{-0.15cm}
        #1
      \end{footnotesize}
}

\newcommand{\myvec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\transpose}[1]{\ensuremath{\operatorname{T}(#1)}}
\newcommand{\normalize}[1]{\ensuremath{\operatorname{N}(#1)}}

\newcommand{\dn}[1]{\draftnote{#1}}
\newcommand{\fn}[1]{\footnote{#1}}

\newcommand{\stateunmarked}{\ensuremath{\state}\xspace}
\newcommand{\statemarked}{\ensuremath{\state^*}\xspace}
\newcommand{\messgunmarked}{\ensuremath{\messg}\xspace}
\newcommand{\messgmarked}{\ensuremath{\messg^*}\xspace}
\newcommand{\actunmarked}{\ensuremath{\act}\xspace}
\newcommand{\actmarked}{\ensuremath{\act^*}\xspace}

\newcommand{\sunmarked}{\ensuremath{\state}\xspace}
\newcommand{\smarked}{\ensuremath{\state^*}\xspace}
\newcommand{\munmarked}{\ensuremath{\messg}\xspace}
\newcommand{\mmarked}{\ensuremath{\messg^*}\xspace}
\newcommand{\aunmarked}{\ensuremath{\act}\xspace}
\newcommand{\amarked}{\ensuremath{\act^*}\xspace}

\newcommand{\ssome}{\mystate{\exists\neg\forall}}
\newcommand{\sall}{\mystate{\forall}}
\newcommand{\msome}{\mymessg{some}}
\newcommand{\mall}{\mymessg{all}}
\newcommand{\asome}{\myact{\exists\neg\forall}}
\newcommand{\aall}{\myact{\forall}}

% for repeating examples with gb4e

\newcounter{myexememory}
\newenvironment{exer}[1]
{
\setcounter{myexememory}{\value{exx}}
\setcounter{exx}{\getrefnumber{#1}}
\addtocounter{exx}{-1}  
\begin{exe}
}
{
\end{exe}
\setcounter{exx}{\value{myexememory}}
}

\newenvironment{nakedlist}{
  \begin{list}{\quad}{}
}
{
  \end{list}
}

\DefineNamedColor{named}{mycol}{cmyk}{0.6,0.6,0,0}
\DefineNamedColor{named}{mygray}{cmyk}{0.05,0.05,0.05,0.05}
\DefineNamedColor{named}{mygraylight}{cmyk}{0.017,0.017,0.017,0.017}
\DefineNamedColor{named}{mycol2}{cmyk}{0.8,0,0.8,0.2}

\newcommand{\mymark}[1]{{\color{mycol}{#1}}}

\DeclareMathOperator{\expo}{exp}

\newcommand{\mygray}[1]{{\textcolor{gray}{#1}}}
\newcommand{\mycol}[1]{{\textcolor{mycol}{#1}}}
\newcommand{\mycolh}[1]{{\textcolor{mycol2}{#1}}}


% \usepackage{calc}
\newsavebox\CBox
\newcommand\msout[2][0.5pt]{%
  \ifmmode\sbox\CBox{$#2$}\else\sbox\CBox{#2}\fi%
  \makebox[0pt][l]{\usebox\CBox}%  
  \rule[0.5\ht\CBox-#1/2]{\wd\CBox}{#1}}

\newcommand{\greensquare}{\raisebox{1.5pt}{\textcolor{Green}{\Large{\ensuremath{\blacksquare}}}}}
\newcommand{\bluecircle}{\textcolor{blue}{\Huge{\ensuremath{\bullet}}}}
\newcommand{\greencircle}{\textcolor{Green}{\Huge{\ensuremath{\bullet}}}}

\newcommand{\greensquareS}{\raisebox{1.5pt}{\textcolor{Green}{\normalsize{\ensuremath{\blacksquare}}}}}
\newcommand{\greensquareSS}{\raisebox{1.5pt}{\textcolor{Green}{\footnotesize{\ensuremath{\blacksquare}}}}}
\newcommand{\bluecircleS}{\textcolor{blue}{\Large{\ensuremath{\bullet}}}}
\newcommand{\greencircleS}{\textcolor{Green}{\Large{\ensuremath{\bullet}}}}

\newcommand{\soc}{\ensuremath{\theta}\xspace}

\usepackage{todonotes}
\usepackage{subcaption}

\usetikzlibrary{positioning,arrows,calc,fit}
%========================================
% Standard Layout
%========================================

\usepackage{bigints}
\usepackage{MnSymbol}

\usepackage{titlesec}

\usepackage{geometry}
 % \geometry{
 % a4paper,
 % left=300mm,
 % right=250mm,
 % top=400mm,
 % bottom=400mm
 % }

\pagenumbering{gobble} 
% left justification:
\raggedright

%\usepackage{authblk}

% misc
\usepackage{graphicx}
\usepackage{babel}
\usepackage{amsmath}

\makeatletter
\makeatother

% Customisations specific to Philosophy of Science:

% paper size, margins, etc:
\usepackage{geometry}
\geometry{verbose,letterpaper,tmargin=1.5in,bmargin=1.5in,lmargin=1.25in,rmargin=1in}

\doublespacing

\usepackage[T1]{fontenc}


% Itemize
\renewcommand{\labelitemi}{\large{$\mathbf{\cdot}$}}    % itemize symbols
\renewcommand{\labelitemii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiv}{\large{$\mathbf{\cdot}$}}
% Description
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\textsc{#1}}

\renewcommand{\footnotesize}{\normalsize}

\usepackage{setspace} % load setspace before footmisc

\usepackage{footmisc}
\setlength{\footnotesep}{\baselineskip}

\usepackage{setspace,caption}
\AtBeginCaption{\doublespacing} 

\renewcommand{\footnotelayout}{\doublespacing}


% Figure Captions
\usepackage{caption} % use corresponding myfiguresize!
\setlength{\captionmargin}{20pt}
\renewcommand{\captionfont}{\normalsize}
\setlength{\belowcaptionskip}{7pt} % standard is 0pt

\newcommand{\myalert}[1]{\textcolor{red}{#1}}

\title{Smart Representations: {R}ationality and Evolution in a Richer  Environment}

\date{}

%\titleformat{\subsection}
%       {\normalfont\fontfamily{phv}\fontsize{12}{17}\itshape}{\thesubsection}{1em}{}
     
     
\usepackage{titlesec}
\titleformat*{\section}{\bf}
\titleformat*{\subsection}{\it}
\titleformat*{\subsubsection}{\it}

% indentation
\setlength\parindent{12pt}

\begin{document}

\maketitle

\begin{abstract}
\normalsize
  Standard applications of evolutionary game theory look at a single, fixed game and focus on
  the evolution of behavior for that game alone. Instead, this paper uses tools from
  evolutionary game theory to study the evolutionary competition between \emph{choice
    mechanisms} in a rich and variable environment. A choice mechanism is a way of representing
  a decision situation, paired with a method for choosing an act based on this 
  subjective representation. We demonstrate the usefulness of this approach by a case study
  that compares a number of classic proposals for decision making. Our
  main result is that regret-based preference representations can be evolutionarily selected
  for, despite being non-veridical.
\end{abstract}

\newpage

\section{Introduction}
\label{sec:intr--motiv}

% \begin{quotation} \textit{This is what I aim at, because the point of philosophy is to start with something so simple as not to seem worth stating, and to end with something so paradoxical that no one will believe it.} \citep{Russ18} \end{quotation}

%Evolutionary game theory has become an established philosophical tool for probing into
%conceptual issues whose complexity requires mathematical modelling at a high level of
%abstraction.  Abstraction entails simplifying assumptions. 

If agents deal with a rich and variable environment, they have to face many different choice
situations.
Standard evolutionary game models frequently
simplify reality in two ways that are relevant for present purposes. Firstly, the environment
is represented as a fixed \emph{stage game}; secondly, the focus of
evolutionary selection is behavior for that stage game alone. In contrast, some argue for
studying the evolutionary competition of general \emph{choice mechanisms} in a rich and
variable environment
\citep[e.g.,][]{FawcettHamblin2013:Exposing-the-be,McNamara2013:Towards-a-Riche,HammStev12}. To show that
there is no tension here, this paper introduces a method for reconciling established notions of
evolutionary game theory with evolutionary selection of choice mechanisms in variable
environments \citep[see, inter alia, related work
by][]{Harley1981:Learning-the-Ev,ZollmanSmead2010:Plasticity-and-,SmeadZollman2013:The-Stability-o,OConnor2016:Evolving-to-Gen}.

In full generality, a choice mechanism associates decision situations with action choices. A crucial
part of a choice mechanism is the \emph{subjective representation} of the decision situation,
in particular the manner of forming preferences and beliefs about a possibly uncertain world (see
figure~\ref{fig:ChoiceMechanism} below). This paper asks: which preference and belief
representations are ecologically valuable and lead to high fitness?
The evolution of
preferences has been subject of recent interest in theoretical economics
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. A contribution of this paper is
to provide a case that questions of preference evolution should take
variability in uncertainty representation into account as well. As a point in case, we demonstrate
that if agents have imprecise probabilistic beliefs \citep[e.g.][]{gardsah82,levi74,walley96},
preference representations in terms of regret can be selected for, even though they may deviate
from the true, objective fitness that evolutionary selection operates on.

% To show this, we study the evolutionary competition of choice mechanisms, or more
% specifically the competition of subjective representations of choice situations, in a
% variable environment. Concretely, we look at evolutionary ``meta games'' whose ``acts'' are
% choice mechanisms and whose ``payoffs'' represent the average expected fitness that different
% choice mechanisms would accrue when playing arbitrary games (from a given class, with a given
% occurrence probability). In this sense, a meta game captures (the modeller's) assumptions
% about the relevant statistics of the environment in which evolutionary forces operate, while
% we are still able to use standard methods from evolutionary game theory, like stability or
% evolutionary dynamics.

Section \ref{sec:rati--subj} reviews different views of rational choice and relates the
approach we take here to the existing literature. Section \ref{sec:basic-notions} introduces the
relevant decision- and game-theoretic notions, and
%spells out four choice mechanisms on whose
%evolutionary competition we zoom in 
Section~\ref{sec:basic-model-1} presents the
main results of the paper. Section \ref{sec:extensions} extends the evolutionary analysis by
introducing more choice mechanisms and alternative models of the environment.


\section{Rationality, Representations and Meta-games}
\label{sec:rati--subj}

The standard textbook definition of \textit{rationality} in economics and decision theory
traces back to the seminal work by \citet{deFinetti37}, \citet{Neumannvon-NeumannMorgenstern1944:Theory-of-Games}
and \citet{Savage1954:The-Foundations}. It says that a choice is rational only if it maximizes
(subjective) expected utility.

\begin{definition}[Rational choice from maximization of expected utility]
  \label{def:rationality}
  Let $S$ be a (finite) set of states and $A$ a (finite) set of actions. Given a
  probability measure $\mu$ over $S$ and a payoff function
  $u \mycolon S \times A \rightarrow \mathbb{R} $, an action $a^* \in A$ is rational
  if it maximizes the \emph{(subjective) expected utility}
  $\EU(a, \mu) := \sum_{s \in S} \mu(s) \ u(s, a)$.
\end{definition}

\noindent Expected utility is subjective in the sense that it is a function of subjective
beliefs $\mu$ and subjective preferences $u$ of the decision maker (DM). To wit, a choice can be
rational, i.e., the best choice from the DM's point of view, even if based on peculiar beliefs
and/or aberrant preferences. % (Indeed, while
% \citeauthor{Neumannvon-NeumannMorgenstern1944:Theory-of-Games} assumed that probabilities were
% objectively given, it was \citet{Savage1954:The-Foundations} who famously extended von Neumann
% and Morgenstern's axiomatization to include a subjective notion of probabilistic beliefs on top
% of the subjective notion of preference that von Neumann and Morgenstern started out with.)

If beliefs and preferences are subjective, there is room for \emph{rationalization} or
\emph{redescriptionism} of observable behavior. For example,
\citet{KahnemannTversky1979:Prospect-Theory} famously demonstrated that what appear to be
violations of rationality norms in human decision making can be explained on the assumption
that subjects' beliefs and preferences deviate systematically from the objectively given
parameters in the presented choice task. Similarly for social decision making, including social
preferences such as fairness, altruism or similar, allows us to describe as rational
empirically observed behavior, such as in experimental Prisoner's Dilemmas or public goods
games, that might otherwise appear irrational \citep[e.g.,][]{fehrschmidt99,charrab02}.

The main objection to redescriptionism is that, without additional constraints, the notion of
rationality is likely to collapse, as it seems possible to deem rational almost everything that
is observed, given the freedom to adjust beliefs and preferences at will. \emph{Normativism}
therefore emphasizes that there are many ways in which ascriptions of beliefs and preferences
should be constrained by normative considerations of rationality as well. As for beliefs, where
possible, subjective beliefs should reflect objective chance. 
%Where this is impossible,
%temporal consistency of choices constrains rational subjective beliefs, else exploitation by
%Dutch books is possible. 
As for preferences, in certain situations it may make sense to
postulate on normative grounds that subjective preferences should be oriented towards tracking
objective fitness. For instance, profit maximization seems a necessary requirement for
evolution in a competitive market because only firms behaving according to profit maximization will survive in the long run \citep[e.g.,][]{alch50,Fried53}.

An alternative view on rationality of choice is \emph{adaptationism}
\citep[e.g.,][]{Anderson1991:Is-human-cognit,ChaterOaksford2000:The-Rational-An,HagenChater2012:Decision-Making}. Adaptationism
aims to explain rational behavior by appeal to evolutionary considerations: DMs have acquired
choice mechanisms that have proved to be adaptive with respect to the variable environment
where they have evolved. A choice mechanism can be a set of distinct heuristics (the DM's
\emph{adaptive toolbox}) that have little in common
\citep[e.g.,][]{TverskyKahnemann1981:The-Framing-of-,GigerenzerGoldstein1996:Reasoning-the-F,ScheibehenneRieskamp2013:Testing-the-Ada}. But
to closely relate to the literature on evolution of preferences and to the philosophical debate
about the nature of rational choice in general, we here suggest to think of a choice mechanism
as a map from choice situations to action choices which includes an explicit level of
subjective representation of the situation, that we call \emph{subjective representation
  scheme} (see figure~\ref{fig:ChoiceMechanism}). Specifically, a subjective representation
scheme is a general way of forming preferences and beliefs about the choice situation, and we
are most interested in the question which subjective representations are better than others
from an evolutionary point of view.

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 2.25cm]

  \begin{scope}

    \node[] (environment) {};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = environment, node distance = 2cm] (situation)
    {environment produces random \\ \textbf{decision situation} $D$};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = situation] (representation) 
    {\textbf{subjective representation} of $D$ \\ \textcolor{gray}{[preference, beliefs, \dots]}};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = representation] (action)
    {\textbf{action choice} $a$};

    \node[rounded corners, draw, rectangle, minimum height = 0.75cm, text width=6cm, text
    centered, below of = action,node distance = 1.5cm] (payoff) {\textbf{payoff} for $a$ in $D$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.5cm]situation.center)--([xshift=-1.75cm,yshift=0.55cm]representation.center)
    node[midway, right] {subj.~representation scheme $\gamma$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.55cm]representation.center)--([xshift=-1.75cm,yshift=0.5cm]action.center)
    node[midway, right] {action selection function $\kappa$};

    \draw[->, thick]
    ([xshift=-1.75cm,yshift=-0.55cm]action.center)--([xshift=-1.75cm,yshift=0.5cm]payoff.center)
    node[midway, right] {};

    \node[minimum height = 0.75cm, text width=6cm, text
    centered, right of = representation, xshift = 2cm, rotate = 90] (mechanism) {choice mechanism};

    \begin{pgfonlayer}{background}
             \node [rounded corners, dashed,
       very thick,draw=black!40,fit={($(situation.south)+(0,-10pt)$) 
                                     ($(mechanism.west)+(+5pt,+50pt)$) 
                                     % ($(action.north)+(0,+10pt)$) 
                                     ($(representation.west)+(-7pt,0)$)
                                   }] (box) {};
    \end{pgfonlayer}

  \end{scope}



\end{tikzpicture}
  \caption{Schematic representation of action choice. A choice mechanism maps decision
    situation $D$ to action choice $a$ by means of a representation of $D$ in terms of a subjective
    preference and a subjective belief.}
  \label{fig:ChoiceMechanism}
\end{figure}


The fitness of a subjective representation scheme is measured in terms of the fitness of a
choice mechanism that contains it, all else equal. The fitness of a choice mechanism $c$ is the
expected payoff of $c$ over arbitrary decision situations $D$:
\begin{align}
  \label{eq:FittnessChoiceMechSolitary}
  F(c) = \int P(D) \  \pi_D(c) \text{ d} D \,,
\end{align}
where $P(D)$ is the probability (density) of $D$ occurring and $\pi_D(c)$ is the evolutionary payoff of
the act selected by $c$ in $D$. The probability measure $P(D)$ encodes the statistical
properties of the environment.

The same logic also applies to game contexts and social decision making. A game $G$ is also a
choice situation, albeit a more complex one. A choice mechanism $c$ maps $G$ onto an action
choice in $G$. The fitness of $c$ is then:\footnote{We assume for simplicity of exposition that
  all games with positive occurrence probability are symmetric two-player games.}
\begin{align}
  \label{eq:FittnessChoiceMechGame}
  F(c) = \int \int P(G) \  P(c') \  \pi_G(c,c') \text{ d} c' \text{ d} G  \,,
\end{align}
where $P(G)$ is the probability (density) of game $G$, $P(c')$ is the probability of
encountering a co-player who uses choice mechanism $c'$ and $\pi_G(c,c')$ is the evolutionary payoff for
a $c$-player in $G$ against the $c'$-player, given by the actions that $c$
and $c'$ select in $G$. For the most part, we will focus on the more complex case of game situations. Results about solitary decision making follow from parallel arguments
(see Section~\ref{sec:extensions}).

\emph{Meta-games} are abstract models for the evolutionary competition between choice
mechanisms in interactive decision making contexts. The expected payoff for a $c$-player in a random
encounter with a $c'$-player is the average of payoffs obtained in a random symmetric game $G$ weighted
by $G$'s occurrence probability:
\begin{align}
  \label{eq:FittnessChoiceMechGamePairwise}
  F(c, c') = \int P(G) \  \pi_G(c,c') \text{ d} G \,.
\end{align}

\begin{definition}[Meta-game]
  \label{def:MetaGame}
  Let $CM$ be a set of choice mechanisms and let $P(G)$ give the occurrence probability of symmetric game
  $G$ in the environment. The meta-game that captures selection of choice mechanisms in this
  environment is the one-population evolutionary game with strategies $CM$ and payoff function
  $\pi \mycolon CM \times CM \rightarrow \mathbb{R}$ such that
  $\pi(c,c') = F(c,c')$.
  % = \int P(G) \  \pi_G(c,c') \text{ d} G$.
\end{definition}

Standard notions of evolutionary game theory can be applied to meta-games as well. A choice
mechanism $c$ is a strict Nash equilibrium if $F(c,c) > F(c',c)$ for all $c'$; it is
evolutionarily stable if for all $c'$ either (i) $F(c,c) > F(c',c)$ or (ii) $F(c,c) = F(c',c)$
and $F(c,c') > F(c',c')$; it is neutrally stable if for all $c'$ either (i) $F(c,c) > F(c',c)$
or (ii) $F(c,c) = F(c',c)$ and $F(c,c') \ge F(c',c')$
\citep{Maynard-Smith1982:Evolution-and-t}. Similarly, evolutionary dynamics can be applied to
meta-games. We will later turn towards replicator dynamics
\citep{TaylorJonker1978:Evolutionary-St} and replicator mutator dynamics
\citep[e.g.][]{Nowak2006:Evolutionary-Dy}.
%\todo{footnote with caveat about agent-level processes}




\section{Classic Choice Rules and Choice Mechanisms}
\label{sec:basic-notions}

We submit that meta-games are a useful tool for structuring and sharpening conceptual inquiry
in a domain that is notorious for its fuzziness and elusiveness. To show how this approach can
be applied to the benefit of conceptual debate, we consider the evolution of preferences
\citep[e.g.,][]{algweib13,DekElyYlan07,RobSam11}. 
%Concretely, we here pit two different ways of
%representing preferences against each other: (i) veridical representations that correctly
%reflect objective fitness, and (ii) non-veridical subjective preferences in terms of regret
%\citep[e.g.,][]{Savage1951:The-theory-of-s,}. 
We show that subjective, non-veridical preference
representations based on a notion of regret can be adaptive and even outperform objective representations of the true evolutionary fitness, when agents, at least on occasion, hold
\emph{imprecise probabilistic beliefs} \citep[e.g.,][]{gilsch89,levi74,gardsah82} and make
decisions based on a security choice rule (here: the maxmin rule).\footnote{Decision-theoretic background and justifications for the use of maxmin and imprecise probabilities is given in Appendix~\ref{sec:impr-prob-beli}.}

The case study under consideration strips the generality of a meta-game approach down to a perspicuous
comparison between four choice mechanisms, summarized in table~\ref{tab:CMs}.
% the cross-product of two different preference representations (veridical vs.~regret) and two
%different belief representations (precise vs.~imprecise). 
\begin{table}[t]
  \centering
  \begin{tabular}{cccc}
  \multicolumn{3}{c}{choice mechanism} \\  \cmidrule(r){1-3}
    \multicolumn{2}{c}{subjective representation} & action selection & corresponding classic rule \\  \cmidrule(r){1-2}
    preference type & belief type  \\ \midrule
    objective fitness & precise & maxmin EU &  Laplace rule \\
    objective fitness & imprecise & maxmin EU & maxmin \\ 
    regret & precise & maxmin EU & Laplace rule \\
    regret & imprecise & maxmin EU & regret minimization \\ 
  \end{tabular}
  \caption{Overview relevant choice mechanisms and the corresponding (behaviorally equivalent)
    classic choice rules.}
  \label{tab:CMs}
\end{table}
\iffalse
This exclusive selection of choice mechanisms is motivated by a number of
considerations. Firstly, the set of conceivable choice mechanisms is vast. Most of that
conceivable vastness is utter nonsense, and some selection of plausible candidates is required in
any case. Secondly, a restriction to only a small number of suitable candidates will
help demonstrate more distinctly how a meta game approach may help structure philosophical
inquiry, which is our main goal. Thirdly, our case study adds to the recent literature on the
evolution of preferences by making evident the possibly non-trivial interaction between
preference and belief representations, all else equal. This is our secondary goal. 
Finally,
\fi 
The
four choice mechanisms,  
%that we look at here
although phrased in terms of preference and belief representations, map onto well-known classic
proposals for rational decision making (see table~\ref{tab:CMs}). In other words, there is also
motivation from historical relevance for our selection. To see this, we first recapitulate the
relevant classics ---to have a handy label, we call them \emph{classic rules}--- and then show
how they relate to our notion of choice mechanism.

\subsection{Classic Rules for Decision Making}
\label{sec:choice-rules}

The classic literature on rational choice contains a number of proposals for which acts a
player should choose if she is rational. Maximization of expected utility, as
given in Definition~\ref{def:rationality}, is just one of them, possible only if DM holds a precise probabilistic belief.
Here, we consider three others:
(i) maxmin, (ii) Laplace rule, and (iii) regret minimization. 
%All of these apply to games and
%solitary decision situations.
We focus on games first, but definitions carry over straightforwardly to solitary decisions.

A strategic-form game is a tuple $ G=\tuple{ N, (A_i , \pi_i)_{i \in N} }$ with $N$ a finite set of
players, $A_i$ a finite set of acts and $\pi_i: \bigtimes_{j \in N} A_j \rightarrow \mathbb{R}$ a payoff function for each player $i \in N$.
Part of this paper will focus on symmetric games with two players. A two-player game is
symmetric if players have the same action set, $A_1 = A_2$, and payoffs are symmetric:
$\pi_1(a_k, a_j) = \pi_2(a_j, a_k)$ for all $a_j,a_k \in A_1$. An example is given in
figure~\ref{coordgame1}. The numbers give the payoff for the player choosing a row act against
a co-player choosing a column act. Since the game is symmetric, it suffices to give just these
numbers.

Rational choice from maximization of expected utility, as given in
Definition~\ref{def:rationality}, applies to games as follows. To say which acts are rational,
it requires specifying a precise probabilistic belief of DM. Let $A_{-i} := \bigtimes_{j \neq i} A_j$ be
the set of all \emph{profiles} of acts for all players except $i$. If player $i$ has a belief
$\mu_i \in \Delta(A_{-i})$ about the behavior of his co-players, he chooses rationally only if
his choice $a^*_i$ maximizes expected utility
$\EU(a_i, \mu) := \sum_{\act_{-i} \in \Acts_{-i}} \mu_i(\act_{-i}) \myts \pi_i(a_i, a_{-i})$. For
the example in figure~\ref{coordgame1}, an expected utility maximizer would choose act $I$ only if he
believes that his co-player is going to choose act $I$ with a probability of at least
$\nicefrac{2}{3}$.

Perhaps the most well-known classic rule that does not require precise probabilistic
beliefs is the \emph{maxmin rule}. It prescribes to choose act $\act^*$ only if it
maximizes payoffs for all worst case scenarios:
$\act^* \in \argmax_{a_i \in \Acts_i} \min_{a_{-i} \in \Acts_{-i}} \pi_i(a_{i},
a_{-i})$.
For the example in figure~\ref{coordgame1}, the minimum of both acts is 0: players choosing by maxmin
rule would be indifferent between acts $I$ and $II$.

\emph{Laplace rule} selects $\act^*$ if
$\act^* \in \argmax_{a_{i} \in \Acts_{i}} \sum_{a_{-i}} \pi(a_{i}, a_{-i})$.
Laplace rule is equivalent to expected utility maximization under a \emph{flat belief} that
assigns equal probability to all of the co-players' choices. In the coordination game of
figure~\ref{coordgame1}, Laplace rule selects act $II$.

The final classic rule that is relevant for our purposes is \emph{regret
  minimization}. The notion of regret in decision theory dates back at least to the work by
\citet{Savage1951:The-theory-of-s}, and has later been developed by \citet{bell82}, \citet{fish82} and \citet{loosug82} independently. Lately,
\citet{HalpernPass2012:Iterated-Regret} showed how the use of regret minimization can give
solutions to game theoretical-puzzles (like the Traveller's dilemma and the Centipede game) in a way that is closer to
everyday intuition and empirical data. In this paper the notion of regret is defined as in
\citet{HalpernPass2012:Iterated-Regret}:

\begin{definition}[Regret, Regret Minimization] \label{defn:regret} Given a game
  $ G=\langle N, (A_i , \pi_i)_{i \in N} \rangle $, the \emph{pairwise regret} of player $i$'s
  act $a_i$ against profile $a_{-i}$ is
  $\text{preg}(a_i,a_{-i}):= \pi_i(a_i^\$,a_{-i})-\pi_i(a_i,a_{-i}) $, where
  $a_i^\$ \in \argmax_{a_i \in A_i} \pi_i(a_i, a_{-i})$ denotes $i$'s best reply to the
  co-players' actions $a_{-i}$. The \emph{regret} of $a_i$ is then
  $\text{reg}(a_i):= \text{max}_{a_{-i}\in A_{-i}} \text{preg}(a_i,a_{-i}) $. An
  action $a^{*}_i $ is regret minimizing if
  $a^{*}_i \in \text{argmin}_{a_i} \text{max}_{a_{-i}} \text{preg}(a_i,a_{-i})$.
\end{definition}

\noindent To illustrate, figure~\ref{coordgame1reg} shows the pairwise regrets and the regrets for the game of figure~\ref{coordgame1}. 
We have
$\text{reg}(I)=2$, because $\text{preg}(I,II)=2$, whereas $ \text{preg}(I,I)=0 $. With the same reasoning, $\text{reg}(II)=1$. A regret minimizer would therefore choose act $II$.


\begin{figure}

  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 1 & 0 \\
      II & 0 & 2\\
      \bottomrule
    \end{tabular}
    \caption{Coordination game $G^C$}
    \label{coordgame1}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \begin{tabular}{ccc|c}
      \toprule
      & $\text{preg}( \cdot, I)$ & $\text{preg}(\cdot, II)$ & $\text{reg}(\cdot)$ \\
      \midrule
      I  & 0 & 2 & 2 \\ 
      II & 1 & 0 & 1\\
      \bottomrule
    \end{tabular}
    \caption{Pairwise regrets and regrets}
    \label{coordgame1reg}
  \end{subfigure}
  \caption{A coordination game (left) and regrets associated with the row player's acts (right).}
    \label{coordgame1mainFig}
\end{figure}



% \begin{example}[The Traveller's Dilemma \citep{HalpernPass2012:Iterated-Regret}]
% \label{example:TravelersDilemma}
% Let us now consider the Traveller's Dilemma case. The story behind the game goes as
% follows. There are two travellers, Ann and Bob, who lost their luggages. Ann and Bob had
% exactly the same luggage and they had insured the luggages with the same insurance company. The
% insurance policy is that the two travellers have to separately claim a certain amount between
% 2\$ and 100\$ for the reimbursement, and if they both claim the same amount then they will be
% reimbursed by that amount. Otherwise, if one of them claims more than the other, then the one
% who claimed the higher amount will get the lower amount minus 2\$, while the one who claimed
% the lower amount will get what (s)he claimed plus 2\$. The game is symmetric, and the payoff
% function for both players is:
% \begin{align*}
%   \pi_i(a_i,a_{-i}) = \begin{cases} a_i \text{ \hspace{1cm} if } a_i = a_{-i} \\ a_i + 2 \text{
%       \hspace{.4cm} if } a_i < a_{-i} \\ a_{-i} -2 \text{ \hspace{.25cm} if } a_{-i} <
%     a_i \end{cases}
% \end{align*}
% By a simple computation we have:  $\forall a_i \in \lbrace 96\$, ..., 100\$ \rbrace \text{, }
% \text{reg}(a_i)=3 $ and $\forall a_i \in \lbrace 2\$, ..., 95\$ \rbrace \text{, }
% \text{reg}(a_i)>3$. If we now eliminate all the actions that do not minimize regret in the
% first place and iterate the regret minimization only on the action set $\lbrace 96\$, ...,
% 100\$ \rbrace$, we get that 97\$ is the action that minimize regret. Indeed, $\text{reg}(97)=2$
% and $\forall a_i \in \lbrace 96\$, 98\$, 99\$, 100\$ \rbrace \text{, } \text{reg}(a_i)=3
% $. Hence, one step of regret minimizing reasoning eliminates all the actions smaller than 96\$,
% and if we iterate the process once more the unique outcome is 97\$. Finally, it is worth
% noticing that the only rationalizable equilibrium of the game corresponds to the outcome
% $(2,2)$, that is consequently the only Nash equilibrium, the only perfect equilibrium, and the
% only sequential equilibrium of the game.

% \begin{itemize}
% \item \textcolor{red}{add here:}
%   \begin{itemize}  
%   \item outcome of rationalizability (max-EU)
%   \item outcome of maxmin rule
%   \item outcome of Laplace rule
%   \end{itemize}

% \end{itemize}

%  $ \medsquare $
  
% \end{example}

Regret minimization is intimately related to the maxmin rule. Let the \emph{negative regret
  transformation} of a strategic game $G$ be the game $G^r$ derived from $G$ by replacing the payoff
functions $\pi_i$ of $G$ with the negative pairwise regrets $-\text{preg}(a_i,a_{-i})$. For
example, the negative regret transformation of the game in figure~\ref{coordgame1} would have the
following payoff matrix, i.e., the negatives of the pairwise regrets given in
figure~\ref{coordgame1reg}:

\begin{center}
    \begin{tabular}{ccc}
      \toprule
      & I & II \\
      \midrule
      I & 0 & -2 \\
      II & -1 & 0\\
      \bottomrule
    \end{tabular}
\end{center}

\noindent Then, for any game $G$, acts selected by maxmin rule for the negative regret transformation of $G$ are
always exactly the acts selected by regret minimization for the original game $G$.
This means that we can think of the maxmin rule and regret minimization as essentially the same
action selection function on different subjective representations of preferences: while maxmin considers actual
objective payoffs, regret minimization considers non-veridical, regret-based subjective
utilities. As the example of figure~\ref{coordgame1} shows, different subjective preferences
can give rise to different choices, all else equal. While maxmin is indifferent
between $I$ and $II$, regret minimization uniquely selects $II$.

In sum, classic proposals for rational decision making are normative prescriptions of action choices
for DM. Such prescriptions may rest on implicit assumptions about what DM
believes (e.g., when interpreting Laplace rule as expected utility maximization under flat beliefs). They may
also make implicit assumptions about what DM prefers (e.g., when considering regret-based preferences). This is rather messy. To disentangle, we rephrase all the above in terms of choice mechanisms.

\subsection{Choice Mechanisms}
\label{sec:choice-mechanisms}

A \emph{choice mechanism} is a map from choice situations to action choices that contains an explicit level, in terms of preferences and beliefs, of subjective representation of
the choice situation. Concretely, let us think of a choice mechanism as a pair of functions
($\gamma,\kappa$), where $\gamma$ is a \emph{subjective representation scheme} and $\kappa$ is
an \emph{action selection function} (see figure~\ref{fig:ChoiceMechanism}).

The subjective representation scheme $\gamma_i$ of agent $i$ is a function that
takes a game $G = \tuple{ N, (A_i , \pi_i)_{i \in N} }$ and maps it onto
$\gamma_i(G) = (\theta, e)$ where
$\theta \mycolon \bigtimes_{j \in N} A_j \rightarrow \mathbb{R}$ is a subjective utility function (the \emph{preference type} of the player), and $e$ is some subjective representation of a player's uncertainty about the co-players'
behavior (the \emph{epistemic} or \emph{belief type} of the player). As already anticipated, we mostly consider two preference types here: the \emph{objective type} maps each game
onto its own, true evolutionary payoffs (i.e., $ \theta = \pi_i $); the \emph{regret type} maps each game onto its negative regret
transformation (i.e., $ \theta = -\text{preg} $). We also consider two belief types: the
\emph{precise type} represents uncertainty about the co-players in a single probability measure
$\mu \in \Delta(\Acts_{-i})$; the \emph{imprecise type}
represents uncertainty as a set of probability measures $\Gamma \subseteq \Delta(\Acts_{-i})$.
It is convenient to conceptualize precise beliefs as a special case of imprecise beliefs by
looking at $\mu$ as a stand-in for the singleton set $\set{\mu}$.
The action selection function $\kappa_i(\gamma_i(G)) \subseteq A_i$ that we consider here is \emph{maximinimization of expected utility} (see Appendix~\ref{sec:impr-prob-beli} for motivations).

\begin{definition}[Maximinimization of expected utility]
  \label{def:maxminSEU}
  Fix a game $G$. Let agent $i$'s subjective representation scheme
  $\gamma_i(G) = (\theta, e)$ be such that $\theta$ is $i$'s subjective utility function and $e \subseteq \Delta(\Acts_{-i})$ is a (possibly singleton) set of probability measures. Agent $i$'s expected utility for act $a_{i}$ and belief
  $\mu \in e$ is
  $\EU(a_{i}, \mu) = \sum_{a_{-i} \in \Acts_{-i}} \mu(a_{-i}) \theta(a_{i},
  a_{-i})$.
  Act $a_{i}^*$ is selected by maximinimization of expected utility iff
  $a_{i}^* \in \arg \max_{a_{i} \in \Acts_{i}} \min_{\mu \in e} \EU(a_{i}, \mu)$.
\end{definition}

\iffalse
\begin{fact} \label{fact:singleton probability set}
Maximinimization under a precise (singleton set) belief is just standard
maximization of expected utility (Definition~\ref{def:rationality}).
\end{fact}
\fi

\begin{fact} \label{fact:maxEU-minReg} 
%Under an identical precise (singleton set) belief, the
%  acts selected by maximization of expected evolutionary payoff in game $G$ are exactly the acts selected
%  by the maximization of expected utility for the negative regret transformation of $G$
Under an identical precise (singleton set) belief, maximization of expected evolutionary payoff and minimization of expected regret coincide. %\citep[e.g.,][]{HalpernPass2012:Iterated-Regret}.
\end{fact}

\noindent This means that objective and regret types are behaviorally indistinguishable, as long as both hold precise beliefs. 
%This will be central in understanding later results. 
On the other hand, if paired with imprecise beliefs, different behavioral predictions
arise.

In the following, we assume that agents are maximally uncertain, so that precise types have a
(singleton set containing a) uniform probability measure
$\overline{\mu} \in \Delta(\Acts_{-i})$ and imprecise types entertain all probabilistic beliefs
about the co-players' behavior $\overline{\Gamma} = \Delta(\Acts_{-i})$. In that case, the
considered choice mechanisms are behaviorally equivalent to classic choice rules, as summarized
in table~\ref{tab:CMs}. This means that most of the following results which we interpret as results about the evolutionary fitness of different subjective representations are also relevant for a comparison of the ecological value of classic choice
rules.

Under maximal uncertainty, all four choice mechanisms considered here are behaviorally
equivalent on $2 \times 2$ symmetric games, except for objective types with
imprecise beliefs:

\begin{fact} \label{fact:equivalence2x2} In the class of $2 \times 2$ symmetric games, the acts
  selected by Laplace rule are exactly the acts selected by regret minimization.
\end{fact} 


\section{The Model}
\label{sec:basic-model-1}

\subsection{Simulation Results}
\label{sec:simulation-results}

To demonstrate the usefulness of the meta-game approach, let us concentrate first on a basic
model that contains the four choice mechanisms from table~\ref{tab:CMs}. From now on we speak
of types when we mean pairs $(\text{preference type}, \text{belief type})$, like $(\text{reg}, \text{imp})$ or $(\text{obj}, \text{prc})$.

Meta-games factor in statistical properties of the environment. For particular empirical
purposes, one would consult a specific class of games $\mathcal{G}$ with appropriate,
empirically informed probabilities $P(G)$ in order to match the natural environment of a given
population. 
%But for our theoretical purposes we adopt another approach. 
For our theoretical purposes, let $\mathcal{G}$ be a set of
symmetric two-player games with two acts for a start. A game is then individuated solely by its payoff
function, which is a matrix with four numbers. As for occurrence probabilities of
games, we imagine that entries in a game's matrix are i.i.d.~random variables sampled
uniformly from set $ \lbrace 0, \dots, 10 \rbrace$.  Using Monte Carlo simulations, we can then
approximate the values of Equation~(\ref{eq:FittnessChoiceMechGamePairwise})
to construct meta-game payoffs. Results based on $100,000$ randomly sampled games are given in
table~\ref{tab:ExpectedFitness_4Types}.\footnote{Concretely, $100,000$ games were sampled
  repeatedly by choosing independently four integers between 0 and 10 uniformly at random. For
  each game, the action choices of all four choice mechanisms were determined and payoffs from
  all pairwise encounters recorded. (Whenever a choice mechanism would not select a unique
  action in a given game, we recorded unbiased averages over all selected actions.)  The number
  in each cell of table~\ref{tab:ExpectedFitness_4Types} is the average payoff for the choice
  mechanism listed in the row when matched with the choice mechanism in the column.}



\begin{table}[t]
\centering
\begin{tabular}{ccccc}
  \toprule
 & $\text{reg}, \text{imp}$ 
 & $\text{obj}, \text{imp}$ 
 & $\text{reg}, \text{prc}$ 
 & $\text{obj}, \text{prc}$ \\ 
  \midrule
  $\text{reg}, \text{imp}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
  $\text{obj}, \text{imp}$ & 6.486 & 6.484 & 6.486 & 6.486 \\ 
  $\text{reg}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\  
  $\text{obj}, \text{prc}$ & 6.663 & 6.662 & 6.663 & 6.663 \\ 
   \bottomrule
\end{tabular}                    
\caption{Average evolutionary fitness from Monte Carlo simulations of 100,000 symmetric $2 \times 2$ games}
\label{tab:ExpectedFitness_4Types}
\end{table}

Simulation results obviously reflect Fact~\ref{fact:equivalence2x2} in that all encounters in
which types $(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ or
$(\text{obj}, \text{prc})$ are substituted for one another yield identical results. More
interestingly, table~\ref{tab:ExpectedFitness_4Types} shows that $(\text{obj}, \text{imp})$,
the maxmin strategy, is strictly dominated by the three other types: in each column (i.e.,
for each kind of co-player), the maxmin strategy is strictly worse than any of the other three
competitors. This has a number of interesting consequences.

If we restrict attention to only imprecise belief types, then a monomorphic state in which every agent has
regret-based preferences is the only \emph{evolutionarily stable state}. More strongly, since
$(\text{obj}, \text{imp})$ is strictly dominated by $(\text{reg}, \text{imp})$, we expect
selection that is driven by (expected) fitness to invariably weed out maxmin players $(\text{obj}, \text{imp})$ in favor of $(\text{reg}, \text{imp})$, regret minimization. 
In other
words, over the class of games considered here, regret
minimization is a better classic rule than maxmin from an evolutionary point of
view.

Next, if we look at the competition between all four types represented in
table~\ref{tab:ExpectedFitness_4Types}, $(\text{reg}, \text{imp})$ is no longer evolutionarily
stable. Rather, given behavioral equivalence (Fact~\ref{fact:equivalence2x2}), types
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$, and $(\text{obj}, \text{prc})$ are all
\emph{neutrally stable} \citep{Maynard-Smith1982:Evolution-and-t}. But since
$(\text{obj}, \text{imp})$ is strictly dominated and so disfavored by fitness-based selection,
we are still drawn to conclude that maxmin behavior is weeded out in favor of a population with
a random distribution of the remaining three types.

Simulation results of the (discrete time) \emph{replicator dynamics}
\citep{TaylorJonker1978:Evolutionary-St} indeed show that random initial population
configurations are attracted to states with only three player types:
$(\text{reg}, \text{imp})$, $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. The
relative proportions of these depend on the initial shares in the population. This variability disappears if
we add a small mutation rate to the dynamics. Take a fixed, small mutation rate $\epsilon$ for
the probability that a player's preference type \emph{or} her belief type changes to another
preference or belief type. The probability that a player's type randomly mutates into a
completely different type with altogether different preference type and belief type would
then be $\epsilon^2$. With these assumptions about ``component-wise mutations'', numerical
simulations of the (discrete time) \emph{replicator mutator dynamics}
\citep{Nowak2006:Evolutionary-Dy} show that already for very small mutation rates almost all
initial population states converge to a single fixed point in which the majority of players are
regret types. For instance, with $\epsilon = 0.001$, almost all initial populations are
attracted to a final distribution with proportions:

\begin{center}
  \begin{tabular}{cccc}
    $(\text{reg}, \text{imp})$ & $(\text{obj}, \text{imp})$ & $(\text{reg},
      \text{prc})$ & $(\text{obj}, \text{prc})$ \\ \hline
    0.289  & 0.021 &   0.398 &    0.289 
  \end{tabular}
\end{center}

What this suggests is that, if biological evolution selects behavior-generating mechanisms, not
behavior as such, it need not be the case that behaviorally equivalent mechanisms are treated
equally all the while. If mutation probabilities are a function of individual components, it can be the case that
certain components of such behavior-generating mechanisms are more strongly favored by a process of random mutation and
selection. This is exactly the case with regret-based preferences. Since regret-based preferences are much better in connection with imprecise
beliefs than veridical preferences are, the proportion of expected regret minimizers,
$(\text{reg}, \text{prc})$, in the attracting state is substantially higher than that of
expected utility maximizers, $(\text{obj}, \text{prc})$, even though these types are
behaviorally equivalent.

\subsection{Analytical Results}
\label{sec:analytical-results}

Results based on the single meta game in table~\ref{tab:ExpectedFitness_4Types} are not fully general and possibly spoiled by random
fluctuations in the sampling procedure. Fortunately, for the case of $2 \times 2$ symmetric
games, the main result that maxmin types $(\text{obj}, \text{imp})$ are strictly dominated can
also be shown analytically for generous general conditions.

\begin{proposition} \label{proposition1}
  Let $\mathcal{G}$ be the class of $2 \times 2$ symmetric games with payoffs sampled from a
  set of i.i.d.~values with at least three elements in the support. Then,
  $(\text{reg}, \text{imp})$ strictly dominates $(\text{obj}, \text{imp})$ in the resulting
  meta-game.
\end{proposition}

\begin{proof}
See Appendix~\ref{sec:proofs}.
\end{proof}

\begin{corollary} \label{corollary1} If we only consider imprecise belief types, $(\text{obj}, \text{imp})$ and
  $(\text{reg}, \text{imp})$, and letting $\mathcal{G}$ be as in Proposition~\ref{proposition1}, then the unique evolutionarily stable state is a monomorphic population of
  $(\text{reg}, \text{imp})$ players.
\end{corollary}

%Obviously, $(\text{reg}, \text{imp})$,
%$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ players will be neutrally stable, as before. 
%and fitness-based selection will tend to weed out maxmin play and leave us with a population of
%arbitrary frequency of $(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
%$(\text{reg}, \text{prc})$ players. 
The result shows that there is support for the main conceptual
point that we wanted to make: 
%non-veridical representations \emph{can}, under
%specific circumstances, persist under evolutionary selection based on objective
%fitness. Moreover, 
non-veridical regret-based preference representations can not only persist, but even be favored by
natural selection if agents may have imprecise beliefs.
This
tells us that the main conclusions drawn in the previous section based on the approximated meta-game of table~\ref{tab:ExpectedFitness_4Types} hold more generally 
for
arbitrary $2 \times 2$ symmetric games with i.i.d. sampled payoffs.

\section{Extensions}
\label{sec:extensions}

How do the basic results from the previous section carry over to richer models?
Section~\ref{sec:more-types} first introduces further conceptually interesting preference types
that have been considered in the literature. Section~\ref{sec:n-times-n} then addresses the
case of $n \times n$ games for $n \ge 2$. Finally, Section~\ref{sec:solitary-decisions} ends
with a brief comparison of game situations with cases of solitary decision making.

\subsection{More Types}
\label{sec:more-types}


Beyond objective and regret-based preferences, other preference types have been investigated. A
famous example is the \textit{altruistic type} \citep[e.g.,][]{Beck76,BestGuth98}, summoned to
explain the possibility of altruistic behavior. At the other end of the spectrum, the \emph{competitive
type} is located. 

\begin{definition}[Altruistic and competitive types] \label{defn:alttype}

  Fix a two-player game $G$ with actual evolutionary payoff functions $\pi_i$. The \textit{altruistic type}
  has subjective utility $\theta(a_i, a_j):=\pi_i(a_i,a_j) + \pi_j(a_i,a_j)$.\footnote{A
    more general formulation would be to define an $ \alpha$-altruistic type, for
    $\alpha \in [0,1]$, with subjective utility
    $ \theta_\alpha(a_i, a_j):=\pi_i(a_i,a_j) + \alpha \pi_j(a_i,a_j)$. Since we are not
    interested in the evolution of degrees of altruism, here we simply fix $ \alpha = 1 $.} The
  \emph{competitive type} has subjective utility
  $\theta(a_i, a_j):=\pi_i(a_i,a_j) - \pi_j(a_i,a_j)$.

\end{definition}



\begin{table}[]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
  \hline
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{com}, \text{imp})$
 & $(\text{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\text{com}, \text{prc})$
 & $(\text{alt}, \text{prc})$ \\ 
  \hline
  $(\text{reg}, \text{imp})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{imp})$ & 6.486 & 6.484 & 6.088 & 6.703 & 6.486 & 6.486 & 6.088 & 6.875 \\
  $(\text{com}, \text{imp})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\text{alt}, \text{imp})$ & 5.949 & 5.722 & 5.326 & 6.396 & 5.949 & 5.949 & 5.326 & 6.568 \\
  $(\text{reg}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{obj}, \text{prc})$ & 6.663 & 6.662 & 5.829 & 7.105 & 6.663 & 6.663 & 5.829 & 7.489 \\
  $(\text{com}, \text{prc})$ & 6.323 & 6.758 & 5.496 & 6.977 & 6.323 & 6.323 & 5.496 & 7.149 \\
  $(\text{alt}, \text{prc})$ & 6.331 & 5.893 & 5.497 & 6.566 & 6.331 & 6.331 & 5.497 & 7.152 \\
   \hline                          
\end{tabular}    
}                  
\caption{Average evolutionary fitness from Monte Carlo simulations of 100,000 symmetric $2 \times 2$ games}
\label{tab:ExpectedFitness_2x2_Full}        
\end{table}   
 
Table~\ref{tab:ExpectedFitness_2x2_Full} shows results of Monte Carlo simulations that
approximate the expected fitness in the relevant meta-game with all types considered so
far. These results confirm basic intuitions about altruistic and competitive types: everybody would like to have an altruistic co-player and nobody
would like to play against a competitive player. Perhaps more surprisingly, $(\text{alt}, \text{imp})$
come up strictly dominated by $(\text{com}, \text{imp})$, but competitive types themselves are worse off
against all types except against maxmin players $(\text{obj}, \text{imp})$ than any of
the behaviorally equivalent types $(\text{reg}, \text{imp})$, $(\text{obj}, \text{prc})$, and
$(\text{reg}, \text{prc})$.
\iffalse This is a noteworthy results in the light of the fact
that evolving altruistic preferences have been shown to support cooperative behavior in a
single stage game \myalert{[CITE]}. In contrast, averaging over payoffs in multiple stage
games, like we do here, makes altruistic preferences prime victims of evolutionary eradication.
\fi
It is easy to see then that the previous result still obtains for the larger meta-game in
table~\ref{tab:ExpectedFitness_2x2_Full}: $(\text{reg}, \text{imp})$,
$(\text{obj}, \text{prc})$, and $(\text{reg}, \text{prc})$ are still neutrally stable;
simulation runs of the (discrete-time) replicator dynamics on the $8 \times 8$ meta-game from
table~\ref{tab:ExpectedFitness_2x2_Full} end up in population states consisting of only these
three types in arbitrary proportion.

In sum, the presence of other plausible preference types, such as competitive and altruistic
types, does not undermine, but rather strengthens the previous results about the evolutionary
drive towards regret-based preferences.


                                   
\subsection{More Actions}
\label{sec:n-times-n}

Results from Section~\ref{sec:basic-model-1} relied heavily on Fact~\ref{fact:equivalence2x2}, that
is no longer true when we look at arbitrary $n \times n$ games. Let us see what
happens in a broader class of games.

Table~\ref{tab:ExpectedFitness_10x10} gives approximations of expected fitness in the class of
$n \times n$ symmetric games. Concretely,
the numbers in table~\ref{tab:ExpectedFitness_10x10} are averages of evolutionary payoffs obtained in
100,000 randomly sampled symmetric games, where each game was sampled by first picking a number of acts
$n \in \set{2, \dots, 10}$ uniformly at random, and then filling the necessary $n \times n$
payoff matrix with i.i.d.~sampled numbers, as before.


\begin{table}[]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccccc}
  \toprule
 & $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{com}, \text{imp})$
 & $(\text{alt}, \text{imp})$
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 & $(\text{com}, \text{prc})$
 & $(\text{alt}, \text{prc})$ \\ 
  \midrule
  $(\text{reg}, \text{imp})$ & 6.567 & 6.570 & 5.650 & 6.992 & 6.564 & 6.564 & 5.593 & 7.409 \\
  $(\text{obj}, \text{imp})$ & 6.476 & 6.483 & 5.896 & 6.818 & 6.484 & 6.484 & 5.850 & 7.124 \\
  $(\text{com}, \text{imp})$ & 6.468 & 6.647 & 5.512 & 7.169 & 6.578 & 6.578 & 5.577 & 7.354 \\
  $(\text{alt}, \text{imp})$ & 5.968 & 5.923 & 5.363 & 6.685 & 5.975 & 5.975 & 5.086 & 6.973 \\
  $(\text{reg}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{obj}, \text{prc})$ & 6.908 & 6.918 & 5.988 & 7.456 & 6.929 & 6.929 & 5.934 & 7.783 \\
  $(\text{com}, \text{prc})$ & 6.529 & 6.680 & 5.445 & 7.276 & 6.542 & 6.542 & 5.521 & 7.440 \\
  $(\text{alt}, \text{prc})$ & 6.450 & 6.337 & 5.772 & 6.978 & 6.457 & 6.457 & 5.479 & 7.500 \\
   \bottomrule                         
\end{tabular}  
}                    
\caption{Average evolutionary fitness for 100,000 randomly generated $n \times n$ symmetric games with $n$ randomly drawn from $\set{2, \dots, 10}$.}
\label{tab:ExpectedFitness_10x10}        
\end{table}

The most important result is that the regret minimizing type $(\text{reg}, \text{imp})$ is
strictly dominated by $(\text{reg}, \text{prc})$ and by $(\text{obj}, \text{prc})$ in the
meta-game from table~\ref{tab:ExpectedFitness_10x10}. This means that while simple regret minimization
\emph{can} thrive in some evolutionary contexts, there are also contexts where it is
demonstrably worse off. While this may be bad news for regret minimizing types
$(\text{reg}, \text{imp})$, it is not the case that regret types \emph{as such} are weeded out
by selection. Since, by Fact~\ref{fact:maxEU-minReg}, $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are behaviorally equivalent in general, it remains that selection
based on meta-games constructed from $n \times n$ games will still not eradicate regret
preferences. So, although regret types may not be selected for in this case, they are also not
selected against.

On the other hand, there are plenty of ways in which the basic insight from
Proposition~\ref{proposition1} that regret-based preferences can be strictly better than
veridical objective preferences in case of imprecise beliefs could make for situations in which
evolution would select for regret types exclusively, even in $n \times n$ games. If, for
example, the belief type of a player is a trait that biological evolution has no bite on, but rather
something that the particular choice situation would exogenously give us, then regret-based
preferences \emph{can} again drive out veridical preferences altogether. For concreteness,
suppose that only preference types compete and that agents' belief types are exogenously given,
in such a way that both players hold precise beliefs with probability $p$ and they both have imprecise
beliefs otherwise. This transforms the meta-game from table~\ref{tab:ExpectedFitness_10x10} into a
simpler $4 \times 4$ meta-game in which the payoff obtained for a preference type is the weighted
average over the payoffs of the choice mechanisms including that preference type in
table~\ref{tab:ExpectedFitness_10x10}. Setting $p = 0.98$ for illustration, we get the meta-game in table~\ref{tab:ExogeneousEpistemics}.
\begin{table}[]
\centering
\begin{tabular}{ccccc}
  \toprule
  & $\text{reg}$ 
  & $\text{obj}$
  & $\text{com}$
  & $\text{alt}$ \\ 
  \midrule
  $\text{reg} $ & 6.926 & 6.926 & 5.942 & 7.757 \\ 
  $\text{obj} $ & 6.924 & 6.924 & 5.948 & 7.751 \\ 
  $\text{com }$ & 6.566 & 6.570 & 5.481 & 7.434 \\ 
  $\text{alt} $ & 6.463 & 6.461 & 5.478 & 7.469 \\ 
   \bottomrule
\end{tabular}
\caption{Meta game for the evolutionary competition between preference types when epistemic types are exogenously
  given (see main text)}
\label{tab:ExogeneousEpistemics}
\end{table}
The only evolutionarily stable state of this meta-game is a monomorphic population of regret types. Accordingly, all of
our simulation runs of the (discrete-time) replicator dynamics converge to monomorphic
regret-type populations. The reason why regret types prosper is because they have a substantial
fitness advantage if paired with imprecise beliefs (Proposition~\ref{proposition1}). If imprecise uncertainty is exogenously given
as something unavoidable that happens to agents (e.g., in situation where they really have no
way of narrowing down the probability set), and even if that happens only very infrequently
(i.e., for rather low $p$), regret types \emph{will} outperform veridical preference types, as
well as competitive and altruistic types.


\subsection{Solitary Decisions}
\label{sec:solitary-decisions}

To see how different choice mechanisms behave in evolutionary competition based on solitary
decision making, we approximated, much in the spirit of meta-games, average accumulated
fitness obtained in randomly generated solitary decision problems. For our purposes, a decision
problem $\tuple{S, \Acts, \pi}$ consists of a set of states $S$, a set of acts $\Acts$, and
a payoff function $\pi \mycolon S \times \Acts \rightarrow \mathbb{R}$.  We generate arbitrary
decision problems by selecting, uniformly at random, numbers of states and acts
$n_s, n_\act \in \set{2, \dots, 10}$ and then filling the utility table, so to speak, by
i.i.d.~samples for each $\pi(s, \act) \in \set{0, 10}$. Unlike with
two-player games, we need to also sample the actual state of the world, which we selected
uniformly at random from the available states in the current decision problem. As player types,
we considered the original cast of four from table~\ref{tab:CMs}, since altruistic and
competitive types are meaningless in solitary decision situations. As before, the
relevant fitness measure, defined in Equation~(\ref{eq:FittnessChoiceMechSolitary}), was
approximated by Monte Carlo simulation, the results of which are given in
table~\ref{tab:SolitaryDecisions}.

\begin{table}
  \centering
  \begin{tabular}{cccc}
    \toprule
   $(\text{reg}, \text{imp})$ 
 & $(\text{obj}, \text{imp})$ 
 & $(\text{reg}, \text{prc})$ 
 & $(\text{obj}, \text{prc})$ 
 \\ \midrule
    6.318 & 6.237 & 6.661 & 6.661 \\ \bottomrule
  \end{tabular}
  \caption{Expected fitness of choice mechanisms approximated from 100,000 simulated solitary
    decision problems (see main text)}
  \label{tab:SolitaryDecisions}
\end{table}

Facts~\ref{fact:maxEU-minReg} and \ref{fact:equivalence2x2} still apply:
$(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$ are behaviorally equivalent in
general, and $(\text{reg}, \text{imp})$ is behaviorally equivalent to the former two in
decision problems with two states and two acts. This shows in the results from
table~\ref{tab:SolitaryDecisions} in that the averages for $(\text{reg}, \text{prc})$ and
$(\text{obj}, \text{prc})$ are identical. But since we included decision problems with more
acts and more states as well, the average for regret minimizers $(\text{reg}, \text{imp})$ is
not identical to the one of $(\text{reg}, \text{prc})$ and $(\text{obj}, \text{prc})$. It
is, in fact, lower, but again not as low as that of $(\text{obj}, \text{imp})$.

This means that every relevant result we have seen about game situations is also borne out for
solitary decisions. Evolutionary selection based on objective fitness will not select against
regret-based preferences, as these are indistinguishable from veridical preferences
when paired with precise beliefs. But when paired with imprecise beliefs, regret-based
preferences outperform veridical preferences. Consequently, if there is a chance, however
small, that agents fall back onto imprecise beliefs, evolution will actually positively select
for non-veridical regret-based preferences.


\iffalse
\subsection{Arbitrary probabilistic beliefs}
\label{sec:arbitr-prob-beli}

So far, we have assumed that epistemic types $\text{prc}$ hold an unbiased, flat belief. It is
worthwhile considering what happens when this assumption is relaxed and we allow agents with
probabilistic beliefs to make use of the full spectrum of probabilistic beliefs. It should be
clear that quality of beliefs directly impacts prospects of successful decision making: if a
decision maker knows the actual state, or can put a high level of credence on the actual state,
accumulated fitness can be expected to be high. We see this, unsurprisingly, in numerical
simulations. We looked at the average payoff accumulated in 100,000 decision problems sampled
as before, except that for belief types with probabilistic beliefs we sampled a random
probabilistic belief $p$ (from an unbiased Dirichlet distribution) and chose the actual world
state with probability weights $p$. This effectively implements a bias for beliefs of the
decision maker that tend to put more weight on the actual state (although the procedure does
admittedly perverse the natural chicken-and-egg logic in this case that the actual world state
should come first and beliefs be a function of it). As a result, the average payoffs of types
$\tuple{\text{reg}, \text{prc}}$ and $\tuple{\text{obj}, \text{prc}}$ are still the exact same
(because we compare what agents with different preference types would do under the same
beliefs; we are not interested in statistical fluctuations given one type better beliefs by
pure happenstance), but with $7.125$ notably higher than before. In effect, better
probabilistic beliefs lead to better decisions. If agents can learn, reason and extrapolate to
form better probabilistic beliefs, that will help them whenever they take their beliefs into
account. 

But this is orthogonal to the arguments in this paper, where the focus is on possibilities for
the evolution of non-veridical preferences. We know from Fact~\ref{fact:maxEU-minReg} that
regret types and veridical preference types, when paired with probabilistic belief types that
maximize/minimize expectations, are behaviorally equivalent, \emph{no matter what they believe},
as long as they belief the same. So, if learning, insight and statistical knowledge of a
recurrent situation \emph{can} be brought to bear, this will not make evolution select against
regret-based preferences. If, on the other hand, agents have no basis for probabilistic
beliefs, then we have shown that there are general and basic circumstances in which
regret-based preferences can actually be selected, despite their non-veridicality.
\fi


\section{Conclusion} \label{sec:conclusion}


The assumption that players and decision makers maximize their (subjective) preferences is central through all economic literature, and the maximization of actual (objective) payoffs is often justified by appealing to evolutionary arguments and natural
selection. In contrast to the standard view, we showed the
existence of player types with subjective utilities different
from the actual evolutionary payoffs that can outperform types whose
subjective utilities coincide with the evolutionary payoffs.
While the literature on evolution of preferences has focused
on fixed games, we have adopted a
more general approach here. We suggested that attention
to “meta-games” is crucial, because what may be a good
subjective representation in one type of game (e.g., cooperative preferences in the Prisoner’s Dilemma class) need not
be generally beneficial. Taken together, we presented a variety of plausible circumstances in which evolutionary competition between choice
mechanisms on a larger class of games can favor non-veridical preference representations focusing on regret.

Meta-games are useful tools for probing into conceptual questions about ecological
rationality. The approach is rich and general, with many possible further
applications imaginable beyond the case study presented here. For example, the evolutionary
investigation of different risk and ambiguity attitudes when competing against each other may bring new insights on both behavioral and theoretic
decision making under uncertainty, as well as on many issues in theoretical biology and
behavioral ecology. Moreover, variability in the action selection function has been entirely
ignored in this paper. Finally, while this paper only used dummy models of diverse decision
environments, the meta-game approach also allows for empirically informed models of rich and
variable environments.


\appendix

\section{Background on Imprecise Probabilities}
\label{sec:impr-prob-beli}

Consider the following problem. I have, on the desk in front of me, a bag containing either red
or black marbles. There is no further information about the distribution of the two
colors. What is then the probability that I will draw a red marble? As in \citet{walley96}, we
are not talking about physical probability, but rather about epistemic probability, i.e., the
subjective probability that an agent attributes to an event.  In the literature there are two
traditional ways of answering this. According to standard Bayesianism, one should always be
able to determine a precise betting rate, that immediately translates into a precise
probability distribution. In the case
of the bag on my desk, a strict Bayesian, in the absence of any further information, would
probably rely on the \textit{principle of insufficient reason} and answer that the DM should
have a uniform measure that considers all the outcomes equipossible.


The second answer involves imprecise probabilities and is related to the literature about
unmeasurable uncertainty. Many authors argued in favor of this kind of approach
\citep[e.g.,][]{levi74,gardsah82,walley96}. Since it is consistent with the information
available that all the marbles are red, or that no marble is red, the resulting model should
take it into account and specify a lower probability of red $\underline{P}(R)$ and an upper
probability of red $\overline{P}(R)$ that define an interval of possible probabilistic beliefs
[$\underline{P}(R), \overline{P}(R)$]. Given the information available in this case we would
have $\underline{P}(R)=0$ and $\overline{P}(R)=1$. This approach appears particularly relevant
in a game theoretical context. Indeed, in a recent paper \citet{BattCerrMM15} write:

\begin{quote}
  Such uncertainty is inherent in situations of strategic interaction. This is quite obvious
  when such situations have been faced only a few times. (p.~646)
\end{quote}

\noindent In evolutionary game theory, for instance, players in a population obviously face
uncertainty about the composition of the population that they are part of, and consequently
about the co-player that they are randomly paired with at each round and about the co-player's
action. 

We therefore represent maximal uncertainty in two different ways that correspond to the two
approaches discussed in the main text: a Bayesian uniform measure and an imprecise probability
set. The latter represents uncertainty for players who are not able to narrow down the set of
probability measures to a single one and they consider all of them as possible. The former
precise belief type conceptualizes the uncertainty about the population by using the principle
of insufficient reason and ascribes equal probability to all the possible alternatives. As we
have seen, both options are reasonable and justifiable ways to deal with situations of
uncertainty.

In decision theory, the imprecise probability model is in line with most of the representation
results of decision making under uncertainty \citep[e.g.,][]{gilsch89,KlibMarMuk05,GhirMar02},
and seems justified by empirical observations too. A prominent example is Ellsberg's paradox
(\citet{ells61}). A bag contains 90 marbles, 30 are red and the remaining 60 are either black
or yellow. A marble is to be drawn and DM can win 100€ if she guesses the color of the drawn
marble. DM is offered two different bets. In the first one she can choose to bet either on
red (R) or on yellow (Y), while in the second she can bet either on red or black (RB) or on
yellow or black (YB), as shown in table \ref{Ellsberg}.

\definecolor{yellow}{RGB}{255,188,1}


\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\cmidrule(l){2-4}
\multicolumn{1}{c}{} & {\color{red}R}   & {\color{yellow}Y}   & B   \\ \cmidrule(l){2-4} 
$f_{{\color{red}R}}$              & 100 & 0   & 0   \\
$f_{{\color{yellow}Y}}$              & 0   & 100 & 0   \\
$f_{{\color{red}R}B}$            & 100 & 0   & 100 \\
$f_{{\color{yellow}Y}B}$             & 0   & 100 & 100 \\ \bottomrule
\end{tabular}
\caption{Ellsberg's paradox}
\label{Ellsberg}
\end{table}

The pattern consistently observed in experiments is:
$f_{{\color{red}R}} \succ f_{{\color{yellow}Y}}$ and
$f_{{\color{red}R}B} \prec f_{{\color{yellow}Y}B}$. This is clearly incompatible with any
precise probabilistic belief about the proportion of black and yellow marbles. This kind of
behavior has been famously axiomatized by \citet{gilsch89}. The
choice pattern is normally explained in terms of aversion to unmeasurable uncertainty, and the standard
representation is given by means of a set of probability measures together with maxmin rule. We
can think of DM in Ellsberg's example as maximinimizing expected utility over the
probability set
$
[\underline{P}(R)=\overline{P}(R)=\frac{1}{3}, \underline{P}(Y)= \underline{P}(B)=0, \overline{P}(Y)= \overline{P}(B)=\frac{2}{3}]
$.
This amounts to calculating for any probability measure contained in the probability set the
expected utility of any available action, and then to choosing the action that guarantees the
highest minimal expected utility. 

Evidence from experimental literature suggests that agents are mostly uncertainty averse \citep[e.g.,][]{TrautKuil16}. In line with empirical data, we assume that players in the population
are uncertainty averse and conform to maxmin expected utility. The resulting behavior is then
produced by maximinimizing the subjective utility given by the player's preference type over
the set of probabilities given by the belief type.


\section{Proofs}
\label{sec:proofs}

The proof of Proposition \ref{proposition1} relies on a partition of $\mathcal{G}$, and on some
lemmas. For brevity, let us denote the regret minimizer $(\text{reg}, \text{imp})$ by $R$ and
the maximinimizer $(\text{obj}, \text{imp})$ by $M$. Following
Equation~(\ref{eq:FittnessChoiceMechGamePairwise}), let $F_{\mathcal{G}}(X,Y)$ denote the
expected payoff of choice mechanism $X$ against choice mechanism $Y$ on the possibly restricted
class of fitness games $\mathcal{G}$.

\vspace{.5cm}


\noindent \textbf{Proof of Proposition \ref{proposition1}.} By definition of strict dominance,
we have to show that in the class $\mathcal{G}$ of symmetric $2\times2$ games with payoffs
sampled from a set of i.i.d. values with at least 3 elements in the support, it holds that:
\begin{multicols}{2}
  \begin{itemize}
  \item[(i)] $F_{\mathcal{G}}(R,R)>F_{\mathcal{G}}(M,R);$
  \item[(ii)] $F_{\mathcal{G}}(M,M)<F_{\mathcal{G}}(R,M).$
  \end{itemize}
\end{multicols}

\noindent To show this we use the following partition of $\mathcal{G}$, based on payoffs parametrized as
follows:
\begin{center}
  \begin{tabular}{ccc}
    \toprule
    & $I$ & $II$ \\ \midrule
    $I$ & $a$ & $b$ \\
    $II$ & $c$ & $d$ \\ \bottomrule
  \end{tabular}
\end{center}
\begin{enumerate}
\item Coordination games $\mathcal{C}$: $a>c$ and $d>b$;
\item Anti-coordination games $\mathcal{A}$: $a<c$ and $d<b$;
\item Strong dominance games $\mathcal{S}$: aut $(a>c$ and $b>d)$
aut $(a<c$ and $b<d)$;
\item Weak dominance games $\mathcal{W}$: aut $a=c$ aut $b=d$;
\item Boring games $\mathcal{B}$: $a=c$ and $b=d$.
\end{enumerate}
Before proving the lemmas, it is convenient to fix some notation. Let us call $x,y,z$ the 3
elements in the support, and without loss of generality suppose that $ x > y > z $.  We denote
by $C$ a coordination game in $\mathcal{C}$ with payoffs $a_{C}$, $b_{C}$, $c_{C}$, and
$d_{C}$; similarly for games $A \in \mathcal{A}$, $S \in \mathcal{S}$, $W \in \mathcal{W}$, and
$B \in \mathcal{B}$.  Let us denote by $I_{RC}$ the event that a $R$-player plays action $I$ in
the game $C$; and similarly for action $II$, for player $M$, and for games $A$, $S$, $W$ and
$B$. We first consider the case of i.i.d. sampling with finite support.

\begin{lemma} \label{lemma:S-B games} $R$ and $M$ perform equally well in $\mathcal{S}$ and in
  $\mathcal{B}$.
\end{lemma}

\begin{proof}
  By definition of regret minimization and maxmin it is easy to check that whenever in a game
  there is a strongly dominant action $a^{\$}$, then $a^{\$}$ is both the maxmin action and the
  regret minimizing action. Then, for all the games in $\mathcal{S}$, $R$ chooses action $a$ if
  and only if $M$ chooses action $a$. Consequently, $R$ and $M$ always perform equally (well)
  in $\mathcal{S}$. In the case of $\mathcal{B}$ it is trivial to see that all the players
  perform equally.
\end{proof}

\begin{lemma} \label{lemma:W games} In $\mathcal{W}$, $R$ strictly dominates $M$.
\end{lemma}

\begin{proof}
  Assume without loss of generality that $b=d$, and that $ a>c $. There are two cases that we have to check:
  (i) $c < b=d$ and (ii) $c \geq b=d$. In the first case it is easy to see that $R$ and $M$
  perform equally: act $I$ is the choice of both $R$ and $M$. In the case of (ii) instead we have that $I$ is the regret minimizing action, whereas both actions have the same minimum and $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$, since both $I$ and
  $II$ maximize the minimal payoff. Consider now a population of $R$ and $M$ playing games from
  the class $\mathcal{W}$.  Whenever (i) is the case $R$ and $M$ perform equally well. But suppose $W \in \mathcal{W}$ and (ii) is the case. Then,
  $\pi_W(R,R)=a>\frac{1}{2}a+\frac{1}{2}c=\pi_W(M,R)$, whereas
  $\pi_W(M,M)=\frac{1}{4}a+\frac{1}{4}b+\frac{1}{4}c+\frac{1}{4}d<\frac{1}{2}a+\frac{1}{2}b=\pi_W(R,M)$.
  Hence, we have that in general
  $F_{\mathcal{W}}(R,R)>F_{\mathcal{W}}(M,R),\mbox{ and
  }F_{\mathcal{W}}(M,M)<F_{\mathcal{W}}(R,M)$.
\end{proof}

\noindent Since it is not difficult to see that both $(R,R)$ and $(M,M)$ are strict Nash equilibria in
$\mathcal{C}$, and that $(R,R)$ and $(M,M)$ are not Nash equilibria in $\mathcal{A}$, the main
part of the proof will be to show that $R$ strictly dominates $C$ in the class
$\mathcal{C}\cup\mathcal{A}$, that is:
\begin{multicols}{2}
  \begin{itemize}
  \item[(i')] $F_{\mathcal{C}\cup\mathcal{A}}(R,R)>F_{\mathcal{C}\cup\mathcal{A}}(M,R),$
  \item[(ii')] $F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M).$
  \end{itemize}
\end{multicols}

\noindent This part needs some more lemmas to be proven, but firstly we introduce
the following bijective function $\phi$ between coordination and
anti-coordination games.

\begin{definition}[$\phi$] \label{def:bijection phi} The permutation $\phi(a,b,c,d)=(c,d,a,b)$
  defines a bijective function $\phi:\mathcal{C}\rightarrow\mathcal{A}$ that for each
  coordination game $C\in\mathcal{C}$ with payoffs $(a_{C},b_{C},c_{C},d_{C})$ gives the
  anti-coordination game $A\in\mathcal{A}$ with payoffs
  $(a_{A},b_{A},c_{A},d_{A})=(c_{C},d_{C},a_{C},b_{C})$. Essentially, $\phi$ swaps rows
  in the payoff matrix.
\end{definition}

\begin{lemma} \label{lemma:probabilities coord-ant}
Occurrence probability of $C$ equals that of $\phi(C)$: $P(\phi(C))=P(C)$.
\end{lemma}

\begin{proof}
  By definition, each game $C\equiv(a_{C},b_{C},c_{C},d_{C})$ is such that $a_{C}>c_{C}$ and
  $d_{C}>b_{C}$, and each game $A\equiv(a_{A},b_{A},c_{A},d_{A})$ is such that $a_{A}<c_{A}$
  and $d_{A}<b_{A}$. Given that $a,b,c,d$ are i.i.d. random variables and that a sequence of
  i.i.d. random variables is exchangeable, it is clear that the probability of
  $(a_{C},b_{C},c_{C},d_{C})$ equals the probability of $(c_{C},d_{C},a_{C},b_{C})$.  Hence,
  $P(\phi(C))=P(C)$.
\end{proof}

\begin{lemma} \label{lemma:probabilities actions coord-ant} Let $P(E)$ be the probability of
  event $E$, e.g., $P(I_{RC})$ is the probability that a random $R$-player plays act $I$ in coordination game $C$, which is either 0, $.5$ or 1.  It then holds that:
\begin{itemize}
\item $P(I_{RC})=P(II_{R\phi(C)})$, and $P(II_{RC})=P(I_{R\phi(C)})$;
\item $P(I_{MC})=P(II_{M\phi(C)})$, and $P(II_{MC})=P(I_{M\phi(C)})$.
\end{itemize}
\end{lemma}

\begin{proof}
  It is easy to check that if $b_{C}-d_{C}>c_{C}-a_{C}$, a $R$-player plays action $I$ in $C$;
  that if $b_{C}-d_{C}<c_{C}-a_{C}$, $R$ plays $II$; and that if $b_{C}-d_{C}=c_{C}-a_{C}$, a
  $R$-player is indifferent between $I$ and $II$ in $C$, and so randomizes with
  $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly, if $a_{A}-c_{A}>d_{A}-b_{A}$, a $R$-player plays
  action $I$ in $A$; if $a_{A}-c_{A}<d_{A}-b_{A}$, $R$ plays $II$; and if
  $a_{A}-c_{A}=d_{A}-b_{A}$, a $R$-player is indifferent between $I$ and $II$ in $A$, and
  randomizes with $(\frac{1}{2}I;\frac{1}{2}II)$. Consequently, if $b_{C}-d_{C}>c_{C}-a_{C}$,
  then $P(I_{RC})=1$, and by definition of $\phi$ we have $P(II_{R\phi(C)})=1$. Likewise, if
  $b_{C}-d_{C}<c_{C}-a_{C}$, then $P(II_{RC})=1=P(I_{R\phi(C)})$; and if
  $b_{C}-d_{C}=c_{C}-a_{C}$,
  then $P(I_{RC})=P(II_{RC})=\frac{1}{2}=P(II_{R\phi(C)})=P(I_{R\phi(C)})$. \\
  In the same way, in coordination games we have that if $b_{C}>c_{C}$, a $M$-player plays $I$;
  if $c_{C}>b_{C}$, a $M$-player plays $II$; and if $b_{C}=c_{C}$, $M$ is indifferent between
  $I$ and $II$, and plays $(\frac{1}{2}I;\frac{1}{2}II)$. In anti-coordination games instead,
  if $a_{A}>d_{A}$, $M$ plays $I$; if $a_{A}<d_{A}$, $M$ plays $II$; if $a_{A}=d_{A}$, $M$
  plays $(\frac{1}{2}I;\frac{1}{2}II)$.  By definition of $\phi$:
  $P(I_{MC})=1=P(II_{M\phi(C)})$ if $b_{C}>c_{C}$; $P(II_{MC})=1=P(I_{M\phi(C)})$ if
  $c_{C}>b_{C}$; and $P(I_{MC})=P(II_{MC})=\frac{1}{2}=P(II_{M\phi(C)})=P(I_{M\phi(C)})$ if
  $b_{C}=c_{C}$.
\end{proof}

\begin{lemma} \label{lemma:action implications}
It holds that:
  \begin{itemize}
  \item $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$;
  \item $a^{C}=d^{C}\rightarrow I_{MC}=I_{RC}$.
  \item $a_{C}<d_{C}\rightarrow(II_{MC}\subseteq II_{RC})$;
  \end{itemize}
\end{lemma}

\begin{proof}
The event that $R$ plays action $I$, $I_{RC}$, with positive probability is the event that $b_{C}-d_{C} \geq c_{C}-a_{C}$: if $b_{C}-d_{C}>c_{C}-a_{C}$, $R$ plays $I$, and if $b_{C}-d_{C}=c_{C}-a_{C}$, $R$ plays
  $(\frac{1}{2}I;\frac{1}{2}II)$. Similarly, the event that $I_{MC}$ has positive occurrence is the event that $b_{C} \geq c_{C}$: if $b_{C}>c_{C}$, $M$ plays $I$, and if
  $b_{C}=c_{C}$, $M$ plays $(\frac{1}{2}I;\frac{1}{2}II)$. 
Then, $I_{RC}$ implies that
  $b_{C}-d_{C}\geq c_{C}-a_{C}$, and $I_{MC}$ implies that $b_{C}\geq c_{C}$. Moreover, on the
  assumption that $a_{C}>d_{C}$, it is easy to check that $b_{C}\geq c_{C}$ implies
  $b_{C}-d_{C}>c_{C}-a_{C}$.  Hence, in any $C$ with $a_{C}>d_{C}$ it holds that
  $I_{MC}\mbox{ implies }I_{RC}$, i.e., $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$.
Instead, it is possible that $a_{C}>d_{C}$, $b_{C}-d_{C}>c_{C}-a_{C}$ and $b_{C}<c_{C}$ hold
  simultaneously, so that $I_{MC}\nsupseteq I_{RC}$.
By a symmetric argument it can be shown that
  $a_{C}<d_{C}\rightarrow(II_{MC}\subseteq II_{RC})$ too. Finally, when $a_{C}=d_{C}$ it holds
  that: $b_{C}-d_{C}>c_{C}-a_{C}$ iff $b_{C}>c_{C}$; $b_{C}-d_{C}<c_{C}-a_{C}$ iff
  $b_{C}<c_{C}$; and $b_{C}-d_{C}=c_{C}-a_{C}$ iff $b_{C}=c_{C}$. Hence,
  $a_{C}=d_{C}\rightarrow I_{MC}=I_{RC}$.
\end{proof}

\vspace{.5cm}

\noindent We are now ready to prove that
$F_{\mathcal{C}\cup\mathcal{A}}(R,R)>F_{\mathcal{C}\cup\mathcal{A}}(M,R)$. With notation like
$P(I_{RC} \cap I_{RC})$ denoting the probability that a random $R$-player plays $I$ and another
$R$-player plays $I$ as well in game $C$, rewrite the inequality as:
\begin{align*}
   \sum_{C \in \mathcal{C}}P(C) & [P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot
  d_{C}+P(I_{RC}\cap II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C}] 
  \\ + \sum_{A \in
    \mathcal{A}}P(A) & [P(I_{RA}\cap I_{RA})\cdot a_{A}+P(II_{RA}\cap II_{RA})\cdot
  d_{A}+P(I_{RA}\cap II_{RA})\cdot b_{A}+P(II_{RA}\cap I_{RA})\cdot c_{A}]\\
  > \sum_{C \in
    \mathcal{C}}P(C) & [P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot
  d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C}] \\
   + \sum_{A \in
    \mathcal{A}}P(A) & [P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot
  d_{A}+P(I_{RA}\cap II_{MA})\cdot c_{A}+P(II_{RA}\cap I_{MA})\cdot b_{A}]
\end{align*}
\noindent By Lemma \ref{lemma:probabilities coord-ant} and Lemma \ref{lemma:probabilities
  actions coord-ant}, we can express everything in terms of $C$ only:
\begin{align*}
  & \textstyle{\sum_{C}} P(C)[P(I_{RC}\cap I_{RC})\cdot a_{C}+P(II_{RC}\cap II_{RC})\cdot d_{C}+P(I_{RC}\cap
  II_{RC})\cdot b_{C}+P(II_{RC}\cap I_{RC})\cdot c_{C} + \\
  & \ \ \ \ P(II_{RC}\cap II_{RC})\cdot c_{C}+P(I_{RC}\cap I_{RC})\cdot b_{C}+P(II_{RC}\cap
  I_{RC})\cdot d_{C}+P(I_{RC}\cap
  II_{RC})\cdot a_{C}] \\
  > & \textstyle{\sum_{C}}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot
  d_{C}+P(I_{RC}\cap II_{MC})\cdot c_{C}+P(II_{RC}\cap I_{MC})\cdot b_{C} + \\
  & \ \ \ \ P(II_{RC}\cap II_{MC})\cdot c_{C}+P(I_{RC}\cap I_{MC})\cdot b_{C}+P(II_{RC}\cap
  I_{MC})\cdot a_{C}+P(I_{RC}\cap II_{MC})\cdot d_{C}]
\end{align*}
This simplifies to:
\begin{align*}
  & \textstyle{\sum_{C}}P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C}
  \cdot (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) + 
\\ & \ \ \ \ c_{C} \cdot
  (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap
  II_{RC})+P(II_{RC}\cap I_{RC}))] 
\\ > & \textstyle{\sum_{C}}P(C)[a_{C} \cdot (P(I_{RC}\cap
  I_{MC})+P(II_{RC}\cap I_{MC})) + b_{C} \cdot (P(II_{RC}\cap I_{MC})+P(I_{RC}\cap I_{MC})) + \\
  & \ \ \ \ c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{RC}\cap
  II_{MC})+P(I_{RC}\cap II_{MC}))]
\end{align*}
\noindent Now let us split into $a>d$ and $a<d$, and consider $a>d$ first.  Notice that, by
Lemma \ref{lemma:action implications}, the case $a=d$ is irrelevant in order to discriminate
between $R$ and $M$. If $a>d$, by Lemma \ref{lemma:action implications} we can eliminate the
cases where $R$ plays $II$ and $M$ plays $I$:
\begin{align*}
  & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot
  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})) \\ 
  & \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC})
  +P(II_{RC}\cap II_{RC})) + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC}))] \\
  > & 
  \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot P(I_{RC}\cap I_{MC}) + b_{C} \cdot P(I_{RC}\cap I_{MC}) +
  c_{C} \cdot (P(I_{RC}\cap II_{MC})+P(II_{RC}\cap II_{MC})) \\ 
  & \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap
  II_{MC})+P(I_{RC}\cap II_{MC}))]
\end{align*}
Rewrite:
\begin{align*}
  & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})-
  P(I_{RC}\cap I_{MC})) \\ 
  & \ \ \ \ + b_{C} \cdot (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC})-
  P(I_{RC}\cap I_{MC})) \\ 
  & \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})-
  P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\ 
  & \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap
  II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
\noindent We now distinguish between two cases: (1) $ a-c = d-b $ and (2) $  a-c \neq d-b $. Notice that $P(I_{RC}\cap II_{RC}) \neq 0$ if and only if case (1) obtains, and that $a>d$ and (1) imply $II_{MC}$. Then, from (1) we have\footnote{Note that when we have only 3 elements in the support it is not guaranteed that case (1), together with $a>d$, may arise in a coordination game, whereas it is guaranteed that case (2), together with $a>d$, occurs with some positive probability. If we take for instance $x= 5, y= 2, z= 1$, then case (1) cannot obtain, whereas if we take $x= 3, y= 2, z= 1$, both (1) and (2) may obtain ($a=3, b=1, c=2, d=2$ for case (1), and $a=3, b=1, c=2, d=2$ for case (2)). Moreover, under the assumption that $a>d$, having 3 elements in the support is a necessary and sufficient condition for case (2) to have positive occurrence in a coordination game. As it will be clear in the following, a positive occurrence of case (2) only is enough for the theorem to hold.}:

\begin{align*} 
& \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) + P(I_{RC}\cap II_{RC})) + b_{C} \cdot  (P(I_{RC}\cap II_{RC}) + P(I_{RC}\cap I_{RC}))\\
& \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap I_{RC}) +P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))\\
& \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap II_{RC})+P(II_{RC}\cap I_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
that is
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (\frac{1}{4}+\frac{1}{4}) + b_{C} \cdot  (\frac{1}{4}+\frac{1}{4}) + c_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2}) + d_{C} \cdot (\frac{1}{4}+\frac{1}{4}-\frac{1}{2}-\frac{1}{2})]> 0
\end{align*}
%\begin{align*}
%\textstyle{\sum_{C_{a>d}}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]> 0
%\end{align*}
\noindent Since we have assumed $ a-c = d-b $, the last inequality is not satisfied. We have instead:

$$ \sum_{C_{a>d}} P(C)[\frac{1}{2}a_{C}+ \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]= 0 $$

\noindent This means that where $a_{C}>d_{C}$ and
where (1) is the case, $R$ and $M$ are equally fit. This changes when we turn to (2). In that
case, since $a_{C}>d_{C}\rightarrow(I_{MC}\subseteq I_{RC})$ by Lemma \ref{lemma:action implications}, we have that
$P(I_{RC} \cap I_{RC})-P(I_{RC}\cap I_{MC})=P(I_{RC}\cap II_{MC})$. Moreover, when $a_{C}>d_{C}$, $b_{C}\geq c_{C}$ implies
  $b_{C}-d_{C}>c_{C}-a_{C}$ (see Lemma \ref{lemma:action implications}). Consequently, when $M$ plays either $I$ or $  (\frac{1}{2}I;\frac{1}{2}II)$, $R$ always plays $I$. Hence, whenever $a_{C}>d_{C}$ and (2) obtain, it also holds that $P(II_{RC}\cap II_{MC})=P(II_{RC} \cap II_{RC})$. In this case we can simplify:
\begin{align*}
& \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{RC}\cap I_{RC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{RC}\cap I_{RC})- P(I_{RC}\cap I_{MC})) \\
& \ \ \ \ + c_{C} \cdot (P(II_{RC}\cap II_{RC})- P(I_{RC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\
& \ \ \ \ + d_{C} \cdot (P(II_{RC}\cap II_{RC})- P(II_{RC}\cap II_{MC})- P(I_{RC}\cap II_{MC}))]> 0
\end{align*}
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot P(I_{RC}\cap II_{MC}) + b_{C} \cdot  P(I_{RC}\cap II_{MC}) \\- c_{C} \cdot P(I_{RC}\cap II_{MC}) - d_{C} \cdot P(I_{RC}\cap II_{MC})]> 0
\end{align*}
\begin{align*}
\textstyle{\sum_{C_{a>d}}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0
\end{align*}
\noindent We know that $I_{RC}$ implies that $a_{C}-c_{C}\geq d_{C}-b_{C}$.
% and $II_{MC}$ implies that $b_{C}\leq c_{C}$.
Since we have assumed that $a_{C}-c_{C}\neq d_{C}-b_{C}$, we have that
$a_{C}-c_{C} > d_{C}-b_{C}$. Hence, the inequality
$$\sum_{C_{a>d}} P(C)[P(I_{RC}\cap II_{MC})\cdot (a_{C} + b_{C} - c_{C} - d_{C})]> 0$$
is satisfied. So, when $a_{C}>d_{C}$, $R$
strictly dominates $M$. Symmetrically, from $a<d$ and by distinguishing between the two cases
(1) and (2) as before, in the end we get:
\begin{itemize}
\item[(1)] $\sum_{C_{a<d}} P(C)[-\frac{1}{2}a_{C}- \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]= 0$; and
\item[(2)] $\sum_{C_{a<d}} P(C)[P(II_{RC}\cap I_{MC})\cdot (-a_{C} - b_{C} + c_{C} + d_{C})]> 0$.
\end{itemize}
\noindent Hence, we can conclude that $R$ strictly dominates $M$ in the class
$\mathcal{C}\cup\mathcal{A}$. Notice that in case of i.i.d. sampling with infinite support,
games in $\mathcal{B} $ and $\mathcal{W} $ never arise, but the proof is the same for the
remaining games in $\mathcal{S}$, $\mathcal{C} $ and $\mathcal{A}$.

\medskip{}

\noindent It remains to be shown that
$F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M)$. As before, spell
this out as:
\begin{align*}
& \sum_{C}P(C)[P(I_{MC}\cap I_{MC})\cdot a_{C}+P(II_{MC}\cap II_{MC})\cdot d_{C}+P(I_{MC}\cap II_{MC})\cdot b_{C}+P(II_{MC}\cap I_{MC})\cdot c_{C}] \\
+ & \sum_{A}P(A)[P(I_{MA}\cap I_{MA})\cdot a_{A}+P(II_{MA}\cap II_{MA})\cdot d_{A}+P(I_{MA}\cap II_{MA})\cdot b_{A}+P(II_{MA}\cap I_{MA})\cdot c_{A}] \\
< & \sum_{C}P(C)[P(I_{RC}\cap I_{MC})\cdot a_{C}+P(II_{RC}\cap II_{MC})\cdot d_{C}+P(I_{RC}\cap II_{MC})\cdot b_{C}+P(II_{RC}\cap I_{MC})\cdot c_{C}]\\ 
+ & \sum_{A}P(A)[P(I_{RA}\cap I_{MA})\cdot a_{A}+P(II_{RA}\cap II_{MA})\cdot d_{A}+P(I_{RA}\cap II_{MA})\cdot b_{A}+P(II_{RA}\cap I_{MA})\cdot c_{A}]
\end{align*}
\noindent When $a>d$, similarly to the above derivation, we get: 
\begin{align*} & \textstyle{\sum_{C_{a>d}}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC}))\\ & + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) + P(I_{MC}\cap II_{MC})- P(I_{RC}\cap I_{MC}) - P(I_{RC}\cap II_{MC}))\\ & + c_{C} \cdot (P(II_{MC}\cap I_{MC}) +P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) \\ & + d_{C} \cdot (P(II_{MC}\cap II_{MC})+P(II_{MC}\cap I_{MC})- P(II_{RC}\cap II_{MC}))]< 0
\end{align*}
\noindent We now distinguish between (1) $b=c$, (2) $b>c$, and (3) $b<c$. Notice that either
(1) or (2), together with $a>d$, implies $I_{RC}$. Then we obtain:\footnote{Note that here, when
  we only have 3 elements in the support, case (2) is impossible, but cases (1) and (3) can
  occur with positive probability, and this enough for our purpose.}
\begin{itemize}

\item[(1)] $\sum_{C_{a>d}} P(C)[-\frac{1}{2}a_{C} - \frac{1}{2}b_{C} + \frac{1}{2}c_{C} + \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC}))]= 0$;

\item[(3)] $\sum_{C_{a>d}} P(C)[a_{C} \cdot (- P(I_{RC}\cap II_{MC})) + b_{C} \cdot  (- P(I_{RC}\cap II_{MC})) + c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] \leq 0$.

\end{itemize}
\noindent When $a<d$, the derivation proceeds symmetrically and we get: 
\begin{itemize}

\item[(1)] $\sum_{C_{a<d}} P(C)[\frac{1}{2}a_{C} + \frac{1}{2}b_{C} - \frac{1}{2}c_{C} - \frac{1}{2}d_{C}]< 0$;

\item[(2)] $\sum_{C_{a<d}} P(C)[a_{C} \cdot (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + b_{C} \cdot  (P(I_{MC}\cap I_{MC}) - P(I_{RC}\cap I_{MC})) + c_{C} \cdot (- P(II_{RC}\cap I_{MC})) + d_{C} \cdot (- P(II_{RC}\cap I_{MC}))] \leq 0$;

\item[(3)] $\sum_{C_{a<d}} P(C)[c_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC})) + d_{C} \cdot (P(II_{MC}\cap II_{MC})- P(II_{RC}\cap II_{MC}))] = 0$.

\end{itemize}
\noindent Finally, we can conclude that
$F_{\mathcal{C}\cup\mathcal{A}}(M,M)<F_{\mathcal{C}\cup\mathcal{A}}(R,M)$. As before, notice
that, when we have i.i.d. sampling with infinite support, games in $\mathcal{W}$ and
$\mathcal{B}$ never occur, but the proof is the same for all the other cases. Hence, both when
the support of $a,b,c,d$ is finite, and when the support is infinite, $R$ strictly dominates
$M$ under the conditions assumed. \hfill $\dashv$


\printbibliography[heading=bibintoc]

\end{document}



